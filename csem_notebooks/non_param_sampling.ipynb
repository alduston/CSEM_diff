{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rZU16q_rj09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5524df-ef3b-404d-d65d-cdd3080ff797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 steps, Blend score( True ): W2 =7.090e-02, W2_floor = 1.905e-02, score_RMSE=3.895e+01, time=36.9s\n",
            " \n",
            "10 steps, Blend score( Proxy ): W2 =6.433e-02, W2_floor = 3.329e-02, score_RMSE=3.744e+01, time=56.0s\n",
            " \n",
            "10 steps, Tweedie score: W2 =5.535e-02, W2_floor = 3.057e-02, score_RMSE=2.496e+01, time=20.1s\n",
            " \n",
            "10 steps, CSEM score: W2 =6.100e-01, W2_floor = 3.269e-02, score_RMSE=1.328e+01, time=20.1s\n",
            " \n"
          ]
        }
      ],
      "source": [
        "from re import S\n",
        "from ctypes import create_unicode_buffer\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import math\n",
        "from itertools import combinations\n",
        "\n",
        "from math import ceil\n",
        "from functools import partial\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------\n",
        "# Multi-scale MMD divergence\n",
        "# ----------------------------------------------\n",
        "\n",
        "try:\n",
        "    import cupy as cp\n",
        "except Exception:\n",
        "    cp = None\n",
        "import numpy as np\n",
        "\n",
        "M_MMD_DEFAULTS = {\n",
        "    # kernel mixture\n",
        "    \"kernel\": \"rbf\",        # 'rbf' or 'imq'\n",
        "    \"scales\": None,         # list/array of bandwidths (RBF sigmas or IMQ c's). If None, use median heuristic.\n",
        "    \"scale_weights\": None,  # list/array of nonnegative weights (same length as scales). If None, uniform.\n",
        "    # median heuristic for automatic scales (used if scales=None)\n",
        "    \"use_median_heuristic\": True,\n",
        "    \"num_scales\": 5,        # number of scales in the mixture\n",
        "    \"scale_min\": 0.5,       # lower multiplier relative to base (logspace)\n",
        "    \"scale_max\": 2.0,       # upper multiplier relative to base (logspace)\n",
        "    # estimator & output\n",
        "    \"unbiased\": True,       # U-statistic (exclude diagonals in Kxx, Kyy)\n",
        "    \"squared\": False,       # return MMD^2 if True, else sqrt(max(MMD^2,0))\n",
        "    # kernel-specific params\n",
        "    \"imq_beta\": 0.5,        # k_IMQ(d2) = (1 + d2 / c^2)^(-beta)\n",
        "    # numerics\n",
        "    \"chunk_size\": 4096,     # process pairwise blocks to limit memory\n",
        "    \"return_per_scale\": False, # if True, also return dict with per-scale contributions\n",
        "    # optional per-sample weights (length N_x / N_y). If provided, computes *weighted* MMD.\n",
        "    \"x_weights\": None,\n",
        "    \"y_weights\": None,\n",
        "}\n",
        "\n",
        "def _xp_of(arr):\n",
        "    \"\"\"Return array module (np or cp) that matches arr.\"\"\"\n",
        "    if cp is not None and isinstance(arr, cp.ndarray):\n",
        "        return cp\n",
        "    return np\n",
        "\n",
        "def _as_xp(x, xp):\n",
        "    \"\"\"Ensure x is an xp array.\"\"\"\n",
        "    if xp is np:\n",
        "        return np.asarray(x)\n",
        "    else:\n",
        "        return x if isinstance(x, xp.ndarray) else xp.asarray(x)\n",
        "\n",
        "def _median_pairwise_sqdist(Z, xp, max_samples=1024):\n",
        "    \"\"\"Robust median of pairwise squared distances from a subsample.\"\"\"\n",
        "    n = Z.shape[0]\n",
        "    if n <= 1:\n",
        "        return xp.array(1.0, dtype=Z.dtype)\n",
        "    idx = xp.random.permutation(n)[:min(n, max_samples)]\n",
        "    S = Z[idx]\n",
        "    # pairwise squared distances for the subsample\n",
        "    S2 = xp.sum(S * S, axis=1, keepdims=True)\n",
        "    D2 = xp.maximum(S2 + S2.T - 2 * (S @ S.T), 0.0)\n",
        "    # exclude zeros on the diagonal\n",
        "    tri = D2[~xp.eye(D2.shape[0], dtype=bool)]\n",
        "    if tri.size == 0:\n",
        "        return xp.array(1.0, dtype=Z.dtype)\n",
        "    return xp.median(tri)\n",
        "\n",
        "def _kernel_from_d2(d2, kernel, scale, xp, beta):\n",
        "    \"\"\"Evaluate kernel from squared distances.\"\"\"\n",
        "    if kernel == \"rbf\":\n",
        "        # k(d2) = exp(- d2 / (2 sigma^2))\n",
        "        inv2sig2 = 1.0 / (2.0 * (scale ** 2))\n",
        "        return xp.exp(-d2 * inv2sig2)\n",
        "    elif kernel == \"imq\":\n",
        "        # k(d2) = (1 + d2 / c^2)^(-beta)\n",
        "        return (1.0 + d2 / (scale ** 2)) ** (-beta)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown kernel '{kernel}' (use 'rbf' or 'imq').\")\n",
        "\n",
        "def _sum_kernel_pairs_XY(X, Y, scales, weights, kernel, xp, beta, chunk, xw=None, yw=None):\n",
        "    \"\"\"\n",
        "    Sum over k(x_i, y_j) for all i,j across a *mixture* of kernels.\n",
        "    Returns per-scale sums as array [S] and normalization constant (weighted).\n",
        "    \"\"\"\n",
        "    nx, ny = X.shape[0], Y.shape[0]\n",
        "    S = xp.zeros((len(scales),), dtype=X.dtype)\n",
        "    # normalization (weighted or unweighted)\n",
        "    if xw is None and yw is None:\n",
        "        norm = float(nx) * float(ny)\n",
        "    else:\n",
        "        xw = xp.ones(nx, dtype=X.dtype) if xw is None else xw\n",
        "        yw = xp.ones(ny, dtype=Y.dtype) if yw is None else yw\n",
        "        norm = float(xp.sum(xw) * xp.sum(yw))\n",
        "\n",
        "    for i0 in range(0, nx, chunk):\n",
        "        Xi = X[i0:i0 + chunk]\n",
        "        # precompute norms for distance\n",
        "        Xi2 = xp.sum(Xi * Xi, axis=1, keepdims=True)  # (bi,1)\n",
        "        wi = None if xw is None else xw[i0:i0 + chunk]\n",
        "        for j0 in range(0, ny, chunk):\n",
        "            Yj = Y[j0:j0 + chunk]\n",
        "            Yj2 = xp.sum(Yj * Yj, axis=1, keepdims=True)  # (bj,1)\n",
        "            # (bi,bj) squared distances\n",
        "            D2 = xp.maximum(Xi2 + Yj2.T - 2.0 * (Xi @ Yj.T), 0.0)\n",
        "            # stack per-scale kernels with broadcasting: (S, bi, bj)\n",
        "            Ks = xp.stack([_kernel_from_d2(D2, kernel, s, xp, beta) for s in scales], axis=0)\n",
        "            if xw is None and yw is None:\n",
        "                S += Ks.sum(axis=(1, 2))\n",
        "            else:\n",
        "                wj = yw[j0:j0 + chunk]\n",
        "                W = (wi[:, None] * wj[None, :])  # (bi,bj)\n",
        "                S += xp.tensordot(Ks, W, axes=((1, 2), (0, 1)))  # sum_{bi,bj} Ks[...,bi,bj]*W[bi,bj]\n",
        "    # apply mixture weights here (leave per-scale sums unweighted for diagnostics)\n",
        "    if weights is not None:\n",
        "        total = float(xp.sum(weights * (S / norm)))\n",
        "    else:\n",
        "        total = float(xp.sum(S / norm))\n",
        "    return S, norm, total\n",
        "\n",
        "def _sum_kernel_pairs_XX(A, scales, weights, kernel, xp, beta, chunk, unbiased=True, aw=None):\n",
        "    \"\"\"\n",
        "    Sum over k(a_i, a_j) across a *mixture* of kernels for a single set A.\n",
        "    Returns per-scale sums as array [S] and normalization constant (weighted).\n",
        "    For unbiased=True, excludes diagonals (i != j) using ordered-pairs normalization n(n-1).\n",
        "    \"\"\"\n",
        "    n = A.shape[0]\n",
        "    S = xp.zeros((len(scales),), dtype=A.dtype)\n",
        "\n",
        "    if aw is None:\n",
        "        if unbiased:\n",
        "            norm = float(n) * float(max(n - 1, 1))\n",
        "        else:\n",
        "            norm = float(n) * float(n)\n",
        "    else:\n",
        "        aw = aw.astype(A.dtype, copy=False)\n",
        "        s1 = xp.sum(aw)\n",
        "        if unbiased:\n",
        "            norm = float(s1 * s1 - xp.sum(aw * aw))\n",
        "        else:\n",
        "            norm = float(s1 * s1)\n",
        "\n",
        "    for i0 in range(0, n, chunk):\n",
        "        Ai = A[i0:i0 + chunk]\n",
        "        Ai2 = xp.sum(Ai * Ai, axis=1, keepdims=True)\n",
        "        wi = None if aw is None else aw[i0:i0 + chunk]\n",
        "        j_start = i0  # upper blocks including diagonal\n",
        "        for j0 in range(j_start, n, chunk):\n",
        "            Aj = A[j0:j0 + chunk]\n",
        "            Aj2 = xp.sum(Aj * Aj, axis=1, keepdims=True)\n",
        "            D2 = xp.maximum(Ai2 + Aj2.T - 2.0 * (Ai @ Aj.T), 0.0)\n",
        "            Ks = xp.stack([_kernel_from_d2(D2, kernel, s, xp, beta) for s in scales], axis=0)  # (S,bi,bj)\n",
        "\n",
        "            if aw is None:\n",
        "                if i0 == j0:\n",
        "                    if unbiased:\n",
        "                        # subtract diagonals; keep off-diagonals both (i,j) and (j,i) inside the block\n",
        "                        diag = xp.stack([xp.diag(Ks[k]) for k in range(Ks.shape[0])], axis=0).sum(axis=1)\n",
        "                        S += Ks.sum(axis=(1, 2)) - diag\n",
        "                    else:\n",
        "                        S += Ks.sum(axis=(1, 2))\n",
        "                else:\n",
        "                    # off-diagonal blocks contribute both (i,j) and (j,i)\n",
        "                    S += 2.0 * Ks.sum(axis=(1, 2))\n",
        "            else:\n",
        "                wj = aw[j0:j0 + chunk]\n",
        "                W = (wi[:, None] * wj[None, :])  # (bi,bj)\n",
        "                if i0 == j0:\n",
        "                    if unbiased:\n",
        "                        # zero-out diagonal weights before summing\n",
        "                        # subtract weighted diagonal: sum_k Ks[k]*W with diag set to 0\n",
        "                        # Equivalent: total sum minus diag(K)*w^2\n",
        "                        diag = xp.stack([xp.diag(Ks[k]) for k in range(Ks.shape[0])], axis=0)  # (S, b)\n",
        "                        diag_w2 = (wi * wi)[None, :] * diag  # (S,b)\n",
        "                        S += xp.tensordot(Ks, W, axes=((1, 2), (0, 1))) - diag_w2.sum(axis=1)\n",
        "                    else:\n",
        "                        S += xp.tensordot(Ks, W, axes=((1, 2), (0, 1)))\n",
        "                else:\n",
        "                    # both (i,j) and (j,i) pairs\n",
        "                    S += 2.0 * xp.tensordot(Ks, W, axes=((1, 2), (0, 1)))\n",
        "    if weights is not None:\n",
        "        total = float(xp.sum(weights * (S / norm)))\n",
        "    else:\n",
        "        total = float(xp.sum(S / norm))\n",
        "    return S, norm, total\n",
        "\n",
        "def M_MMD(X, Y, params=None):\n",
        "    \"\"\"\n",
        "    Multi-scale MMD divergence between sample sets X ~ P and Y ~ Q.\n",
        "\n",
        "    Args:\n",
        "        X: (N_x, d) numpy or cupy array.\n",
        "        Y: (N_y, d) numpy or cupy array.\n",
        "        params: dict overriding M_MMD_DEFAULTS.\n",
        "            - kernel: 'rbf' or 'imq'\n",
        "            - scales: list/array of bandwidths (RBF sigmas or IMQ c's). If None, automatic.\n",
        "            - scale_weights: list/array (same length as scales). Defaults to uniform.\n",
        "            - use_median_heuristic: if True and scales=None, set base scale from combined median distance.\n",
        "                * RBF: base sigma = sqrt(0.5 * median(D^2)), then logspace multipliers [scale_min, scale_max]\n",
        "                * IMQ: base c = sqrt(median(D^2)), then logspace multipliers\n",
        "            - num_scales, scale_min, scale_max: for automatic scales (logspace).\n",
        "            - unbiased: U-statistic (exclude diagonals in within terms).\n",
        "            - squared: return MMD^2 if True; else return sqrt(max(MMD^2, 0)).\n",
        "            - imq_beta: exponent for IMQ kernel.\n",
        "            - chunk_size: block size for memory control.\n",
        "            - x_weights, y_weights: optional per-sample nonnegative weights.\n",
        "            - return_per_scale: if True, also returns a dict with per-scale pieces.\n",
        "\n",
        "    Returns:\n",
        "        mmd_value (float), and optionally (if return_per_scale) a dict with\n",
        "        'per_scale_mmd2', 'scales', 'scale_weights'.\n",
        "    \"\"\"\n",
        "    # merge params with defaults\n",
        "    p = dict(M_MMD_DEFAULTS)\n",
        "    if params is not None:\n",
        "        p.update(params)\n",
        "\n",
        "    # choose backend\n",
        "    xp = _xp_of(X)\n",
        "    X = _as_xp(X, xp)\n",
        "    Y = _as_xp(Y, xp)\n",
        "    X = X.reshape(X.shape[0], -1)\n",
        "    Y = Y.reshape(Y.shape[0], -1)\n",
        "\n",
        "    # weights (optional)\n",
        "    xw = None if p[\"x_weights\"] is None else _as_xp(p[\"x_weights\"], xp).astype(X.dtype, copy=False)\n",
        "    yw = None if p[\"y_weights\"] is None else _as_xp(p[\"y_weights\"], xp).astype(Y.dtype, copy=False)\n",
        "\n",
        "    # build scales & weights\n",
        "    if p[\"scales\"] is None:\n",
        "        # base scale from combined median distance\n",
        "        Z = xp.concatenate([X, Y], axis=0)\n",
        "        med2 = _median_pairwise_sqdist(Z, xp, max_samples=1024)\n",
        "        if p[\"kernel\"] == \"rbf\":\n",
        "            base = xp.sqrt(xp.maximum(0.5 * med2, 1e-12))\n",
        "        else:  # imq\n",
        "            base = xp.sqrt(xp.maximum(med2, 1e-12))\n",
        "        mul = xp.logspace(np.log10(p[\"scale_min\"]), np.log10(p[\"scale_max\"]), p[\"num_scales\"])\n",
        "        scales = base * mul\n",
        "    else:\n",
        "        scales = _as_xp(p[\"scales\"], xp).astype(X.dtype, copy=False)\n",
        "\n",
        "    if p[\"scale_weights\"] is None:\n",
        "        scale_weights = xp.ones((len(scales),), dtype=X.dtype) / float(len(scales))\n",
        "    else:\n",
        "        w = _as_xp(p[\"scale_weights\"], xp).astype(X.dtype, copy=False)\n",
        "        w_sum = xp.sum(w)\n",
        "        scale_weights = w / (w_sum if float(w_sum) > 0 else 1.0)\n",
        "\n",
        "    # compute per-set & cross sums, with mixture applied after normalization\n",
        "    Sxx, nxx, mix_xx = _sum_kernel_pairs_XX(X, scales, scale_weights, p[\"kernel\"], xp,\n",
        "                                            p[\"imq_beta\"], p[\"chunk_size\"], p[\"unbiased\"], xw)\n",
        "    Syy, nyy, mix_yy = _sum_kernel_pairs_XX(Y, scales, scale_weights, p[\"kernel\"], xp,\n",
        "                                            p[\"imq_beta\"], p[\"chunk_size\"], p[\"unbiased\"], yw)\n",
        "    Sxy, nxy, mix_xy = _sum_kernel_pairs_XY(X, Y, scales, scale_weights, p[\"kernel\"], xp,\n",
        "                                            p[\"imq_beta\"], p[\"chunk_size\"], xw, yw)\n",
        "\n",
        "    # Full mixture MMD^2\n",
        "    mmd2 = (mix_xx + mix_yy - 2.0 * mix_xy)\n",
        "    # numerical guard\n",
        "    if mmd2 < 0.0:\n",
        "        mmd2 = 0.0\n",
        "\n",
        "    if p[\"squared\"]:\n",
        "        out = float(mmd2)\n",
        "    else:\n",
        "        out = float(np.sqrt(mmd2))  # safe cast; cp scalar ok through numpy sqrt of float\n",
        "\n",
        "    if p[\"return_per_scale\"]:\n",
        "        # per-scale contributions *before* mixture weighting, but normalized\n",
        "        per_scale = (Sxx / nxx) + (Syy / nyy) - 2.0 * (Sxy / nxy)\n",
        "        # clamp small negatives\n",
        "        per_scale = xp.maximum(per_scale, 0.0)\n",
        "        return out, {\n",
        "            \"per_scale_mmd2\": _as_xp(per_scale, np),  # move to numpy for easy logging\n",
        "            \"scales\": _as_xp(scales, np),\n",
        "            \"scale_weights\": _as_xp(scale_weights, np),\n",
        "        }\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------ Single-σ MMD ----------------------------------\n",
        "def OT_MMD(X, Y, *, sigma=0.5, unbiased=False, chunk=4096):\n",
        "    \"\"\"\n",
        "    Single-scale RBF MMD between X and Y with bandwidth `sigma`.\n",
        "    Works with NumPy or CuPy arrays; keeps everything on-device if CuPy.\n",
        "    \"\"\"\n",
        "    # choose xp\n",
        "    xp = cp if (cp is not None and isinstance(X, cp.ndarray)) or \\\n",
        "               (cp is not None and isinstance(Y, cp.ndarray)) else np\n",
        "    X = xp.asarray(X, dtype=xp.float64).reshape(X.shape[0], -1)\n",
        "    Y = xp.asarray(Y, dtype=xp.float64).reshape(Y.shape[0], -1)\n",
        "    n, m = X.shape[0], Y.shape[0]\n",
        "    inv2s2 = 1.0 / (2.0 * (float(sigma) ** 2))\n",
        "\n",
        "    def _rbf_sum(A, B):\n",
        "        SA = xp.sum(A * A, axis=1, keepdims=True)\n",
        "        SB = xp.sum(B * B, axis=1, keepdims=True)\n",
        "        D2 = xp.maximum(SA + SB.T - 2.0 * (A @ B.T), 0.0)\n",
        "        return xp.exp(-inv2s2 * D2).sum()\n",
        "\n",
        "    if unbiased:\n",
        "        # U-statistic: exclude self terms in Kxx, Kyy, normalize by n(n-1), m(m-1)\n",
        "        # Block to limit memory\n",
        "        def _self_sum(A):\n",
        "            s = xp.array(0.0, dtype=xp.float64)\n",
        "            NA = A.shape[0]\n",
        "            for i0 in range(0, NA, chunk):\n",
        "                Ai = A[i0:i0 + chunk]\n",
        "                Ai2 = xp.sum(Ai * Ai, axis=1, keepdims=True)\n",
        "                for j0 in range(0, NA, chunk):\n",
        "                    Aj = A[j0:j0 + chunk]\n",
        "                    Aj2 = xp.sum(Aj * Aj, axis=1, keepdims=True)\n",
        "                    D2 = xp.maximum(Ai2 + Aj2.T - 2.0 * (Ai @ Aj.T), 0.0)\n",
        "                    K = xp.exp(-inv2s2 * D2)\n",
        "                    if i0 == j0:\n",
        "                        K = K - xp.eye(K.shape[0], dtype=K.dtype)\n",
        "                    s += K.sum()\n",
        "            return s\n",
        "\n",
        "        s_xx = _self_sum(X) / max(n * (n - 1), 1)\n",
        "        s_yy = _self_sum(Y) / max(m * (m - 1), 1)\n",
        "\n",
        "    else:\n",
        "        # Biased estimator: include diagonals, normalize by n^2, m^2\n",
        "        s_xx = _rbf_sum(X, X) / max(n * n, 1)\n",
        "        s_yy = _rbf_sum(Y, Y) / max(m * m, 1)\n",
        "\n",
        "    s_xy = _rbf_sum(X, Y) / max(n * m, 1)\n",
        "    mmd2 = float((s_xx + s_yy - 2.0 * s_xy))\n",
        "    return math.sqrt(mmd2) if mmd2 > 0 else 0.0\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def _pairwise_min_k_dists(A, B, k=1, chunk=2000):\n",
        "    \"\"\"\n",
        "    For each a in A, return distance to k-th NN in B (Euclidean).\n",
        "    Chunked to stay memory-safe. Returns shape [len(A),].\n",
        "    \"\"\"\n",
        "    nA, nB = A.shape[0], B.shape[0]\n",
        "    out = np.empty(nA, dtype=np.float64)\n",
        "    k = max(1, int(k))\n",
        "\n",
        "    for i in range(0, nA, chunk):\n",
        "        Ai = A[i:i+chunk]  # [m, d]\n",
        "        # pairwise squared distances (Ai - B)^2\n",
        "        # (m,1,d) - (1,nB,d) -> (m,nB,d) -> sum over d -> (m,nB)\n",
        "        d2 = np.sum((Ai[:, None, :] - B[None, :, :])**2, axis=2)\n",
        "        # sort small k efficiently\n",
        "        # np.partition is O(n), returns k smallest unordered; take [:, k-1]\n",
        "        kth = np.partition(d2, kth=k-1, axis=1)[:, k-1]\n",
        "        out[i:i+chunk] = np.sqrt(kth)\n",
        "    return out\n",
        "\n",
        "def copying_index(gen, train, test, k=1, alphas=(0.9, 0.8), max_pts=8000, chunk=2000):\n",
        "    \"\"\"\n",
        "    Train-vs-Test kNN ratio memorization metric.\n",
        "    Returns dict with ratio stats and alpha-thresholded fractions.\n",
        "    Downsamples to max_pts if needed for speed.\n",
        "    \"\"\"\n",
        "    # (Optional) downsample for speed\n",
        "    if gen.shape[0] > max_pts:\n",
        "        idx = np.random.permutation(gen.shape[0])[:max_pts]\n",
        "        gen = gen[idx]\n",
        "    if train.shape[0] > max_pts:\n",
        "        idx = np.random.permutation(train.shape[0])[:max_pts]\n",
        "        train = train[idx]\n",
        "    if test.shape[0] > max_pts:\n",
        "        idx = np.random.permutation(test.shape[0])[:max_pts]\n",
        "        test = test[idx]\n",
        "\n",
        "    dg_train = _pairwise_min_k_dists(gen, train, k=k, chunk=chunk)\n",
        "    dg_test  = _pairwise_min_k_dists(gen, test,  k=k, chunk=chunk)\n",
        "\n",
        "    eps = 1e-12\n",
        "    ratios = dg_train / (dg_test + eps)  # <1 => closer to TRAIN\n",
        "    out = {\n",
        "        \"copy_ratio_mean\": float(np.mean(ratios)),\n",
        "        \"copy_ratio_median\": float(np.median(ratios)),\n",
        "    }\n",
        "    for a in alphas:\n",
        "        out[f\"copy_frac_a{str(a).replace('.','')}\"] = float(np.mean(ratios < a))\n",
        "    return out\n",
        "\n",
        "def duplicate_rate(gen, eps=1e-5, max_pts=10000, chunk=2000):\n",
        "    \"\"\"\n",
        "    Near-duplicate rate inside generated samples (birthday test).\n",
        "    Fraction of points whose 2nd-NN distance (exclude self) < eps.\n",
        "    \"\"\"\n",
        "    if gen.shape[0] > max_pts:\n",
        "        idx = np.random.permutation(gen.shape[0])[:max_pts]\n",
        "        gen = gen[idx]\n",
        "    # distance to 2nd NN: compute to B=A; the 1st NN is itself at 0\n",
        "    d2 = []\n",
        "    for i in range(0, gen.shape[0], chunk):\n",
        "        Ai = gen[i:i+chunk]\n",
        "        dist2 = np.sum((Ai[:, None, :] - gen[None, :, :])**2, axis=2)\n",
        "        # set self-distances to +inf to ignore true diagonal\n",
        "        rows = np.arange(dist2.shape[0])\n",
        "        dist2[rows, i + rows] = np.inf\n",
        "        # take the minimum distance (actual 1st NN excluding self)\n",
        "        nn1 = np.min(dist2, axis=1)\n",
        "        d2.append(nn1)\n",
        "    nn = np.sqrt(np.concatenate(d2, axis=0))\n",
        "    return float(np.mean(nn < eps))\n",
        "\n",
        "\n",
        "\n",
        "# Script One, works well, just fits np estimators uses for sampling\n",
        "\n",
        "\n",
        "def _to_numpy(arr):\n",
        "    \"\"\"Return a NumPy view; copy from GPU to CPU if the input is a CuPy array.\"\"\"\n",
        "    try:\n",
        "        import cupy as cp\n",
        "        if isinstance(arr, cp.ndarray):\n",
        "            return arr.get()          # move to host\n",
        "    except ImportError:\n",
        "        pass                          # no CuPy in this env\n",
        "    return np.asarray(arr)\n",
        "\n",
        "\n",
        "def prune_cp_arr(x_cp, p_percent = 1):\n",
        "    x = cp.asarray(x_cp)\n",
        "    if x.ndim != 2:\n",
        "        raise ValueError(\"expected a 2D CuPy array (N,d)\")\n",
        "    if not (0 <= p_percent <= 100):\n",
        "        raise ValueError(\"p_percent must be in [0, 100]\")\n",
        "\n",
        "    N = x.shape[0]\n",
        "    if N == 0:\n",
        "        return x, cp.zeros(0, dtype=cp.bool_)\n",
        "\n",
        "    finite_rows = cp.isfinite(x).all(axis=1)\n",
        "    idx_finite = cp.where(finite_rows)[0]\n",
        "    xf = x[finite_rows]\n",
        "    M = xf.shape[0]\n",
        "\n",
        "    if M == 0:\n",
        "        return x[:0], cp.zeros(N, dtype=cp.bool_)\n",
        "\n",
        "    if p_percent == 0:\n",
        "        keep_mask = cp.zeros(N, dtype=cp.bool_)\n",
        "        keep_mask[idx_finite] = True\n",
        "        return xf, keep_mask\n",
        "    if p_percent == 100:\n",
        "        return x[:0], cp.zeros(N, dtype=cp.bool_)\n",
        "\n",
        "    k = int(cp.ceil(M * (1.0 - p_percent / 100.0)).item())  # rows to keep among finite ones\n",
        "    if k <= 0:\n",
        "        return x[:0], cp.zeros(N, dtype=cp.bool_)\n",
        "\n",
        "    norms = cp.linalg.norm(xf, axis=1)\n",
        "    keep_local = cp.argpartition(norms, kth=k-1)[:k]  # indices of k smallest norms in xf\n",
        "\n",
        "    keep_mask = cp.zeros(N, dtype=cp.bool_)\n",
        "    keep_mask[idx_finite[keep_local]] = True\n",
        "    return x[keep_mask], keep_mask\n",
        "\n",
        "\n",
        "import cupy as cp\n",
        "\n",
        "\n",
        "\n",
        "def sample_gmm_gpu(n_samples, means, stds, weights, *, seed=None):\n",
        "    xp = cp\n",
        "    means   = xp.asarray(means,   dtype=xp.float64)\n",
        "    stds    = xp.asarray(stds,    dtype=xp.float64)\n",
        "    weights = xp.asarray(weights, dtype=xp.float64).reshape(-1)\n",
        "\n",
        "    C, D = means.shape\n",
        "    if stds.ndim == 1 and stds.shape == (C,):\n",
        "        cov_mode = \"isotropic\"\n",
        "    elif stds.ndim == 2 and stds.shape == (C, D):\n",
        "        cov_mode = \"diag\"\n",
        "    elif stds.ndim == 3 and stds.shape == (C, D, D):\n",
        "        cov_mode = \"full\"\n",
        "    else:\n",
        "        raise ValueError(\"stds must be (C,), (C,D), or (C,D,D)\")\n",
        "\n",
        "    probs = (weights / float(weights.sum())).ravel()\n",
        "    rng = xp.random.RandomState(seed) if seed is not None else xp.random\n",
        "    choices = rng.choice(C, size=int(n_samples), p=cp.asnumpy(probs))\n",
        "    out = xp.empty((n_samples, D), dtype=xp.float64)\n",
        "\n",
        "    for i in range(C):\n",
        "        mask = (choices == i); cnt = int(mask.sum())\n",
        "        if not cnt: continue\n",
        "        z = rng.standard_normal((cnt, D), dtype=xp.float64)\n",
        "        if cov_mode == \"isotropic\":\n",
        "            out[mask] = means[i] + z * stds[i]\n",
        "        elif cov_mode == \"diag\":\n",
        "            out[mask] = means[i] + z * stds[i]          # (cnt,D) * (D,)\n",
        "        else:  # full\n",
        "            L = xp.linalg.cholesky(stds[i])             # Σ^{1/2}\n",
        "            out[mask] = means[i] + z @ L.T\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "def score_gmm_gpu(x, means, stds, weights, batch_size=4096):\n",
        "    \"\"\"\n",
        "    ∇_x log ∑_c w_c N(x; μ_c, Σ_c)\n",
        "    stds:\n",
        "      - (C,)   : isotropic std per comp  (σ_c)          -> Σ_c = σ_c^2 I\n",
        "      - (C,D)  : diagonal std per comp   (σ_{c,d})      -> Σ_c = diag(σ^2)\n",
        "      - (C,D,D): full covariance per comp (Σ_c SPD)\n",
        "    \"\"\"\n",
        "    xp = cp\n",
        "    x       = xp.asarray(x,       dtype=xp.float64)\n",
        "    means   = xp.asarray(means,   dtype=xp.float64)\n",
        "    weights = xp.asarray(weights, dtype=xp.float64).reshape(-1)\n",
        "    stds    = xp.asarray(stds,    dtype=xp.float64)\n",
        "\n",
        "    N, D = x.shape\n",
        "    C    = means.shape[0]\n",
        "    if N == 0:\n",
        "        return xp.zeros((0, D), dtype=xp.float64)\n",
        "\n",
        "    const = -0.5 * D * xp.log(2.0 * xp.pi)\n",
        "    log_w = xp.log(weights + 1e-300)\n",
        "    out   = xp.zeros_like(x)\n",
        "\n",
        "    # ----- (1) Isotropic: stds = (C,)\n",
        "    if stds.ndim == 1:\n",
        "        inv_vars     = 1.0 / (stds**2)                 # (C,)\n",
        "        log_det_vars = D * xp.log(stds**2)             # (C,)\n",
        "        norm_const   = const - 0.5 * log_det_vars      # (C,)\n",
        "\n",
        "        for i in range(0, N, batch_size):\n",
        "            xb   = x[i:i+batch_size]                   # (B,D)\n",
        "            diff = means[None, :, :] - xb[:, None, :]  # (B,C,D)\n",
        "            quad = (diff**2).sum(axis=-1) * inv_vars[None, :]         # (B,C)\n",
        "\n",
        "            logp = log_w[None, :] + norm_const[None, :] - 0.5 * quad  # (B,C)\n",
        "            m    = xp.max(logp, axis=1, keepdims=True)\n",
        "            resp = xp.exp(logp - m); resp /= resp.sum(axis=1, keepdims=True) + 1e-300\n",
        "\n",
        "            contrib = (resp[:, :, None] * diff) * inv_vars[None, :, None]   # (B,C,D)\n",
        "            out[i:i+batch_size] = contrib.sum(axis=1)\n",
        "\n",
        "        return out\n",
        "\n",
        "    # ----- (2) Diagonal: stds = (C,D)\n",
        "    if stds.ndim == 2 and stds.shape[1] == D:\n",
        "        inv_vars     = 1.0 / (stds**2)                 # (C,D)\n",
        "        log_det_vars = xp.log(stds**2).sum(axis=1)     # (C,)\n",
        "        norm_const   = const - 0.5 * log_det_vars      # (C,)\n",
        "\n",
        "        for i in range(0, N, batch_size):\n",
        "            xb   = x[i:i+batch_size]                   # (B,D)\n",
        "            diff = means[None, :, :] - xb[:, None, :]  # (B,C,D)\n",
        "            quad = ((diff**2) * inv_vars[None, :, :]).sum(axis=-1)          # (B,C)\n",
        "\n",
        "            logp = log_w[None, :] + norm_const[None, :] - 0.5 * quad        # (B,C)\n",
        "            m    = xp.max(logp, axis=1, keepdims=True)\n",
        "            resp = xp.exp(logp - m); resp /= resp.sum(axis=1, keepdims=True) + 1e-300\n",
        "\n",
        "            contrib = (resp[:, :, None] * diff) * inv_vars[None, :, :]      # (B,C,D)\n",
        "            out[i:i+batch_size] = contrib.sum(axis=1)\n",
        "\n",
        "        return out\n",
        "\n",
        "    # ----- (3) Full: stds = (C,D,D)  (here 'stds' is Σ_c)\n",
        "    if stds.ndim == 3 and stds.shape[1] == D and stds.shape[2] == D:\n",
        "        # Cholesky once per component\n",
        "        Ls      = xp.linalg.cholesky(stds)                                     # (C,D,D), Σ = L L^T\n",
        "        log_det = 2.0 * xp.log(xp.diagonal(Ls, axis1=1, axis2=2)).sum(axis=1)  # (C,)\n",
        "        norm_const = const - 0.5 * log_det                                     # (C,)\n",
        "\n",
        "        for i in range(0, N, batch_size):\n",
        "            xb    = x[i:i+batch_size]                         # (B,D)\n",
        "            B     = xb.shape[0]\n",
        "            logp  = xp.empty((B, C), dtype=xp.float64)\n",
        "            grads = xp.empty((B, C, D), dtype=xp.float64)\n",
        "\n",
        "            for c in range(C):\n",
        "                L = Ls[c]                                      # (D,D)\n",
        "\n",
        "                # quad term: (x - μ)^T Σ^{-1} (x - μ) = || L^{-1}(x-μ) ||^2\n",
        "                diff_x = xb - means[c][None, :]                # (B,D)\n",
        "                v = xp.linalg.solve(L, diff_x.T)               # (D,B)\n",
        "                quad = xp.sum(v * v, axis=0)                   # (B,)\n",
        "                logp[:, c] = log_w[c] + norm_const[c] - 0.5 * quad\n",
        "\n",
        "                # gradient per comp: Σ^{-1}(μ - x) = L^{-T} L^{-1} (μ - x)\n",
        "                diff_m = means[c][None, :] - xb                # (B,D)\n",
        "                y = xp.linalg.solve(L, diff_m.T)               # (D,B)\n",
        "                g = xp.linalg.solve(L.T, y).T                  # (B,D)\n",
        "                grads[:, c, :] = g\n",
        "\n",
        "            m    = xp.max(logp, axis=1, keepdims=True)\n",
        "            resp = xp.exp(logp - m); resp /= resp.sum(axis=1, keepdims=True) + 1e-300\n",
        "\n",
        "            out[i:i+batch_size] = xp.einsum(\"bc,bcd->bd\", resp, grads)\n",
        "\n",
        "        return out\n",
        "\n",
        "    raise ValueError(\"stds must be (C,), (C,D), or (C,D,D) (the last is full covariances).\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def score_div_gmm(x, means, covs, weights, batch_size=4096):\n",
        "\n",
        "    xp = cp\n",
        "    x       = xp.asarray(x, dtype=xp.float64)\n",
        "    means   = xp.asarray(means, dtype=xp.float64)\n",
        "    covs    = xp.asarray(covs,  dtype=xp.float64)\n",
        "    weights = xp.asarray(weights, dtype=xp.float64).reshape(-1)\n",
        "\n",
        "    N, D = x.shape\n",
        "    C    = means.shape[0]\n",
        "    if N == 0:\n",
        "        return xp.zeros_like(x), xp.zeros((0,), dtype=xp.float64)\n",
        "\n",
        "    # --- weight normalization & constants\n",
        "    wsum = xp.sum(weights)\n",
        "    if not xp.isfinite(wsum) or wsum <= 0:\n",
        "        raise ValueError(\"Mixture weights must sum to a positive value.\")\n",
        "    log_w = xp.log(weights / wsum + 1e-300)\n",
        "    const = -0.5 * D * xp.log(2.0 * xp.pi)\n",
        "\n",
        "    # --- covariance handling\n",
        "    kind = None\n",
        "    if covs.ndim == 1 and covs.shape[0] == C:             # isotropic stds\n",
        "        kind     = \"iso\"\n",
        "        inv_vars = 1.0 / (covs**2)                         # (C,)\n",
        "        log_det  = D * xp.log(covs**2)                     # (C,)\n",
        "        trA      = D * inv_vars                            # (C,)\n",
        "    elif covs.ndim == 2 and covs.shape == (C, D):          # diagonal stds\n",
        "        kind     = \"diag\"\n",
        "        inv_vars = 1.0 / (covs**2)                         # (C,D)\n",
        "        log_det  = xp.log(covs**2).sum(axis=1)             # (C,)\n",
        "        trA      = inv_vars.sum(axis=1)                    # (C,)\n",
        "    elif covs.ndim == 3 and covs.shape[1:] == (D, D):      # full Σ\n",
        "        kind = \"full\"\n",
        "        Ls, log_det, trA = [], xp.empty(C), xp.empty(C)\n",
        "        I = xp.eye(D, dtype=xp.float64)\n",
        "        for c in range(C):\n",
        "            L = xp.linalg.cholesky(covs[c])\n",
        "            Ls.append(L)\n",
        "            log_det[c] = 2.0 * xp.sum(xp.log(xp.diag(L)))\n",
        "            Z = xp.linalg.solve(L, I)                      # L^{-1}\n",
        "            trA[c] = xp.sum(Z * Z)                         # ||L^{-1}||_F^2 = tr(Σ^{-1})\n",
        "    else:\n",
        "        raise ValueError(\"covs must be (C,), (C,D) with stds, or (C,D,D) with full covariances.\")\n",
        "\n",
        "    # --- outputs\n",
        "    score = xp.empty_like(x)\n",
        "    div   = xp.empty((N,), dtype=xp.float64)\n",
        "\n",
        "    for i in range(0, N, batch_size):\n",
        "        xb = x[i:i+batch_size]                             # (B,D)\n",
        "        B  = xb.shape[0]\n",
        "\n",
        "        # per-component log-densities and a_c = A_c (mu_c - x)\n",
        "        if kind in (\"iso\", \"diag\"):\n",
        "            diff = means[None, :, :] - xb[:, None, :]      # (B,C,D)\n",
        "            if kind == \"iso\":\n",
        "                quad = (diff**2).sum(-1) * inv_vars[None, :]          # (B,C)\n",
        "                a    = diff * inv_vars[None, :, None]                  # (B,C,D)\n",
        "            else:\n",
        "                quad = ((diff**2) * inv_vars[None, :, :]).sum(-1)      # (B,C)\n",
        "                a    = diff * inv_vars[None, :, :]                     # (B,C,D)\n",
        "        else:\n",
        "            quad = xp.empty((B, C), dtype=xp.float64)\n",
        "            a    = xp.empty((B, C, D), dtype=xp.float64)\n",
        "            for c in range(C):\n",
        "                diff = (means[c][None, :] - xb)                        # (B,D)\n",
        "                # a_c = Σ_c^{-1} diff via two triangular solves\n",
        "                z = xp.linalg.solve(Ls[c], diff.T)                     # (D,B)\n",
        "                u = xp.linalg.solve(Ls[c].T, z)                        # (D,B)\n",
        "                a_c = u.T                                              # (B,D)\n",
        "                a[:, c, :] = a_c\n",
        "                quad[:, c] = xp.sum(diff * a_c, axis=1)                # diff^T Σ^{-1} diff\n",
        "\n",
        "        log_comp = log_w[None, :] + const - 0.5*log_det[None, :] - 0.5*quad  # (B,C)\n",
        "        m   = xp.max(log_comp, axis=1, keepdims=True)\n",
        "        resp = xp.exp(log_comp - m)\n",
        "        resp /= (resp.sum(axis=1, keepdims=True) + 1e-300)             # (B,C)\n",
        "        s = xp.sum(resp[:, :, None] * a, axis=1)                        # (B,D)\n",
        "\n",
        "        a_norm2 = xp.sum(a * a, axis=2)                                 # (B,C)\n",
        "        s_dot_a = xp.sum(s[:, None, :] * a, axis=2)                     # (B,C)\n",
        "        div_b   = xp.sum(resp * (a_norm2 - s_dot_a - trA[None, :]), axis=1)  # (B,)\n",
        "\n",
        "        score[i:i+B] = s\n",
        "        div[i:i+B]   = div_b\n",
        "\n",
        "    return score, div\n",
        "\n",
        "\n",
        "def hyvarinen_to_target(X, score_fn, div_fn, batch_size=65536):\n",
        "    \"\"\"\n",
        "    Hyvärinen-to-target loss: E[ 0.5||s(x)||^2 + div s(x) ].\n",
        "    `score_fn` returns (N,D), `div_fn` returns (N,).\n",
        "    \"\"\"\n",
        "    xp = cp\n",
        "    X  = xp.asarray(X, dtype=xp.float64)\n",
        "    N  = X.shape[0]\n",
        "    acc = 0.0\n",
        "    cnt = 0\n",
        "    for i in range(0, N, int(batch_size)):\n",
        "        xb = X[i:i+batch_size]\n",
        "        s  = score_fn(xb)\n",
        "        dv = div_fn(xb)\n",
        "        acc += float(xp.mean(0.5 * xp.sum(s*s, axis=1) + dv))\n",
        "        cnt += 1\n",
        "    return acc / max(cnt, 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def density_gmm_gpu(x, means, stds, weights, mode=\"absolute\", iso_std=1.0, batch_size=4096):\n",
        "    xp = cp\n",
        "    x      = x.astype(xp.float64, copy=False)\n",
        "    means  = means.astype(xp.float64, copy=False)\n",
        "    stds   = stds.astype(xp.float64, copy=False)\n",
        "    weights= weights.astype(xp.float64, copy=False)\n",
        "\n",
        "    N, D = x.shape\n",
        "    if N == 0:\n",
        "        return xp.zeros((0,), dtype=xp.float64)\n",
        "\n",
        "    # Normalize mixture weights safely\n",
        "    wsum = weights.sum()\n",
        "    if wsum <= 0:\n",
        "        raise ValueError(\"Mixture weights must sum to a positive value.\")\n",
        "    w = weights / wsum\n",
        "    log_w = xp.log(w + 1e-40)  # [C]\n",
        "\n",
        "    # Handle variances and normalization constants\n",
        "    if stds.ndim == 1:\n",
        "        # Isotropic per-component: Σ_c = (std_c^2) * I\n",
        "        inv_vars = 1.0 / (stds**2)                 # [C]\n",
        "        log_det_vars = D * xp.log(stds**2)         # [C]  (since det = (std^2)^D)\n",
        "    elif stds.ndim == 2 and stds.shape[1] == D:\n",
        "        # Diagonal per-component: Σ_c = diag(std_{c,1}^2, ..., std_{c,D}^2)\n",
        "        inv_vars = 1.0 / (stds**2)                 # [C, D]\n",
        "        log_det_vars = xp.log(stds**2).sum(axis=1) # [C]  (since det = prod_d std_{c,d}^2)\n",
        "    else:\n",
        "        raise ValueError(\"`stds` must have shape (C,) or (C, D).\")\n",
        "\n",
        "    const = -0.5 * D * xp.log(2.0 * xp.pi)         # scalar\n",
        "    norm_const = const - 0.5 * log_det_vars        # [C]\n",
        "\n",
        "    out = xp.empty((N,), dtype=xp.float64)\n",
        "\n",
        "    for i in range(0, N, batch_size):\n",
        "        xb = x[i:i+batch_size]                     # [B, D]\n",
        "        diff = xb[:, None, :] - means[None, :, :]  # [B, C, D]\n",
        "\n",
        "        if inv_vars.ndim == 1:\n",
        "            # quadratic term = sum_d (diff^2) * (1/std_c^2)\n",
        "            d2 = (diff**2).sum(axis=-1)            # [B, C]\n",
        "            quad = d2 * inv_vars[None, :]          # [B, C]\n",
        "        else:\n",
        "            # quadratic term = sum_d (diff^2 / std_{c,d}^2)\n",
        "            quad = ((diff**2) * inv_vars[None, :, :]).sum(axis=-1)  # [B, C]\n",
        "\n",
        "        # log component densities (including weights)\n",
        "        log_comp = log_w[None, :] + norm_const[None, :] - 0.5 * quad  # [B, C]\n",
        "\n",
        "        # log-sum-exp over components for numerical stability\n",
        "        m = xp.max(log_comp, axis=1, keepdims=True)                   # [B, 1]\n",
        "        log_mix = xp.squeeze(m, 1) + xp.log(xp.exp(log_comp - m).sum(axis=1))  # [B]\n",
        "\n",
        "        if mode == \"absolute\":\n",
        "            out[i:i+batch_size] = xp.exp(log_mix)                     # p_mix(x)\n",
        "        elif mode == \"relative\":\n",
        "            s2 = float(iso_std) ** 2\n",
        "            # log phi_iso(x) for N(0, s2 I)\n",
        "            log_iso = -0.5 * D * xp.log(2.0 * xp.pi * s2) - 0.5 * (xb**2).sum(axis=1) / s2  # [B]\n",
        "            out[i:i+batch_size] = xp.exp(log_mix - log_iso)           # p_mix(x)/phi_iso(x)\n",
        "        else:\n",
        "            raise ValueError(\"mode must be 'absolute' or 'relative'.\")\n",
        "\n",
        "    return out\n",
        "import cupy as cp\n",
        "\n",
        "\n",
        "\n",
        "def get_ou_evolved_gmm_params(t, means0, stds0, w0):\n",
        "    xp = cp\n",
        "    means0 = xp.asarray(means0)\n",
        "    e = xp.exp(-t)\n",
        "    e2 = e * e\n",
        "\n",
        "    # Evolved means\n",
        "    m_t = e * means0\n",
        "\n",
        "    S0 = xp.asarray(stds0)\n",
        "    K = means0.shape[0]\n",
        "    D = means0.shape[1]\n",
        "\n",
        "    # --- Full covariance cases ---\n",
        "    if (S0.ndim == 3 and S0.shape[-1] == S0.shape[-2]) or (S0.ndim == 2 and S0.shape[0] == S0.shape[1]):\n",
        "        # Ensure shape (K, D, D)\n",
        "        if S0.ndim == 2:  # (D, D) for all components\n",
        "            S0 = xp.broadcast_to(S0[None, :, :], (K, D, D))\n",
        "        # Σ_t = e^{-2t} Σ_0 + (1 - e^{-2t}) I\n",
        "        eye = xp.eye(D, dtype=S0.dtype)[None, :, :]  # (1, D, D)\n",
        "        Sigma_t = e2 * S0 + (1.0 - e2) * eye\n",
        "        return m_t, Sigma_t, w0\n",
        "\n",
        "    # --- Diagonal / isotropic std cases ---\n",
        "    # Treat S0 as stds; compute variances elementwise with broadcasting\n",
        "    v0 = S0 * S0\n",
        "    v_t = e2 * v0 + (1.0 - e2)  # adds isotropic I term\n",
        "    std_t = xp.sqrt(v_t)\n",
        "    return m_t, std_t, w0\n",
        "\n",
        "\n",
        "def OU_evolve_samples(y_cp, t_scalar):\n",
        "    import cupy as cp\n",
        "    y = cp.asarray(y_cp)\n",
        "    t = float(t_scalar)\n",
        "    et = cp.exp(-t)\n",
        "    sigma = cp.sqrt(cp.maximum(1.0 - et*et, 0.0))\n",
        "    z = cp.random.standard_normal(y.shape, dtype=y.dtype)\n",
        "    return et * y + sigma * z\n",
        "\n",
        "\n",
        "def samples_to_sampler(samples):\n",
        "    samples = np.asarray(samples)\n",
        "    N = samples.shape[0]\n",
        "    def sampler(n):\n",
        "        if n <= N:\n",
        "            return samples[:n].copy()\n",
        "        k = n // N; r = n % N\n",
        "        out = [samples for _ in range(k)]\n",
        "        if r: out.append(samples[:r])\n",
        "        return np.concatenate(out, axis=0).copy()\n",
        "    return sampler\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "try:\n",
        "    import cupy as cp\n",
        "except Exception:\n",
        "    cp = None\n",
        "\n",
        "import numpy as np\n",
        "try:\n",
        "    import cupy as cp\n",
        "except Exception:\n",
        "    cp = None\n",
        "\n",
        "\n",
        "\n",
        "def normalize_gmm_params(means, stds, weights, *, mode=\"global\", eps=1e-12):\n",
        "    \"\"\"\n",
        "    Center & scale a GMM so the mixture mean is 0 and the marginal variance is 1.\n",
        "\n",
        "    Inputs\n",
        "    ------\n",
        "    means   : (C, D)\n",
        "    stds    : (C,)  or (C, D)     # isotropic or diagonal per component (std, not var)\n",
        "    weights : (C,)\n",
        "    mode    : \"global\" -> one scalar scale s so average marginal variance == 1\n",
        "              \"per_dim\" -> vector scale s_d so EACH marginal variance == 1\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    means_n : (C, D)\n",
        "    stds_n  : (C,)  if input was (C,) and mode=\"global\"\n",
        "              (C,D) if mode=\"per_dim\" or input was (C,D)\n",
        "    weights_n : (C,)   (re-normalized to sum to 1)\n",
        "    \"\"\"\n",
        "    # pick backend\n",
        "    try:\n",
        "        import cupy as cp\n",
        "        xp = cp if isinstance(means, cp.ndarray) else __import__(\"numpy\")\n",
        "    except Exception:\n",
        "        import numpy as np\n",
        "        xp = np\n",
        "\n",
        "    m = xp.asarray(means,   dtype=xp.float64)\n",
        "    s = xp.asarray(stds,    dtype=xp.float64)\n",
        "    w = xp.asarray(weights, dtype=xp.float64)\n",
        "\n",
        "    # normalize weights\n",
        "    w = xp.clip(w, 0, xp.inf)\n",
        "    ws = w.sum()\n",
        "    if not xp.isfinite(ws) or ws <= 0:\n",
        "        raise ValueError(\"weights must sum to a positive value.\")\n",
        "    w = w / ws\n",
        "\n",
        "    C, D = m.shape\n",
        "\n",
        "    # per-dimension variance contributions\n",
        "    if s.ndim == 1:          # isotropic per component\n",
        "        var_terms = (s**2)[:, None]                # (C,1) -> broadcast to (C,D)\n",
        "    elif s.ndim == 2 and s.shape[1] == D:\n",
        "        var_terms = s**2                            # (C,D)\n",
        "    else:\n",
        "        raise ValueError(\"stds must have shape (C,) or (C,D).\")\n",
        "\n",
        "    # mixture stats\n",
        "    mu_mix  = w @ m                                  # (D,)\n",
        "    Ex2     = (w[:, None] * (var_terms + m**2)).sum(axis=0)  # (D,)\n",
        "    var_mix = xp.maximum(Ex2 - mu_mix**2, eps)       # (D,)\n",
        "\n",
        "    if mode == \"global\":\n",
        "        scale = xp.sqrt(var_mix.mean())              # scalar s\n",
        "        m_n = (m - mu_mix[None, :]) / scale\n",
        "        s_n = s / scale                              # keeps shape of s\n",
        "    elif mode == \"per_dim\":\n",
        "        scale = xp.sqrt(var_mix)                     # (D,)\n",
        "        m_n = (m - mu_mix[None, :]) / scale[None, :]\n",
        "        if s.ndim == 1:\n",
        "            s_n = s[:, None] / scale[None, :]        # becomes diagonal (C,D)\n",
        "        else:\n",
        "            s_n = s / scale[None, :]\n",
        "    else:\n",
        "        raise ValueError('mode must be \"global\" or \"per_dim\".')\n",
        "\n",
        "    return (m_n.astype(means.dtype, copy=False),\n",
        "            s_n.astype(stds.dtype,  copy=False),\n",
        "            w.astype(weights.dtype, copy=False))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_gmm_funcs(\n",
        "    num_c: int = 200,\n",
        "    k_dim: int = 5,\n",
        "    variant: str = \"helix\",\n",
        "    comp_std: float = 0.10,\n",
        "    overall_scale: float = 1.0,\n",
        "    rs=None,\n",
        "    m_dim: int = 3,\n",
        "    powerlaw_weights: bool = True,\n",
        "    seed: int = 0,\n",
        "    stds_specified = None,\n",
        "    weights_specified = None,\n",
        "    preset_params = None,\n",
        "    normalize = False,\n",
        "    size = 1,\n",
        "    embedding_mode: str = \"linear\"  # NEW\n",
        "):\n",
        "    # ---- RNG (seeded if requested) -----------------------------------------\n",
        "    if rs is not None and not isinstance(rs, np.random.RandomState):\n",
        "        raise TypeError(\"rs must be a numpy.random.RandomState\")\n",
        "    rng = rs if rs is not None else (np.random.RandomState(seed) if seed is not None\n",
        "                                     else np.random.RandomState())\n",
        "\n",
        "    xp = cp\n",
        "    m_embed = m_dim  # intrinsic dimension placeholder; updated by some variants\n",
        "\n",
        "    # -------------------------- helpers (NEW) -------------------------------\n",
        "    def _qr_tall(k, m):\n",
        "        \"\"\"Q in R^{k×m} with orthonormal columns (k >= m).\"\"\"\n",
        "        Q, _ = np.linalg.qr(rng.randn(k, m))\n",
        "        return Q[:, :m]\n",
        "\n",
        "    def _qr_wide(m, k):\n",
        "        \"\"\"A in R^{m×k} with orthonormal columns (k <= m).\"\"\"\n",
        "        G = rng.randn(m, k)\n",
        "        # Orthonormalize columns of G via SVD\n",
        "        U, _, Vt = np.linalg.svd(G, full_matrices=False)\n",
        "        return (U @ Vt)  # shape (m×k), columns orthonormal\n",
        "\n",
        "    def _orthonormal_complement(Q, k, s):\n",
        "        \"\"\"Given Q (k×m) with orthonormal cols, return Q_perp (k×s) orthonormal & Q^T Q_perp = 0.\"\"\"\n",
        "        if s <= 0:\n",
        "            return None\n",
        "        G = rng.randn(k, s)\n",
        "        G = G - Q @ (Q.T @ G)\n",
        "        Qp, _ = np.linalg.qr(G)\n",
        "        return Qp[:, :s]\n",
        "\n",
        "    def _embed_linear(X_intr):\n",
        "        # Preserve original behavior (branch uses m_dim, not m_embed)\n",
        "        if k_dim > m_dim:\n",
        "            Q = _qr_tall(k_dim, X_intr.shape[1])  # k×m_embed\n",
        "            Z = X_intr @ Q.T                      # num_c×k\n",
        "            covs = np.array([ (comp_std**2) * (Q @ Q.T) + (1e-4*comp_std)**2*np.eye(k_dim)\n",
        "                              for _ in range(num_c) ])\n",
        "        else:\n",
        "            Z = X_intr[:, :k_dim]\n",
        "            covs = np.array([ (comp_std**2) * np.eye(k_dim) for _ in range(num_c) ])\n",
        "        return Z, covs\n",
        "\n",
        "    def _embed_sine_wiggle(X_intr, amp=0.25, freq=1.0):\n",
        "        m = X_intr.shape[1]\n",
        "        # base linear map into k-dim\n",
        "        if k_dim >= m:\n",
        "            Q = _qr_tall(k_dim, m)               # k×m\n",
        "            Zlin = X_intr @ Q.T                  # num_c×k\n",
        "            # add wiggle in orthogonal directions\n",
        "            s = min(max(1, m), max(0, k_dim - m))\n",
        "            Q_perp = _orthonormal_complement(Q, k_dim, s)\n",
        "            if Q_perp is not None:\n",
        "                R = rng.randn(s, m) * freq\n",
        "                b = rng.uniform(0.0, 2*np.pi, size=(s,))\n",
        "                H = np.sin(X_intr @ R.T + b)     # num_c×s\n",
        "                Zwig = H @ Q_perp.T              # num_c×k\n",
        "                Z = Zlin + amp * Zwig\n",
        "            else:\n",
        "                Z = Zlin\n",
        "        else:\n",
        "            # reduce then wiggle using internal orthonormal columns\n",
        "            A = _qr_wide(m, k_dim)               # m×k\n",
        "            Zlin = X_intr @ A                    # num_c×k\n",
        "            # small nonlinear bump inside the current k-subspace\n",
        "            R = rng.randn(k_dim, m) * freq\n",
        "            b = rng.uniform(0.0, 2*np.pi, size=(k_dim,))\n",
        "            H = np.sin(X_intr @ R.T + b)         # num_c×k\n",
        "            Z = Zlin + amp * H\n",
        "        covs = np.array([ (comp_std**2) * np.eye(k_dim) for _ in range(num_c) ])\n",
        "        return Z, covs\n",
        "\n",
        "    def _embed_rff(X_intr, freq=1.0):\n",
        "        m = X_intr.shape[1]\n",
        "        M = max(1, k_dim // 2)\n",
        "        W = rng.randn(M, m) * freq\n",
        "        b = rng.uniform(0.0, 2*np.pi, size=(M,))\n",
        "        C = np.cos(X_intr @ W.T + b)\n",
        "        S = np.sin(X_intr @ W.T + b)\n",
        "        Z = np.concatenate([C, S], axis=1)\n",
        "        if Z.shape[1] < k_dim:\n",
        "            pad = X_intr @ rng.randn(m, k_dim - Z.shape[1])\n",
        "            Z = np.concatenate([Z, pad], axis=1)\n",
        "        else:\n",
        "            Z = Z[:, :k_dim]\n",
        "        covs = np.array([ (comp_std**2) * np.eye(k_dim) for _ in range(num_c) ])\n",
        "        return Z, covs\n",
        "\n",
        "    def _embed_radial(X_intr):\n",
        "        r = np.linalg.norm(X_intr, axis=1, keepdims=True)\n",
        "        feats = [X_intr, r, r**2, np.sin(r), np.cos(r)]\n",
        "        Z = np.concatenate(feats, axis=1)\n",
        "        while Z.shape[1] < k_dim:\n",
        "            w = rng.randn(X_intr.shape[1])\n",
        "            Z = np.hstack([Z, (X_intr @ w[:, None])**2])\n",
        "        Z = Z[:, :k_dim]\n",
        "        covs = np.array([ (comp_std**2) * np.eye(k_dim) for _ in range(num_c) ])\n",
        "        return Z, covs\n",
        "\n",
        "    # --------------------------- custom branch ------------------------------\n",
        "    if variant == \"custom\":\n",
        "        if preset_params is not None:\n",
        "            means_np, stds_np, weights_np = preset_params\n",
        "        else:\n",
        "            # draw random means in R^k_dim\n",
        "            means_np = rng.randn(num_c, k_dim) * overall_scale\n",
        "            # isotropic component stds\n",
        "            stds_np  = np.full(num_c, comp_std, dtype=float)\n",
        "            # random non-uniform weights\n",
        "            weights_np = rng.rand(num_c)\n",
        "            weights_np /= weights_np.sum()\n",
        "        # move to GPU\n",
        "        means   = xp.asarray(means_np, dtype=xp.float64)\n",
        "        stds    = xp.asarray(stds_np,   dtype=xp.float64)\n",
        "        weights = xp.asarray(weights_np, dtype=xp.float64)\n",
        "\n",
        "        def sampler(n, seed=None, debug=False):\n",
        "            return sample_gmm_gpu(n, means, stds, weights, seed=seed)\n",
        "\n",
        "        def score_func(x):\n",
        "            return score_gmm_gpu(x, means, stds, weights)\n",
        "\n",
        "        def density_func(x):\n",
        "            return density_gmm_gpu(x, means, stds, weights, mode='absolute')\n",
        "\n",
        "        def score_div_func(x):\n",
        "            _, d = score_div_gmm(x, means, stds, weights)\n",
        "            return d\n",
        "\n",
        "        params = (means, stds, weights)\n",
        "        return params, sampler, score_func, density_func, score_div_func\n",
        "\n",
        "    # ---------- 1) build intrinsic manifold in R^{m_embed} -----------------\n",
        "    if variant == \"helix\":\n",
        "        t = np.linspace(0, 4*np.pi, num_c)\n",
        "        r = 1.0 + 0.25 * np.sin(3*t)\n",
        "        xyz = np.stack([r*np.cos(t), r*np.sin(t), 0.4*t], axis=1)\n",
        "        X_intr = xyz; m_embed = 3\n",
        "\n",
        "    elif variant == \"concentric\":\n",
        "        shells    = 3\n",
        "        shell_id  = np.repeat(np.arange(shells),\n",
        "                              int(np.ceil(num_c / shells)))[:num_c]\n",
        "        radii     = 0.7 + 0.7 * shell_id\n",
        "        vecs      = rng.randn(num_c, m_dim)\n",
        "        vecs     /= np.linalg.norm(vecs, axis=1, keepdims=True)\n",
        "        X_intr    = radii[:, None] * vecs\n",
        "        m_embed   = m_dim\n",
        "\n",
        "    elif variant == \"sparse\":\n",
        "        xyz       = rng.uniform(-2.5, 2.5, size=(num_c, m_dim))\n",
        "        if m_dim >= 2:\n",
        "            xyz[:, 1] *= 0.3\n",
        "        if m_dim >= 3:\n",
        "            xyz[:, 2] *= 0.1\n",
        "        if m_dim > 3:\n",
        "            xyz[:, 3:] *= 0.05\n",
        "        X_intr = xyz; m_embed = m_dim\n",
        "\n",
        "    elif variant == \"knotted_torus\":\n",
        "        t = np.linspace(0, 2*np.pi, num_c)\n",
        "        r = 0.1\n",
        "        knot_x = (2 + np.cos(5*t)) * np.cos(3*t)\n",
        "        knot_y = (2 + np.cos(5*t)) * np.sin(3*t)\n",
        "        knot_z = np.sin(5*t)\n",
        "        base = np.stack([knot_x, knot_y, knot_z], axis=1)\n",
        "\n",
        "        tan = np.gradient(base, axis=0)\n",
        "        tan /= np.linalg.norm(tan, axis=1, keepdims=True) + 1e-12\n",
        "        rnd = rng.randn(num_c, 3)\n",
        "        n1 = rnd - (rnd * tan).sum(1, keepdims=True) * tan\n",
        "        n1 /= np.linalg.norm(n1, axis=1, keepdims=True) + 1e-12\n",
        "        n2 = np.cross(tan, n1)\n",
        "        n2 /= np.linalg.norm(n2, axis=1, keepdims=True) + 1e-12\n",
        "\n",
        "        theta  = rng.uniform(0, 2*np.pi, num_c)\n",
        "        tube_r = r * (0.7 + 0.3 * np.sin(6*t + 0.5))\n",
        "        xyz = base + tube_r[:, None] * (np.cos(theta)[:, None]*n1 + np.sin(theta)[:, None]*n2)\n",
        "        xyz += 0.05 * rng.randn(*xyz.shape) * (0.5 + 0.5 * np.sin(8*t)[:, None])\n",
        "        X_intr = xyz; m_embed = 3\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"unknown variant\")\n",
        "\n",
        "    # ---------- 2) embed into R^{k_dim} per embedding_mode -----------------\n",
        "    if embedding_mode == \"linear\":\n",
        "        means_np, covs = _embed_linear(X_intr)\n",
        "    elif embedding_mode == \"sine_wiggle\":\n",
        "        means_np, covs = _embed_sine_wiggle(X_intr, amp=0.5, freq=1.0)\n",
        "    elif embedding_mode == \"rff\":\n",
        "        means_np, covs = _embed_rff(X_intr, freq=1.0)\n",
        "    elif embedding_mode == \"radial\":\n",
        "        means_np, covs = _embed_radial(X_intr)\n",
        "    else:\n",
        "        raise ValueError(f\"unknown embedding_mode: {embedding_mode}\")\n",
        "\n",
        "    # ---------- 3) center & scale ------------------------------------------\n",
        "    means_np *= overall_scale / np.max(np.abs(means_np))\n",
        "    means  = xp.asarray(means_np, dtype=xp.float64)\n",
        "\n",
        "    # ---------- 4) stds/weights (respect variant-provided if any) ----------\n",
        "    if not stds_specified:\n",
        "        stds_np    = np.full(num_c, comp_std, dtype=float)\n",
        "    if not weights_specified:\n",
        "        weights_np = np.full(num_c, 1.0/num_c, dtype=float)\n",
        "\n",
        "    stds    = xp.asarray(stds_np, dtype=xp.float64)\n",
        "    weights = xp.asarray(weights_np, dtype=xp.float64)\n",
        "\n",
        "    # Optional normalization to a target size box\n",
        "    if normalize:\n",
        "        means, stds, weights = normalize_gmm_params(means, stds, weights)\n",
        "        means, stds = size * means, size * stds\n",
        "\n",
        "    # ---------- 5) public API: sampler/score/density -----------------------\n",
        "    def sampler(n, seed=None):\n",
        "        return sample_gmm_gpu(n, means, stds, weights, seed=seed)\n",
        "\n",
        "    def score_func(x):\n",
        "        return score_gmm_gpu(x, means, stds, weights)\n",
        "\n",
        "    def density_func(x):\n",
        "        return density_gmm_gpu(x, means, stds, weights, mode='absolute')\n",
        "\n",
        "    def score_div_func(x):\n",
        "        _, d = score_div_gmm(x, means, stds, weights)\n",
        "        return d\n",
        "\n",
        "    params = (means, stds, weights)  # covs computed above if you need it internally\n",
        "    return params, sampler, score_func, density_func, score_div_func\n",
        "\n",
        "\n",
        "def _rand_orth(d, k, seed=None):\n",
        "    rs = cp.random.RandomState(seed) if seed is not None else cp.random\n",
        "    G = rs.standard_normal((d, k), dtype=cp.float64)\n",
        "    # thin QR, make orthonormal columns\n",
        "    Q, _ = cp.linalg.qr(G, mode='reduced')\n",
        "    return Q  # d x k\n",
        "\n",
        "\n",
        "def embed_base(dist, d: int, *, sigma_perp=0.0, rotate=True, seed=None):\n",
        "    \"\"\"\n",
        "    dist: object with 2 callables\n",
        "        - dist['sample_k'](n, seed) -> cp.ndarray [n,k]\n",
        "        - dist['score_k'](Z)        -> cp.ndarray [n,k]\n",
        "    Returns (sampler_func, score_func) on R^d.\n",
        "\n",
        "    Embedding: x = Q_k z  +  Q_perp eps,  eps ~ N(0, sigma_perp^2 I_{d-k})\n",
        "    Score:     s(x) = Q_k s_z  +  Q_perp s_eps,  s_eps = -(1/sigma_perp^2) eps\n",
        "    \"\"\"\n",
        "    k = dist['sample_k'](1, seed=seed).shape[1]\n",
        "    assert 1 <= k <= d\n",
        "    if rotate:\n",
        "        Qk = _rand_orth(d, k, seed=seed)\n",
        "        # Build an orthonormal complement (cheap Gram-Schmidt on random)\n",
        "        Qp = _rand_orth(d, d, seed=(None if seed is None else seed+1))\n",
        "        # Re-orth so [Qk | Qperp] spans R^d with Qk preserved\n",
        "        # Householder trick: project Qp off Qk\n",
        "        Qp = Qp - Qk @ (Qk.T @ Qp)\n",
        "        # Orthonormalize columns and take first d-k\n",
        "        Qp, _ = cp.linalg.qr(Qp, mode='reduced')\n",
        "        Qp = Qp[:, :d-k] if d>k else cp.zeros((d,0), dtype=cp.float64)\n",
        "    else:\n",
        "        Qk = cp.pad(cp.eye(k, dtype=cp.float64), ((0, d-k),(0,0)))  # top k of I_d\n",
        "        Qp = cp.pad(cp.eye(d-k, dtype=cp.float64), ((k,0),(0,0)))\n",
        "\n",
        "    sigma_perp = float(sigma_perp)\n",
        "    inv_perp2  = 0.0 if sigma_perp==0.0 else 1.0/(sigma_perp**2)\n",
        "\n",
        "    def sampler_func(n, seed=None):\n",
        "        Z = dist['sample_k'](int(n), seed=seed)                  # [n,k]\n",
        "        if d == k:\n",
        "            return (Qk @ Z.T).T\n",
        "        rs = cp.random.RandomState(seed) if seed is not None else cp.random\n",
        "        eps = rs.standard_normal((int(n), d-k), dtype=cp.float64) * sigma_perp\n",
        "        X = (Qk @ Z.T).T + (Qp @ eps.T).T\n",
        "        return X\n",
        "\n",
        "    def score_func(X):\n",
        "        X = cp.asarray(X, dtype=cp.float64)\n",
        "        Z   = (Qk.T @ X.T).T                                     # [n,k]\n",
        "        s_k = dist['score_k'](Z)                                 # [n,k]\n",
        "        if d == k:\n",
        "            return (Qk @ s_k.T).T\n",
        "        eps = (Qp.T @ X.T).T                                     # [n,d-k]\n",
        "        s_perp = -inv_perp2 * eps\n",
        "        S = (Qk @ s_k.T).T + (Qp @ s_perp.T).T\n",
        "        return S\n",
        "\n",
        "    return sampler_func, score_func\n",
        "\n",
        "\n",
        "def make_banana_2d(beta=0.25, a=1.0, sigma1=1.0, sigma2=0.1):\n",
        "    def sample_k(n, seed=None):\n",
        "        rs = cp.random.RandomState(seed) if seed is not None else cp.random\n",
        "        z1 = rs.standard_normal(int(n)) * sigma1\n",
        "        u  = rs.standard_normal(int(n)) * sigma2\n",
        "        z2 = u - beta*(z1**2 - a)\n",
        "        return cp.stack([z1, z2], axis=1).astype(cp.float64)\n",
        "\n",
        "    def score_k(Z):\n",
        "        Z = cp.asarray(Z, dtype=cp.float64)\n",
        "        z1, z2 = Z[:,0], Z[:,1]\n",
        "        y = z2 + beta*(z1*z1 - a)\n",
        "        s1 = - z1/(sigma1**2) - (2*beta*z1*y)/(sigma2**2)\n",
        "        s2 = - y/(sigma2**2)\n",
        "        return cp.stack([s1, s2], axis=1)\n",
        "    return {'sample_k': sample_k, 'score_k': score_k}\n",
        "\n",
        "\n",
        "def make_ring_2d(R=3.0, sigma_r=0.1, with_jacobian=True):\n",
        "    def sample_k(n, seed=None):\n",
        "        rs = cp.random.RandomState(seed) if seed is not None else cp.random\n",
        "        theta = rs.uniform(0, 2*cp.pi, int(n))\n",
        "        r = R + rs.standard_normal(int(n)) * sigma_r\n",
        "        z = cp.stack([r*cp.cos(theta), r*cp.sin(theta)], axis=1)\n",
        "        return z.astype(cp.float64)\n",
        "\n",
        "    def score_k(Z):\n",
        "        Z = cp.asarray(Z, dtype=cp.float64)\n",
        "        r = cp.linalg.norm(Z, axis=1) + 1e-12\n",
        "        coef = -(r - R)/(sigma_r**2)\n",
        "        if with_jacobian:\n",
        "            coef = coef + 1.0/r\n",
        "        return (coef[:,None] * (Z / r[:,None])).astype(cp.float64)\n",
        "\n",
        "    return {'sample_k': sample_k, 'score_k': score_k}\n",
        "\n",
        "\n",
        "\n",
        "def _xp_random(seed=None, use_cupy=True):\n",
        "    if use_cupy and cp is not None:\n",
        "        if seed is not None: cp.random.seed(seed)\n",
        "        return cp\n",
        "    rng = np.random.RandomState(seed)\n",
        "    class NPShim:\n",
        "        random = rng\n",
        "        def asarray(self, x, dtype=None): return np.asarray(x, dtype=dtype)\n",
        "        def zeros(self, shape, dtype=None): return np.zeros(shape, dtype=dtype)\n",
        "        def ones(self, shape, dtype=None): return np.ones(shape, dtype=dtype)\n",
        "        def exp(self, x): return np.exp(x)\n",
        "        def sin(self, x): return np.sin(x)\n",
        "        def cos(self, x): return np.cos(x)\n",
        "        def dot(self, a,b): return a @ b\n",
        "        def maximum(self, a,b): return np.maximum(a,b)\n",
        "        def minimum(self, a,b): return np.minimum(a,b)\n",
        "        def sqrt(self, x): return np.sqrt(x)\n",
        "        def where(self, cond, a, b): return np.where(cond, a, b)\n",
        "        def eye(self, n): return np.eye(n)\n",
        "        def stack(self, arrs, axis=0): return np.stack(arrs, axis=axis)\n",
        "        def concatenate(self, arrs, axis=0): return np.concatenate(arrs, axis=axis)\n",
        "        def norm(self, x, axis=None): return np.linalg.norm(x, axis=axis)\n",
        "    return NPShim()\n",
        "\n",
        "def make_corrugated_gaussian(k=6, r=None, omega=80.0, c=0.6, use_cupy=True, seed=0):\n",
        "    \"\"\"\n",
        "    Toy distribution in R^k with 'r' sinusoidal corrugations along orthonormal directions.\n",
        "    Returns a dict with 'sample_k' and 'score_k' just like the banana/ring toys.\n",
        "\n",
        "    log p(x) = -1/2 ||x||^2  - (c/omega) * sum_{j=1}^r sin(omega v_j^T x)\n",
        "    ⇒ score(x) = -x - c * sum_j cos(omega v_j^T x) v_j\n",
        "    The likelihood ratio to N(0,I) is bounded by exp(± r*c/omega) (so no huge spikes if r*c/omega is modest).\n",
        "    \"\"\"\n",
        "    # pick backend RNG/array module (your helper keeps cp when available)\n",
        "    xp = _xp_random(seed=seed, use_cupy=use_cupy)\n",
        "    if r is None:\n",
        "        r = min(3, k)  # default: a few ripples but <= k\n",
        "    assert 1 <= r <= k, \"require 1 <= r <= k\"\n",
        "\n",
        "    # Build V ∈ R^{k×r} with orthonormal columns in the SAME array library\n",
        "    # Generate Gaussian (k×r), QR -> V\n",
        "    if xp is cp:\n",
        "        W = xp.random.randn(k, r)\n",
        "        Q, _ = xp.linalg.qr(W, mode='reduced')   # (k,r)\n",
        "        V = Q\n",
        "    else:\n",
        "        W = np.random.RandomState(seed).randn(k, r)\n",
        "        Q, _ = np.linalg.qr(W)                   # (k,r)\n",
        "        V = xp.asarray(Q)\n",
        "\n",
        "    omega = float(omega); c = float(c)\n",
        "    rho = c / omega   # appears in the target potential, controls closeness to Gaussian\n",
        "\n",
        "    def _proj(X):  # (n,k) -> (n,r)\n",
        "        return X @ V\n",
        "\n",
        "    # exact score in R^k\n",
        "    def score_k(Z):\n",
        "        Z = xp.asarray(Z, dtype=xp.float64)\n",
        "        U = _proj(Z)                              # (n,r)\n",
        "        C = xp.cos(omega * U)                     # (n,r)\n",
        "        return (-Z - (c * C) @ V.T).astype(xp.float64)\n",
        "\n",
        "    # rejection sample from N(0,I) with envelope exp(r * rho)\n",
        "    # acceptance ≥ exp(-r*rho) and no host/device copying\n",
        "    def sample_k(n, seed=None, batch=8192):\n",
        "        xp_loc = _xp_random(seed=seed, use_cupy=use_cupy)\n",
        "        out = []\n",
        "        need = int(n)\n",
        "        logC = r * rho\n",
        "        while need > 0:\n",
        "            m = max(batch, need)\n",
        "            Z = xp_loc.random.randn(m, k).astype(xp.float64)\n",
        "            U = Z @ V                              # (m,r)\n",
        "            w = xp_loc.exp(-rho * xp_loc.sin(omega * U)).prod(axis=1)\n",
        "            acc = w / xp_loc.exp(logC)\n",
        "            u = xp_loc.random.rand(m)\n",
        "            keep = u < acc\n",
        "            if keep.sum() > 0:\n",
        "                out.append(Z[keep])\n",
        "                need -= int(keep.sum())\n",
        "        return xp.concatenate(out, axis=0)[:n]\n",
        "\n",
        "    return {'sample_k': sample_k, 'score_k': score_k}\n",
        "\n",
        "\n",
        "\n",
        "def make_funnel(m):\n",
        "    d = m + 1\n",
        "    def sample_k(n, seed=None):\n",
        "        rs = cp.random.RandomState(seed) if seed is not None else cp.random\n",
        "        z  = rs.standard_normal(int(n))\n",
        "        x  = rs.standard_normal((int(n), m)) * cp.exp(z)[:,None]\n",
        "        return cp.concatenate([x, z[:,None]], axis=1).astype(cp.float64)\n",
        "\n",
        "    def score_k(Z):\n",
        "        Z = cp.asarray(Z, dtype=cp.float64)\n",
        "        x, z = Z[:, :m], Z[:, m]\n",
        "        ezm2 = cp.exp(-2.0*z)\n",
        "        s_x  = - (ezm2[:,None] * x)\n",
        "        s_z  = - z + ezm2 * cp.sum(x*x, axis=1) - m\n",
        "        return cp.concatenate([s_x, s_z[:,None]], axis=1)\n",
        "    return {'sample_k': sample_k, 'score_k': score_k}\n",
        "\n",
        "\n",
        "\n",
        "def make_student_t(k, nu=5.0, Sigma=None, seed=None):\n",
        "    rs = cp.random.RandomState(seed) if seed is not None else cp.random\n",
        "    if Sigma is None:\n",
        "        # strong anisotropy\n",
        "        scales = cp.asarray(cp.logspace(np.log10(0.1), np.log10(5.0), num=k), dtype=cp.float64)\n",
        "        L = cp.diag(scales)\n",
        "        Sinv = cp.diag(1.0/(scales**2))\n",
        "    else:\n",
        "        S = cp.asarray(Sigma, dtype=cp.float64)\n",
        "        L = cp.linalg.cholesky(S)\n",
        "        Sinv = cp.linalg.inv(S)\n",
        "\n",
        "    def sample_k(n, seed=None):\n",
        "        rng = cp.random.RandomState(seed) if seed is not None else cp.random\n",
        "        eps = rng.standard_normal((int(n), k), dtype=cp.float64)\n",
        "        g   = rng.chisquare(df=nu, size=int(n)).astype(cp.float64)\n",
        "        x   = (eps @ L.T) / cp.sqrt(g/nu)[:,None]\n",
        "        return x\n",
        "\n",
        "    def score_k(Z):\n",
        "        Z = cp.asarray(Z, dtype=cp.float64)\n",
        "        Mah = cp.einsum('nd,dd,nd->n', Z, Sinv, Z)           # x^T S^{-1} x\n",
        "        coef = - (nu + k) / (nu + Mah)\n",
        "        return coef[:,None] * (Z @ Sinv.T)\n",
        "    return {'sample_k': sample_k, 'score_k': score_k}\n",
        "\n",
        "\n",
        "\n",
        "def make_k_rotated_banana(K=6, beta=0.25, a=1.0, sigma1=1.0, sigma2=0.1):\n",
        "    base = make_banana_2d(beta, a, sigma1, sigma2)\n",
        "    thetas = cp.linspace(0, 2*cp.pi, K, endpoint=False)\n",
        "    Rks = cp.stack([cp.stack([cp.cos(t), -cp.sin(t)], axis=0)\n",
        "                    for t in thetas], axis=0)  # [K,2]\n",
        "    Rks = cp.stack([cp.stack([Rks[i], cp.stack([cp.sin(thetas[i]), cp.cos(thetas[i])])],axis=0)\n",
        "                    for i in range(K)], axis=0)  # [K,2,2]\n",
        "\n",
        "    def sample_k(n, seed=None):\n",
        "        rs = cp.random.RandomState(seed) if seed is not None else cp.random\n",
        "        idx = rs.randint(0, K, size=int(n))\n",
        "        Z   = base['sample_k'](int(n), seed=None)\n",
        "        # rotate each sample by chosen R_k\n",
        "        X = cp.einsum('nij,nj->ni', Rks[idx], Z)\n",
        "        return X\n",
        "\n",
        "    def _logp_k(Z):\n",
        "        # compute log p_k for each k by un-rotating into banana frame\n",
        "        # log p up to constant from base form\n",
        "        Z = cp.asarray(Z, dtype=cp.float64)\n",
        "        Y = cp.einsum('kij,nj->nki', Rks.transpose(0,2,1), Z)  # [n,K,2]\n",
        "        z1, z2 = Y[...,0], Y[...,1]\n",
        "        y = z2 + beta*(z1*z1 - a)\n",
        "        lp = -0.5*(z1*z1/(sigma1**2) + y*y/(sigma2**2))  # [n,K]\n",
        "        return lp\n",
        "\n",
        "    def score_k(Z):\n",
        "        Z = cp.asarray(Z, dtype=cp.float64)\n",
        "        lp = _logp_k(Z)                       # [n,K]\n",
        "        m  = lp.max(axis=1, keepdims=True)\n",
        "        W  = cp.exp(lp - m); W /= W.sum(1, keepdims=True) + 1e-30\n",
        "        # component scores in x-space: rotate base score\n",
        "        Y = cp.einsum('kij,nj->nki', Rks.transpose(0,2,1), Z)         # un-rotate\n",
        "        sb = base['score_k'](Y.reshape(-1,2)).reshape(Z.shape[0], K, 2)\n",
        "        sx = cp.einsum('kij,nkj->nki', Rks, sb)                       # rotate back\n",
        "        s  = cp.einsum('nk,nki->ni', W, sx)                           # mixture score\n",
        "        return s\n",
        "\n",
        "    return {'sample_k': sample_k, 'score_k': score_k}\n",
        "\n",
        "\n",
        "\n",
        "def make_folded_2d(c=1.0):\n",
        "    def sample_k(n, seed=None):\n",
        "        rs = cp.random.RandomState(seed) if seed is not None else cp.random\n",
        "        u1 = rs.standard_normal(int(n))\n",
        "        u2 = rs.standard_normal(int(n))\n",
        "        z1 = u1\n",
        "        z2 = u2 + c*cp.abs(u1)\n",
        "        return cp.stack([z1, z2], axis=1).astype(cp.float64)\n",
        "\n",
        "    def score_k(Z):\n",
        "        Z = cp.asarray(Z, dtype=cp.float64)\n",
        "        z1, z2 = Z[:,0], Z[:,1]\n",
        "        y = z2 - c*cp.abs(z1)\n",
        "        s2 = -y\n",
        "        s1 = -z1 - y*c*cp.sign(z1)\n",
        "        return cp.stack([s1, s2], axis=1)\n",
        "    return {'sample_k': sample_k, 'score_k': score_k}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_spiky_gmm_funcs(\n",
        "    num_c=200, k_dim=6, base_std=0.15, spike_std=0.001, spike_frac=0.1, seed=0\n",
        "):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    C_spike = max(1, int(num_c * spike_frac))\n",
        "    C_base  = num_c - C_spike\n",
        "\n",
        "    means_base = rng.randn(C_base, k_dim)\n",
        "    means_spk  = rng.randn(C_spike, k_dim)*0.5 + 3.0*rng.choice([-1,1], size=(C_spike,k_dim))\n",
        "    means = np.concatenate([means_base, means_spk], 0)\n",
        "\n",
        "    stds  = np.concatenate([np.full(C_base, base_std), np.full(C_spike, spike_std)])\n",
        "    w     = np.full(num_c, 1.0/num_c)\n",
        "\n",
        "    means = cp.asarray(means, dtype=cp.float64)\n",
        "    stds  = cp.asarray(stds,  dtype=cp.float64)\n",
        "    w     = cp.asarray(w,     dtype=cp.float64)\n",
        "\n",
        "    def sampler(n, seed=None):\n",
        "        return sample_gmm_gpu(n, means, stds, w, seed=seed)\n",
        "    def score(x): return score_gmm_gpu(x, means, stds, w)\n",
        "    return sampler, score\n",
        "\n",
        "\n",
        "def make_pathological_gmm(\n",
        "    C=500, d=39, sep=5.0, cov_scale=0.05, seed=0):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    # place means on a d‐dim hypercube corners or random large coordinates\n",
        "    means = rng.randn(C, d) * sep\n",
        "    # tiny isotropic covariances\n",
        "    stds = np.full(C, cov_scale)\n",
        "    # nonuniform weights\n",
        "    weights = rng.rand(C)\n",
        "    weights /= weights.sum()\n",
        "    # wrap for existing API\n",
        "    params = (means, stds, weights)\n",
        "    sampler, score = get_gmm_funcs(\n",
        "        num_c=C, k_dim=d, variant='custom', comp_std=cov_scale,\n",
        "        overall_scale=sep, rs=rng)\n",
        "    return params, sampler, score\n",
        "\n",
        "\n",
        "def resample_by_alpha(X, alpha, N, return_indices=False, rng=None):\n",
        "    xp = np\n",
        "    is_cupy = type(X).__module__.split('.')[0] == 'cupy' or type(alpha).__module__.split('.')[0] == 'cupy'\n",
        "    if is_cupy:\n",
        "        import cupy as cp\n",
        "        xp = cp\n",
        "\n",
        "    X = xp.asarray(X)\n",
        "    alpha = xp.asarray(alpha).reshape(-1)\n",
        "    M = alpha.size\n",
        "    if X.shape[0] != M:\n",
        "        raise ValueError(f\"Length mismatch: len(alpha)={M} but X.shape[0]={X.shape[0]}\")\n",
        "\n",
        "    # Normalize weights safely\n",
        "    w = xp.clip(alpha, 0, xp.inf).astype(xp.float64)\n",
        "    w_sum = w.sum()\n",
        "    if not xp.isfinite(w_sum) or w_sum <= 0:\n",
        "        # Fallback: uniform sampling if weights are invalid or all zero\n",
        "        p = xp.full(M, 1.0 / M, dtype=xp.float64)\n",
        "    else:\n",
        "        p = w / w_sum\n",
        "\n",
        "    # Draw indices\n",
        "    if rng is None:\n",
        "        idx = xp.random.choice(M, size=int(N), replace=True, p=p)\n",
        "    else:\n",
        "        # rng must have a .choice method (NumPy/CuPy Generator)\n",
        "        idx = rng.choice(M, size=int(N), replace=True, p=p)\n",
        "\n",
        "    Y = X[idx]\n",
        "    return (Y, idx) if return_indices else Y\n",
        "\n",
        "# --- Rank-k linear likelihood on GPU (CuPy) ---\n",
        "\n",
        "\n",
        "\n",
        "def _orthonormal_rows_A(D, k, seed=None):\n",
        "    \"\"\"\n",
        "    Build A with k orthonormal rows in R^{k x D}.\n",
        "    Uses QR on a (D x k) Gaussian and transposes.\n",
        "    \"\"\"\n",
        "    rs = cp.random.RandomState(int(seed)) if (seed is not None and seed != 'rand') else cp.random\n",
        "    G = rs.standard_normal((D, k), dtype=cp.float64)      # (D,k)\n",
        "    Q, _ = cp.linalg.qr(G, mode='reduced')                # (D,k) with orthonormal columns\n",
        "    A = Q.T                                               # (k,D) with orthonormal rows\n",
        "    return A.astype(cp.float64, copy=False)\n",
        "\n",
        "def make_rank_k_likelihood(\n",
        "    D, rank_k, obs_sigma,\n",
        "    sampler_func=None,            # optional: to synthesize y as A x* + eps\n",
        "    y_obs=None,                   # optional: provide your own observed y in R^k\n",
        "    seed=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Build rank-k linear Gaussian likelihood:\n",
        "        log p(y | x) = - (1/(2 sigma^2)) || y - A x ||^2\n",
        "        grad_x log p  = (1/sigma^2) A^T ( y - A x )\n",
        "\n",
        "    Returns:\n",
        "      A              : (k, D) CuPy array\n",
        "      y_obs_k        : (k,)   CuPy array\n",
        "      likelyhood_func: (N,D) -> (N,)  log-lik up to additive const\n",
        "      log_likelyhood_func: alias to likelyhood_func\n",
        "      loglik_grad_fn : (N,D) -> (N,D)\n",
        "    \"\"\"\n",
        "    k = int(rank_k)\n",
        "    assert 1 <= k <= D, f\"rank_k must be in [1, {D}]\"\n",
        "    A = _orthonormal_rows_A(D, k, seed=seed)             # (k,D)\n",
        "    inv_var = cp.asarray(1.0 / (obs_sigma * obs_sigma), dtype=cp.float64)\n",
        "    alpha = 0.5 * inv_var                                 # = 1/(2 sigma^2)\n",
        "\n",
        "    if y_obs is None:\n",
        "        if sampler_func is None:\n",
        "            raise ValueError(\"Provide sampler_func to synthesize y_obs or pass y_obs directly.\")\n",
        "        x_star = sampler_func(1, seed=seed).reshape(-1)   # (D,)\n",
        "        eps = cp.random.normal(0.0, obs_sigma, size=(k,)).astype(cp.float64)\n",
        "        y_obs_k = (A @ x_star.astype(cp.float64)) + eps   # (k,)\n",
        "    else:\n",
        "        y_obs_k = cp.asarray(y_obs, dtype=cp.float64).reshape(k)\n",
        "        # if user passed a D-dim y, project it:\n",
        "        if y_obs_k.size == D:\n",
        "            y_obs_k = (A @ y_obs_k)\n",
        "\n",
        "    def likelyhood_func(x_ref):\n",
        "        \"\"\"\n",
        "        x_ref: (N,D) -> returns log-likelihood up to constant: -(1/(2σ^2)) ||y - A x||^2\n",
        "        \"\"\"\n",
        "        X = cp.asarray(x_ref, dtype=cp.float64)\n",
        "        Ax = X @ A.T                                      # (N,k)\n",
        "        resid = y_obs_k[None, :] - Ax                     # (N,k)\n",
        "        return -alpha * cp.sum(resid * resid, axis=1)     # (N,)\n",
        "\n",
        "    def loglik_grad_fn(x_ref):\n",
        "        \"\"\"\n",
        "        x_ref: (N,D) -> gradient wrt x: (1/σ^2) A^T (y - A x)\n",
        "        \"\"\"\n",
        "        X = cp.asarray(x_ref, dtype=cp.float64)\n",
        "        Ax = X @ A.T                                      # (N,k)\n",
        "        resid = y_obs_k[None, :] - Ax                     # (N,k)\n",
        "        return (inv_var * (resid @ A))                    # (N,D)\n",
        "\n",
        "    # keep your API surface:\n",
        "    def log_likelyhood_func(x_ref):\n",
        "        # already a log-likelihood; return it directly (do NOT take cp.log again)\n",
        "        return likelyhood_func(x_ref)\n",
        "\n",
        "    return A, y_obs_k, likelyhood_func, log_likelyhood_func, loglik_grad_fn\n",
        "\n",
        "\n",
        "def calculate_true_score_at_t(y, t, means0, stds0, w0, batch_size):\n",
        "    m, s, w = get_ou_evolved_gmm_params(t, means0, stds0, w0)\n",
        "    return score_gmm_gpu(y, m, s, w, batch_size)\n",
        "\n",
        "\n",
        "def ou_logweights_with_lik(x, t, x_ref, loglik_ref=None):\n",
        "    \"\"\"\n",
        "    x:      [B, d] current particles (y_t in your notation)\n",
        "    x_ref:  [N, d] reference particles ~ p(x0)\n",
        "    t:      float\n",
        "    loglik_ref: [N] log p(y | x_ref) (any constant offset OK)\n",
        "\n",
        "    returns: normalized weights wbar [B, N], plus cached scalars\n",
        "    \"\"\"\n",
        "    xp = cp\n",
        "    et  = xp.exp(-t)\n",
        "    sig2 = 1.0 - xp.exp(-2*t)             # sigma_t^2\n",
        "\n",
        "    # pairwise squared distances ||x - e^{-t} x_i||^2\n",
        "    diff = x[:, None, :] - et * x_ref[None, :, :]\n",
        "    sq   = xp.sum(diff*diff, axis=-1)     # [B, N]\n",
        "\n",
        "    logw = -0.5 * sq / sig2               # OU kernel (const cancels)\n",
        "    if loglik_ref is not None:\n",
        "        logw = logw + loglik_ref[None, :] # add the likelihood term\n",
        "\n",
        "    # stabilize & normalize\n",
        "    m = xp.max(logw, axis=1, keepdims=True)\n",
        "    w = xp.exp(logw - m)\n",
        "    wbar = w / xp.sum(w, axis=1, keepdims=True)  # [B, N]\n",
        "    return wbar, et, sig2\n",
        "\n",
        "\n",
        "def gaussian_obs_loglik(x_ref, y, alpha):\n",
        "    diff = x_ref - y[None, :]\n",
        "    return -alpha * cp.sum(diff*diff, axis=1)     # [N]\n",
        "\n",
        "\n",
        "# --- helper (drop near your other utils) ------------------------------------\n",
        "def _resolve_loglik(x_ref, xp, loglik_ref=None, loglik_fn=None):\n",
        "    \"\"\"Return xp.ndarray of shape [N] with per-ref log-likelihoods, or None.\"\"\"\n",
        "    if loglik_fn is not None:\n",
        "        ll = loglik_fn(x_ref)\n",
        "    else:\n",
        "        ll = loglik_ref\n",
        "    if ll is None:\n",
        "        return None\n",
        "    ll = xp.asarray(ll).reshape(-1)\n",
        "    assert ll.shape[0] == x_ref.shape[0], \"loglik_ref must have shape [N]\"\n",
        "    return ll\n",
        "\n",
        "\n",
        "def kss_score_t(y_batch, t, x_ref, s_ref,\n",
        "                loglik_ref=None, loglik_fn=None,\n",
        "                w_correct=1.0,\n",
        "                grad_loglik_ref=None, grad_loglik_fn=None):\n",
        "    xp = cp\n",
        "    ll = _resolve_loglik(x_ref, xp, loglik_ref, loglik_fn)\n",
        "\n",
        "    # build posterior-at-0 reference score if gradient supplied\n",
        "    if grad_loglik_ref is None and grad_loglik_fn is not None:\n",
        "        grad_loglik_ref = grad_loglik_fn(x_ref)           # [N,d]\n",
        "\n",
        "    s_ref_eff = s_ref if grad_loglik_ref is None else (s_ref + grad_loglik_ref)\n",
        "\n",
        "    var = 1.0 - xp.exp(-2.0*t); inv_v = 1.0/var; et = xp.exp(-t)\n",
        "    diff   = y_batch[:, None, :] - et * x_ref[None, :, :]\n",
        "    logits = -0.5 * inv_v * xp.sum(diff**2, axis=-1)\n",
        "    if ll is not None:\n",
        "        logits = logits + ll[None, :]\n",
        "\n",
        "    m = xp.max(logits, axis=1, keepdims=True)\n",
        "    w = xp.exp(logits - m)\n",
        "    w *= w_correct\n",
        "    w = w / (xp.sum(w, axis=1, keepdims=True) + 1e-30)\n",
        "\n",
        "    return xp.exp(t) * (w @ s_ref_eff)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def tweedie_score_t(y, t, x_ref, loglik_ref=None, loglik_fn=None , w_correct = 1.0):\n",
        "    xp = cp\n",
        "    if t < 1e-12:\n",
        "        raise ValueError(\"t must be >0 for Tweedie estimator.\")\n",
        "    ll = _resolve_loglik(x_ref, xp, loglik_ref, loglik_fn)\n",
        "\n",
        "    B, D = y.shape\n",
        "    var   = 1.0 - xp.exp(-2.0 * t)\n",
        "    inv_v = 1.0 / var\n",
        "    et    = xp.exp(-t)\n",
        "\n",
        "    mu_i  = et * x_ref                                  # (N,D)\n",
        "    diff  = y[:, None, :] - mu_i[None, :, :]            # (B,N,D)\n",
        "\n",
        "    logits = -0.5 * inv_v * xp.sum(diff**2, axis=-1)    # (B,N)\n",
        "    if ll is not None:\n",
        "        logits = logits + ll[None, :]                   # add log-lik\n",
        "\n",
        "    m = xp.max(logits, axis=1, keepdims=True)\n",
        "\n",
        "    w = xp.exp(logits - m)\n",
        "    w *= w_correct\n",
        "    w = w / (xp.sum(w, axis=1, keepdims=True) + 1e-30)  # (B,N)\n",
        "\n",
        "\n",
        "    score = - xp.einsum('bn, bnd -> bd', w, diff) * inv_v\n",
        "    return score\n",
        "\n",
        "def proxy_tweedie_score_t(y, t, mu_ref, loglik_ref=None, loglik_fn=None, w_correct=1.0):\n",
        "    \"\"\"\n",
        "    Tweedie NP score but centered on proxy means mu_i (from local Gaussian fits),\n",
        "    i.e. weights and conditional mean are computed w.r.t. e^{-t} mu_i.\n",
        "    \"\"\"\n",
        "    xp = cp\n",
        "    if t < 1e-12:\n",
        "        raise ValueError(\"t must be >0 for Tweedie estimator.\")\n",
        "    Z = xp.asarray(mu_ref, dtype=xp.float64)  # basis = mu_i\n",
        "\n",
        "    # optional likelihood values evaluated at mu_i\n",
        "    if loglik_fn is not None:\n",
        "        ll = loglik_fn(Z)\n",
        "    else:\n",
        "        ll = loglik_ref\n",
        "    if ll is not None:\n",
        "        ll = xp.asarray(ll).reshape(-1)\n",
        "\n",
        "    B, D = y.shape\n",
        "    var   = 1.0 - xp.exp(-2.0 * t)\n",
        "    inv_v = 1.0 / var\n",
        "    et    = xp.exp(-t)\n",
        "\n",
        "    mu_i_t = et * Z                                      # (N,D)\n",
        "    diff   = y[:, None, :] - mu_i_t[None, :, :]          # (B,N,D)\n",
        "    logits = -0.5 * inv_v * xp.sum(diff**2, axis=-1)     # (B,N)\n",
        "    if ll is not None:\n",
        "        logits = logits + ll[None, :]\n",
        "\n",
        "    m = xp.max(logits, axis=1, keepdims=True)\n",
        "    w = xp.exp(logits - m)\n",
        "    w *= w_correct\n",
        "    w = w / (xp.sum(w, axis=1, keepdims=True) + 1e-30)   # (B,N)\n",
        "\n",
        "    score = - xp.einsum('bn,bnd->bd', w, diff) * inv_v\n",
        "    return score\n",
        "\n",
        "\n",
        "\n",
        "# ===== SCORE-WHITENING UTILITIES ============================================\n",
        "def _eig_clip(lam, lam_min=1e-8, lam_max=None, xp=None):\n",
        "    xp = cp if xp is None else xp\n",
        "    lam = xp.asarray(lam, dtype=xp.float64)\n",
        "    if lam_max is not None:\n",
        "        lam = xp.minimum(lam, float(lam_max))\n",
        "    lam = xp.maximum(lam, float(lam_min))\n",
        "    return lam\n",
        "\n",
        "def build_score_whitener(s_ref, *, center=True, det_one=True,\n",
        "                         lam_min=1e-8, lam_max=None):\n",
        "    \"\"\"\n",
        "    Given score samples s_ref [N, d] (CuPy or NumPy), return whitening maps:\n",
        "      W, Winv, WTinv  such that  Cov(W^{-T} s) ≈ I.\n",
        "    If det_one=True, normalize W to have det=1 (volume-preserving).\n",
        "    \"\"\"\n",
        "    # choose backend\n",
        "    xp = cp if (cp is not None and isinstance(s_ref, cp.ndarray)) else np\n",
        "    S = xp.asarray(s_ref, dtype=xp.float64)\n",
        "    N, d = S.shape\n",
        "    mu = xp.mean(S, axis=0) if center else xp.zeros(d, dtype=S.dtype)\n",
        "    S0 = S - mu[None, :]\n",
        "\n",
        "    # empirical covariance of scores\n",
        "    Sig = (S0.T @ S0) / float(max(N, 1))\n",
        "\n",
        "    # symmetric eigendecomp\n",
        "    lam, U = xp.linalg.eigh(Sig)                   # Sig = U diag(lam) U^T\n",
        "    lam = _eig_clip(lam, lam_min=lam_min, lam_max=lam_max, xp=xp)\n",
        "\n",
        "    # choose W = Sig^{1/2} (so W^{-T} Sig W^{-1} = I)\n",
        "    W = (U * xp.sqrt(lam)[None, :]) @ U.T          # SPD\n",
        "    if det_one:\n",
        "        # normalize to det=1: divide by (det(W))^{1/d}\n",
        "        # det(W) = prod sqrt(lam) = (prod lam)^{1/2}\n",
        "        logdetW = 0.5 * float(xp.sum(xp.log(lam)))\n",
        "        c = xp.exp(logdetW / d)\n",
        "        W = W / c\n",
        "\n",
        "    # inverses (use solves for stability)\n",
        "    Winv = xp.linalg.inv(W)\n",
        "    WTinv = xp.linalg.inv(W.T)\n",
        "    return W, Winv, WTinv, mu\n",
        "\n",
        "def apply_linear(x, M, xp=None):\n",
        "    xp = cp if xp is None else xp\n",
        "    return (xp.asarray(x, dtype=xp.float64) @ xp.asarray(M, dtype=xp.float64).T)\n",
        "\n",
        "def wrap_likelihood_for_whiten(like_fn, Winv):\n",
        "    \"\"\"z-space wrapper: ℓ_z(z)=ℓ_x(W^{-1} z).\"\"\"\n",
        "    if like_fn is None: return None\n",
        "    def like_z(Z):\n",
        "        xp = cp if (cp is not None and isinstance(Z, cp.ndarray)) else np\n",
        "        X = apply_linear(Z, Winv, xp=xp)\n",
        "        return like_fn(X)\n",
        "    return like_z\n",
        "\n",
        "def wrap_grad_like_for_whiten(grad_like_fn, Winv):\n",
        "    \"\"\"Chain rule: ∇_z log p(W^{-1} z) = W^{-T} ∇_x log p(x).\"\"\"\n",
        "    if grad_like_fn is None: return None\n",
        "    WTinv = Winv.T\n",
        "    def gl_z(Z):\n",
        "        xp = cp if (cp is not None and isinstance(Z, cp.ndarray)) else np\n",
        "        X = apply_linear(Z, Winv, xp=xp)\n",
        "        g = grad_like_fn(X)\n",
        "        return apply_linear(g, WTinv, xp=xp)\n",
        "    return gl_z\n",
        "\n",
        "def wrap_blackbox_score_for_whiten(score_fn, Winv, WTinv):\n",
        "    \"\"\"\n",
        "    Given s_x(x,t), return s_z(z,t)=W^{-T} s_x(W^{-1} z, t).\n",
        "    Works for signature score_fn(x, t) or score_fn(x) (no t).\n",
        "    \"\"\"\n",
        "    if score_fn is None: return None\n",
        "    import inspect\n",
        "    takes_t = (len(inspect.signature(score_fn).parameters) == 2)\n",
        "    def s_z(z, t=None):\n",
        "        xp = cp if (cp is not None and isinstance(z, cp.ndarray)) else np\n",
        "        x = apply_linear(z, Winv, xp=xp)\n",
        "        s_x = score_fn(x, t) if takes_t else score_fn(x)\n",
        "        return apply_linear(s_x, WTinv, xp=xp)\n",
        "    return s_z\n",
        "\n",
        "def recompute_proxy_scores_in_white(x0_white, proxy_fit_fn):\n",
        "    \"\"\"\n",
        "    Refit proxy score model directly in z-space.\n",
        "    Expect proxy_fit_fn to accept data and return a callable s_z0(·) or array on x0_white.\n",
        "    Minimal shim to keep API flexible:\n",
        "      - If proxy_fit_fn returns an array, we use it as s0_white at anchors.\n",
        "      - If it returns a function, we evaluate it on x0_white for s0_white,\n",
        "        and also return the function for general queries.\n",
        "    \"\"\"\n",
        "    s_obj = proxy_fit_fn(x0_white)\n",
        "    if callable(s_obj):\n",
        "        s0_white = s_obj(x0_white)\n",
        "        return s0_white, s_obj\n",
        "    else:\n",
        "        return s_obj, None\n",
        "\n",
        "\n",
        "\n",
        "def snis_blend(y_batch, x_ref, s0_ref, t, *,\n",
        "               xp=None, eps=1e-12, return_details=False,\n",
        "               loglik_ref=None, loglik_fn=None, w_correct=1.0,\n",
        "               grad_loglik_ref=None, grad_loglik_fn=None,\n",
        "               twd_basis=None):   # <-- NEW\n",
        "    if xp is None:\n",
        "        try: import cupy as _cp; xp = _cp\n",
        "        except: import numpy as _np; xp = _np\n",
        "\n",
        "    # Use proxy basis for Tweedie if provided; else fall back to x_ref\n",
        "    Z_ref = x_ref if twd_basis is None else xp.asarray(twd_basis, dtype=xp.float64)\n",
        "\n",
        "    # likelihood for the basis used in Tweedie weights\n",
        "    ll = _resolve_loglik(Z_ref, xp, loglik_ref, loglik_fn)\n",
        "\n",
        "    if grad_loglik_ref is None and grad_loglik_fn is not None:\n",
        "        grad_loglik_ref = grad_loglik_fn(x_ref)  # still eval grad at true anchors\n",
        "\n",
        "    s_ref_eff = s0_ref if grad_loglik_ref is None else (s0_ref + grad_loglik_ref)\n",
        "\n",
        "    B, d = y_batch.shape\n",
        "    var  = xp.maximum(1.0 - xp.exp(-2.0*t), eps)\n",
        "    inv_v = 1.0/var\n",
        "    et    = xp.exp(-t)\n",
        "\n",
        "    # weights built against Z_ref (proxy means if provided)\n",
        "    diff   = y_batch[:, None, :] - et * Z_ref[None, :, :]\n",
        "    logits = -0.5 * inv_v * xp.sum(diff*diff, axis=-1)\n",
        "    if ll is not None:\n",
        "        logits = logits + ll[None, :]\n",
        "\n",
        "    m  = xp.max(logits, axis=1, keepdims=True)\n",
        "    w  = xp.exp(logits - m)\n",
        "    w *= w_correct\n",
        "    w /= xp.sum(w, axis=1, keepdims=True) + eps\n",
        "\n",
        "    w2 = w * w\n",
        "    S0 = xp.sum(w2, axis=1)\n",
        "\n",
        "    # scores (KSS uses same weights w; Tweedie uses Z_ref)\n",
        "    s_kss = xp.exp(t) * (w @ s_ref_eff)\n",
        "    mu_x  = w @ Z_ref\n",
        "    s_twd = -(inv_v) * (y_batch - et * mu_x)\n",
        "\n",
        "    den_sn = xp.maximum(1.0 - S0, eps)\n",
        "\n",
        "    # --- Vk ---\n",
        "    a_i = xp.exp(t) * s_ref_eff\n",
        "    a_norm2 = xp.sum(a_i * a_i, axis=1)\n",
        "    S1a = w2 @ a_norm2\n",
        "    S2a = w2 @ a_i\n",
        "    mu_a = s_kss\n",
        "    mu_a_norm2 = xp.sum(mu_a * mu_a, axis=1)\n",
        "    num_Vk = S1a - 2.0 * xp.sum(mu_a * S2a, axis=1) + mu_a_norm2 * S0\n",
        "    Vk = num_Vk / den_sn\n",
        "\n",
        "    # --- Vt (now uses Z_ref everywhere) ---\n",
        "    y_norm2 = xp.sum(y_batch*y_batch, axis=1)\n",
        "    z_norm2 = xp.sum(Z_ref*Z_ref, axis=1)\n",
        "    w2_z    = w2 @ Z_ref\n",
        "    y_dot_zW = xp.sum(y_batch * w2_z, axis=1)\n",
        "    S1b = (y_norm2 * S0 - 2.0 * et * y_dot_zW + (et**2) * (w2 @ z_norm2)) * (inv_v**2)\n",
        "    S2b = -(inv_v) * (y_batch * S0[:, None] - et * w2_z)\n",
        "    mu_b = s_twd\n",
        "    mu_b_norm2 = xp.sum(mu_b * mu_b, axis=1)\n",
        "    num_Vt = S1b - 2.0 * xp.sum(mu_b * S2b, axis=1) + mu_b_norm2 * S0\n",
        "    Vt = num_Vt / den_sn\n",
        "\n",
        "    # --- C (cross, use Z_ref in the a⋅z term) ---\n",
        "    a_dot_z = xp.sum(a_i * Z_ref, axis=1)      # [N]\n",
        "    Wa = w2 @ a_i\n",
        "    term1 = xp.sum(Wa * y_batch, axis=1)\n",
        "    term2 = w2 @ a_dot_z\n",
        "    Sab = -(inv_v) * (term1 - et * term2)\n",
        "    num_C = (Sab\n",
        "             - xp.sum(mu_a * S2b, axis=1)\n",
        "             - xp.sum(mu_b * S2a, axis=1)\n",
        "             + xp.sum(mu_a * mu_b, axis=1) * S0)\n",
        "    C = num_C / den_sn\n",
        "\n",
        "    denom = xp.maximum(Vk + Vt - 2.0 * C, eps)\n",
        "    lam = (Vk - C) / denom\n",
        "    lam = xp.clip(lam, 0.0, 1.0)\n",
        "    lam = xp.nan_to_num(lam, nan=0.5, posinf=1.0, neginf=0.0)\n",
        "\n",
        "    if return_details:\n",
        "        ess = 1.0 / (S0 + eps)\n",
        "        return lam, dict(w=w, ess=ess, s_kss=s_kss, s_twd=s_twd, Vk=Vk, Vt=Vt, C=C)\n",
        "    return lam\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def s_blend(x, t, x_ref, s_ref, loglik_ref=None, loglik_fn=None,\n",
        "            loglik_grad_fn=None, w_correct=1.0,\n",
        "            twd_basis=None):   # <-- NEW (None = use x_ref; array = use mu_i)\n",
        "    if twd_basis is None:\n",
        "        # original path (real Tweedie + KSS)\n",
        "        s_tweedie = tweedie_score_t(x, t, x_ref,\n",
        "            loglik_ref=loglik_ref, loglik_fn=loglik_fn, w_correct=w_correct)\n",
        "        s_kss = kss_score_t(x, t, x_ref, s_ref, grad_loglik_fn=loglik_grad_fn,\n",
        "            loglik_ref=loglik_ref, loglik_fn=loglik_fn, w_correct=w_correct)\n",
        "        wt = snis_blend(x, x_ref, s_ref, t, grad_loglik_fn=loglik_grad_fn,\n",
        "            loglik_ref=loglik_ref, loglik_fn=loglik_fn, w_correct=w_correct)\n",
        "        return (1.0 - wt)[:, None] * s_kss + wt[:, None] * s_tweedie\n",
        "    else:\n",
        "        # proxy path: get λ, s_kss, s_twd consistently from the same weights\n",
        "        lam, det = snis_blend(x, x_ref, s_ref, t, return_details=True,\n",
        "                              grad_loglik_fn=loglik_grad_fn,\n",
        "                              loglik_ref=loglik_ref, loglik_fn=loglik_fn,\n",
        "                              w_correct=w_correct, twd_basis=twd_basis)\n",
        "        s_kss = det['s_kss']\n",
        "        s_twd = det['s_twd']\n",
        "        return (1.0 - lam)[:, None] * s_kss + lam[:, None] * s_twd\n",
        "\n",
        "\n",
        "\n",
        "def gmm_posterior_params(\n",
        "    y, pis, mus, Sigmas, *, A=None, b=None, R=None, alpha=None, out=\"full\"\n",
        "):\n",
        "    # --- backend ---\n",
        "    try:\n",
        "        import cupy as cp\n",
        "        xp = cp.get_array_module(mus)\n",
        "    except Exception:\n",
        "        import numpy as np\n",
        "        xp = np\n",
        "    import numpy as _np\n",
        "\n",
        "    K, D = mus.shape\n",
        "    pis = xp.asarray(pis, dtype=mus.dtype).reshape(K)\n",
        "    mus = xp.asarray(mus, dtype=mus.dtype).reshape(K, D)\n",
        "\n",
        "    # Likelihood set-up\n",
        "    if A is None and alpha is not None:\n",
        "        sigma2 = 1.0 / (2.0 * alpha)\n",
        "        A = xp.eye(D, dtype=mus.dtype)\n",
        "        b = xp.zeros(D, dtype=mus.dtype)\n",
        "        R = sigma2 * xp.eye(D, dtype=mus.dtype)\n",
        "    elif A is None:\n",
        "        raise ValueError(\"Provide (A,b,R) or alpha.\")\n",
        "    A = xp.asarray(A, dtype=mus.dtype)\n",
        "    k = int(A.shape[0])\n",
        "\n",
        "    y = xp.asarray(y, dtype=mus.dtype).reshape(k)\n",
        "    b = xp.zeros(k, dtype=mus.dtype) if b is None else xp.asarray(b, dtype=mus.dtype).reshape(k)\n",
        "    if R is None and alpha is not None:\n",
        "        sigma2 = 1.0 / (2.0 * alpha)\n",
        "        R = sigma2 * xp.eye(k, dtype=mus.dtype)\n",
        "    elif R is None:\n",
        "        raise ValueError(\"R must be provided when using general A without alpha.\")\n",
        "    R = xp.asarray(R, dtype=mus.dtype).reshape(k, k)\n",
        "\n",
        "    # Prior variances (support isotropic or diagonal)\n",
        "    Sigmas = xp.asarray(Sigmas, dtype=mus.dtype)\n",
        "    if Sigmas.ndim == 1 and Sigmas.shape[0] == K:\n",
        "        s2 = (Sigmas**2)[:, None] * xp.ones((1, D), dtype=mus.dtype)     # (K, D)\n",
        "    elif Sigmas.ndim == 2 and Sigmas.shape == (K, D):\n",
        "        s2 = Sigmas**2                                                  # (K, D)\n",
        "    else:\n",
        "        raise ValueError(\"Sigmas must have shape (K,) or (K,D) (isotropic or diagonal).\")\n",
        "    inv_s2 = 1.0 / xp.maximum(s2, 1e-300)                                # (K, D)\n",
        "\n",
        "    # Data terms: J = A^T R^{-1} A, h = A^T R^{-1} (y - b)\n",
        "    Rinv_A = xp.linalg.solve(R, A)               # (k, D)\n",
        "    J = A.T @ Rinv_A                             # (D, D)\n",
        "    h = A.T @ xp.linalg.solve(R, (y - b))        # (D,)\n",
        "\n",
        "    means_post = xp.empty((K, D), dtype=mus.dtype)\n",
        "    stds_post  = xp.empty((K, D), dtype=mus.dtype) if out == \"diag\" else None\n",
        "    covs_post  = xp.empty((K, D, D), dtype=mus.dtype) if out == \"full\" else None\n",
        "    logw       = xp.empty(K, dtype=mus.dtype)\n",
        "\n",
        "    TWO_PI = xp.asarray(2.0 * _np.pi, dtype=mus.dtype)\n",
        "    y_center = y[None, :] - (A @ mus.T).T - b[None, :]                   # (K, k)\n",
        "    I_D = xp.eye(D, dtype=mus.dtype)\n",
        "\n",
        "    for i in range(K):\n",
        "        # Posterior precision: P_i = J + diag(inv_s2[i])\n",
        "        P_i = J + xp.diag(inv_s2[i])\n",
        "\n",
        "        # Mean: m_i = P_i^{-1} (diag(inv_s2[i]) mu_i + h) via Cholesky\n",
        "        L = xp.linalg.cholesky(P_i)                                      # lower\n",
        "        rhs = inv_s2[i] * mus[i] + h\n",
        "        z = xp.linalg.solve(L, rhs)\n",
        "        m_i = xp.linalg.solve(L.T, z)\n",
        "        means_post[i] = m_i\n",
        "\n",
        "        # Covariance: S_i = P_i^{-1}; compute from Cholesky\n",
        "        # L L^T = P_i => P_i^{-1} = L^{-T} L^{-1}\n",
        "        Z = xp.linalg.solve(L, I_D)                                      # L^{-1}\n",
        "        if out == \"full\":\n",
        "            covs_post[i] = Z.T @ Z\n",
        "        else:  # \"diag\"\n",
        "            diag_S = xp.sum(Z * Z, axis=0)\n",
        "            stds_post[i] = xp.sqrt(xp.maximum(diag_S, 0.0))\n",
        "\n",
        "        # Evidence weight for component i: N(y; A mu_i + b, A Σ_i A^T + R)\n",
        "        AiSigmaAT = (A * s2[i][None, :]) @ A.T                           # A diag(s2_i) A^T\n",
        "        C_i = AiSigmaAT + R                                              # (k, k)\n",
        "        Lc = xp.linalg.cholesky(C_i)\n",
        "        v  = y_center[i]\n",
        "        zc = xp.linalg.solve(Lc, v)\n",
        "        #quad = zc @ xp.linalg.solve(Lc.T, zc)\n",
        "        quad = zc @ zc\n",
        "        log_det = 2.0 * xp.sum(xp.log(xp.diag(Lc)))\n",
        "        log_like = -0.5 * (k * xp.log(TWO_PI) + log_det + quad)\n",
        "        logw[i] = xp.log(pis[i] + 1e-300) + log_like\n",
        "\n",
        "    # normalize mixture weights\n",
        "    mlog = xp.max(logw)\n",
        "    w_post = xp.exp(logw - mlog)\n",
        "    w_post /= xp.sum(w_post) + 1e-300\n",
        "\n",
        "    if out == \"full\":\n",
        "        return w_post, means_post, covs_post\n",
        "    else:\n",
        "        return w_post, means_post, stds_post\n",
        "\n",
        "\n",
        "\n",
        "# ─── 2. SLICED-WASSERSTEIN-2 HELPER ────────────────────────────────────────────\n",
        "def sliced_wasserstein2(a, b, n_proj: int = 256, max_pts: int = 5000):\n",
        "    xp = cp                                             # keep GPU\n",
        "\n",
        "    # ----- subsample to equal size -----------------------------------------\n",
        "    n = min(max_pts, a.shape[0], b.shape[0])\n",
        "    ia = xp.random.choice(a.shape[0], n, replace=False)\n",
        "    ib = xp.random.choice(b.shape[0], n, replace=False)\n",
        "    a_, b_ = a[ia], b[ib]\n",
        "\n",
        "    # ----- random directions in R^D ----------------------------------------\n",
        "    D = a_.shape[1]\n",
        "    dirs = xp.random.randn(n_proj, D, dtype=xp.float64)\n",
        "    dirs /= xp.linalg.norm(dirs, axis=1, keepdims=True) + 1e-12   # unit vectors\n",
        "\n",
        "    # ----- project, sort, average squared diffs ----------------------------\n",
        "    pa = xp.sort(a_ @ dirs.T, axis=0)\n",
        "    pb = xp.sort(b_ @ dirs.T, axis=0)\n",
        "    w2_sq = xp.mean((pa - pb)**2)\n",
        "\n",
        "    return float(xp.sqrt(w2_sq).get())          # move single value to host\n",
        "\n",
        "\n",
        "\n",
        "def _median_pairwise_dist(X):\n",
        "    # median ||x_i - x_j|| over i<j (approx via full matrix; subsample with max_pts)\n",
        "    x2 = cp.sum(X*X, axis=1, keepdims=True)\n",
        "    D2 = cp.maximum(x2 + x2.T - 2.0*(X @ X.T), 0.0)\n",
        "    i = cp.triu_indices(D2.shape[0], 1)\n",
        "    d = cp.sqrt(D2[i])\n",
        "    return float(cp.median(d))\n",
        "\n",
        "def _median_knn_dist(X, k=5):\n",
        "    # median distance to the k-th NN (leave-one-out via inf diagonal)\n",
        "    x2 = cp.sum(X*X, axis=1, keepdims=True)\n",
        "    D2 = cp.maximum(x2 + x2.T - 2.0*(X @ X.T), 0.0)\n",
        "    cp.fill_diagonal(D2, cp.inf)\n",
        "    # kth smallest along rows\n",
        "    kth = cp.partition(D2, kth=k-1, axis=1)[:, k-1]\n",
        "    return float(cp.sqrt(cp.median(kth)))\n",
        "\n",
        "def _choose_sigmas(X, mode=\"auto_pair+mnn\", knn_k=5,\n",
        "                   mnn_multipliers=(0.5, 1.0, 2.0, 4.0),\n",
        "                   pair_multipliers=(0.5, 1.0, 2.0)):\n",
        "    sigs = []\n",
        "    if \"pair\" in mode:\n",
        "        md = _median_pairwise_dist(X)\n",
        "        sigs += [md*m for m in pair_multipliers]\n",
        "    if \"mnn\" in mode:\n",
        "        mnn = _median_knn_dist(X, k=knn_k)\n",
        "        sigs += [mnn*m for m in mnn_multipliers]\n",
        "    sigs = sorted({float(s) for s in sigs if s > 0})\n",
        "    if not sigs:  # fallback\n",
        "        md = _median_pairwise_dist(X)\n",
        "        sigs = [md]\n",
        "    return cp.asarray(sigs, dtype=cp.float64)\n",
        "\n",
        "def auto_multiscale_ksd(\n",
        "    x, score_fn, *,\n",
        "    sigmas=\"auto_pair+mnn\",      # \"auto_mnn\", \"auto_pair\", \"auto_pair+mnn\", or array-like\n",
        "    knn_k=5,\n",
        "    mnn_multipliers=(0.5, 1.0, 2.0, 4.0),\n",
        "    pair_multipliers=(0.5, 1.0, 2.0),\n",
        "    max_pts=5000,\n",
        "    agg=\"mean\",                  # \"mean\" or \"max\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Multiscale KSD with data-adaptive σ including median k-NN spacing.\n",
        "    Uses your existing RBF-Stein formula and averages (or maxes) across σ's.\n",
        "    \"\"\"\n",
        "    xp = cp\n",
        "    X = xp.asarray(x, dtype=xp.float64)\n",
        "    if X.shape[0] > max_pts:\n",
        "        idx = xp.random.choice(X.shape[0], max_pts, replace=False)\n",
        "        X = X[idx]\n",
        "    N, D = X.shape\n",
        "\n",
        "    s = score_fn(X)                             # (N,D)\n",
        "    diff = X[:, None, :] - X[None, :, :]        # (N,N,D)\n",
        "    r2 = xp.sum(diff*diff, axis=-1)             # (N,N)\n",
        "\n",
        "    if isinstance(sigmas, str) and sigmas.startswith(\"auto\"):\n",
        "        sig = _choose_sigmas(\n",
        "            X, mode=sigmas, knn_k=knn_k,\n",
        "            mnn_multipliers=mnn_multipliers,\n",
        "            pair_multipliers=pair_multipliers\n",
        "        )\n",
        "    else:\n",
        "        sig = xp.asarray(sigmas, dtype=xp.float64).ravel()\n",
        "    sig = sig[xp.isfinite(sig) & (sig > 0)]\n",
        "    assert sig.size >= 1, \"No valid σ found.\"\n",
        "\n",
        "    # Precompute score inner products\n",
        "    sdot = s @ s.T                                # (N,N)\n",
        "    # r·s terms\n",
        "    r_dot_sx = xp.einsum('ijd,id->ij', diff, s)   # (N,N)\n",
        "    r_dot_sy = xp.einsum('ijd,jd->ij', diff, s)   # (N,N)\n",
        "\n",
        "    vals = []\n",
        "    for σ in sig:\n",
        "        σ2 = σ*σ\n",
        "        K = xp.exp(-r2 / (2.0*σ2))                # RBF\n",
        "        term1 = sdot * K\n",
        "        term2 = (r_dot_sx - r_dot_sy) * (K / σ2)\n",
        "        term3 = (D/σ2 - r2/(σ2*σ2)) * K\n",
        "        U = term1 + term2 + term3\n",
        "        ksd2 = xp.sum(U) / (N*N)\n",
        "        vals.append(ksd2)\n",
        "\n",
        "    vals = xp.asarray(vals, dtype=xp.float64)\n",
        "    out = (float(xp.mean(vals)) if agg == \"mean\" else float(xp.max(vals)))\n",
        "    return out, cp.asnumpy(sig)  # return σ’s too for debugging\n",
        "\n",
        "\n",
        "def compute_multiscale_ksd(x, score_fn, sigmas = (0.1, 0.2, 0.4, .8)): #sigmas=(0.33, 0.5, 1.0, 2.0)):\n",
        "    xp = cp\n",
        "    N, D = x.shape\n",
        "    s = score_fn(x)\n",
        "    diff = x[:,None,:] - x[None,:,:]\n",
        "    r2   = xp.sum(diff**2, axis=-1)\n",
        "    ksd2 = 0.0\n",
        "    for σ in sigmas:\n",
        "        K = xp.exp(-r2 / (2*σ*σ))\n",
        "        sdot = s @ s.T\n",
        "        r_dot_sx = xp.einsum('ijd,id->ij', diff, s)\n",
        "        r_dot_sy = xp.einsum('ijd,jd->ij', diff, s)\n",
        "        term1 = sdot * K\n",
        "        term2 = (r_dot_sx - r_dot_sy) / (σ*σ) * K\n",
        "        term3 = (D/(σ*σ) - r2/(σ**4)) * K\n",
        "        U = term1 + term2 + term3\n",
        "        ksd2 += xp.sum(U) / (N*N)\n",
        "    return float(ksd2 / len(sigmas))\n",
        "\n",
        "\n",
        "def xp_rmse(x,y):\n",
        "  xp = cp\n",
        "  diff = xp.square(x-y)\n",
        "  return xp.sqrt(xp.mean(diff))\n",
        "\n",
        "\n",
        "\n",
        "def blend_diagnostics(y, t, x_ref, s0_ref, *, xp=cp):\n",
        "    # weights\n",
        "    var = xp.maximum(1.0 - xp.exp(-2.0*t), 1e-12)\n",
        "    inv_v = 1.0 / var\n",
        "    et    = xp.exp(-t)\n",
        "    diff  = y[:, None, :] - et * x_ref[None, :, :]\n",
        "    logits = -0.5 * inv_v * xp.sum(diff*diff, axis=-1)\n",
        "    m = xp.max(logits, axis=1, keepdims=True)\n",
        "    w = xp.exp(logits - m); w /= xp.sum(w, axis=1, keepdims=True) + 1e-12\n",
        "\n",
        "    # scores\n",
        "    s_twd = -(inv_v) * (y - et * (w @ x_ref))\n",
        "    s_kss = xp.exp(t) * (w @ s0_ref)\n",
        "\n",
        "    # cosine\n",
        "    num = xp.sum(s_twd * s_kss, axis=1)\n",
        "    den = xp.linalg.norm(s_twd, axis=1) * xp.linalg.norm(s_kss, axis=1) + 1e-12\n",
        "    cos = num / den\n",
        "\n",
        "    # ESS and λ (using your current snis_blend)\n",
        "    lam, det = snis_blend(y, x_ref, s0_ref, t, return_details=True)\n",
        "    ess = det['ess']\n",
        "    return dict(\n",
        "        cos=cos, ess=ess, lam=lam, w=w,\n",
        "        s_twd=s_twd, s_kss=s_kss\n",
        "    )\n",
        "\n",
        "def summarize_diag(diag, tag=\"\"):\n",
        "    to_np = lambda z: cp.asnumpy(z)\n",
        "    cos = to_np(diag['cos']); ess = to_np(diag['ess']); lam = to_np(diag['lam'])\n",
        "    print(f\"[{tag}] cos  median/05%/95%: {np.median(cos):+.3f}  {np.percentile(cos,5):+.3f}  {np.percentile(cos,95):+.3f}\")\n",
        "    print(f\"[{tag}] ESS  median/min/max: {np.median(ess):.1f}   {ess.min():.1f}  {ess.max():.1f}\")\n",
        "    print(f\"[{tag}] lam  median/min/max: {np.median(lam):.3f}   {lam.min():.3f}  {lam.max():.3f}\")\n",
        "\n",
        "def blend_sampler(\n",
        "    N_part, N_ref, time_pts, batch_size, sampler_func, score_func, mode,\n",
        "    likelyhood_func=None, loglik_grad_fn=None, true_score_func=None,\n",
        "    h_coeff=0.5, seed='rand', weight_mode='snis', w_correct_func=None,\n",
        "    use_proxy_tweedie=False, proxy_mu_func=None,\n",
        "    # --- NEW whitening knobs ---\n",
        "    whiten_scores=False,\n",
        "    whiten_mode=\"map_white_scores\",   # or \"recompute_white_scores\"\n",
        "    proxy_score_fit=None,             # required if whiten_mode=\"recompute_white_scores\"\n",
        "    whiten_center=True,\n",
        "    whiten_det_one=True,\n",
        "    whiten_lam_min=1e-8,\n",
        "    whiten_lam_max=None,\n",
        "):\n",
        "    xp = cp\n",
        "    rmse_list = []\n",
        "    dim = int(sampler_func(5).shape[1])\n",
        "    # Work in z-space natively\n",
        "    y = xp.random.randn(N_part, dim).astype(xp.float64)\n",
        "\n",
        "    x0_ref_x = sampler_func(N_ref) if seed == 'rand' else sampler_func(N_ref, seed=seed)\n",
        "\n",
        "    if w_correct_func is None:\n",
        "        w_correct = 1.0\n",
        "    else:\n",
        "        w_correct = w_correct_func(x0_ref_x)\n",
        "\n",
        "    s0_ref_x = score_func(x0_ref_x)\n",
        "\n",
        "    # ---------- Whitening block --------------------------------------------\n",
        "    if whiten_scores:\n",
        "        # Build W from x-space scores\n",
        "        W, Winv, WTinv, _ = build_score_whitener(\n",
        "            s0_ref_x, center=whiten_center, det_one=whiten_det_one,\n",
        "            lam_min=whiten_lam_min, lam_max=whiten_lam_max\n",
        "        )\n",
        "        # map anchors to z-space\n",
        "        x0_ref = apply_linear(x0_ref_x, W, xp=xp)\n",
        "\n",
        "        # score anchors in z\n",
        "        if whiten_mode == \"map_white_scores\":\n",
        "            s0_ref = apply_linear(s0_ref_x, WTinv, xp=xp)  # W^{-T} s_x\n",
        "            proxy_score_callable_z = None\n",
        "        elif whiten_mode == \"recompute_white_scores\":\n",
        "            if proxy_score_fit is None:\n",
        "                raise ValueError(\"whiten_mode='recompute_white_scores' requires proxy_score_fit.\")\n",
        "            s0_ref, proxy_score_callable_z = recompute_proxy_scores_in_white(x0_ref, proxy_score_fit)\n",
        "        else:\n",
        "            raise ValueError(\"whiten_mode must be 'map_white_scores' or 'recompute_white_scores'\")\n",
        "\n",
        "        # wrap likelihoods/gradients for z-space\n",
        "        likelyhood_func_z   = wrap_likelihood_for_whiten(likelyhood_func, Winv)\n",
        "        loglik_grad_fn_z    = wrap_grad_like_for_whiten(loglik_grad_fn, Winv)\n",
        "        true_score_func_z   = wrap_blackbox_score_for_whiten(true_score_func, Winv, WTinv)\n",
        "\n",
        "        # optional Tweedie proxy basis in z-space\n",
        "        mu_ref = None\n",
        "        if use_proxy_tweedie:\n",
        "            if proxy_mu_func is None:\n",
        "                raise ValueError(\"use_proxy_tweedie=True requires proxy_mu_func.\")\n",
        "            mu_ref, _maybe_var = proxy_mu_func(x0_ref) if callable(proxy_mu_func) else proxy_mu_func\n",
        "            if isinstance(mu_ref, tuple):  # (mu, ...)\n",
        "                mu_ref = mu_ref[0]\n",
        "\n",
        "        # if we refit proxy scores and need general score queries in z, wrap\n",
        "        def maybe_score_ref_z_query(Z):\n",
        "            if proxy_score_callable_z is None:\n",
        "                # only anchor scores are used by KSS path; no query function needed\n",
        "                raise RuntimeError(\"No z-score callable available.\")\n",
        "            return proxy_score_callable_z(Z)\n",
        "\n",
        "        # --------------- evolution loop in z-space -------------------------\n",
        "        for k_step in range(len(time_pts)-1):\n",
        "            t_cur, t_prev = time_pts[k_step], time_pts[k_step+1]\n",
        "            dt = t_cur - t_prev\n",
        "            noise = xp.random.randn(N_part, dim).astype(xp.float64)\n",
        "\n",
        "            if weight_mode == 'snis' or not true_score_func:\n",
        "                S_cur = s_blend(y, t_cur, x0_ref, s0_ref,\n",
        "                                loglik_fn=likelyhood_func_z,\n",
        "                                loglik_grad_fn=loglik_grad_fn_z,\n",
        "                                w_correct=w_correct,\n",
        "                                twd_basis=mu_ref)\n",
        "                y_hat = y + dt * (y + 2 * S_cur) + xp.sqrt(2.0 * dt) * noise\n",
        "                S_proxy = s_blend(y_hat, t_prev, x0_ref, s0_ref,\n",
        "                                  loglik_fn=likelyhood_func_z,\n",
        "                                  loglik_grad_fn=loglik_grad_fn_z,\n",
        "                                  w_correct=w_correct,\n",
        "                                  twd_basis=mu_ref)\n",
        "            elif weight_mode == 'oracle':\n",
        "                if true_score_func_z is None:\n",
        "                    raise ValueError(\"Provide true_score_func for oracle mode.\")\n",
        "                S_cur = true_score_func_z(y, t_cur)\n",
        "                y_hat = y + dt * (y + 2 * S_cur) + xp.sqrt(2.0 * dt) * noise\n",
        "                S_proxy = true_score_func_z(y_hat, t_prev)\n",
        "\n",
        "            if mode == 'ou_sde':\n",
        "                y = y_hat\n",
        "            elif mode == 'pf_ode':\n",
        "                y = y + dt * (y + S_cur)\n",
        "            elif mode == 'heun_pc':\n",
        "                drift_avg = 0.5 * ((y + 2 * S_cur) + (y_hat + 2 * S_proxy))\n",
        "                y = y + dt * drift_avg + xp.sqrt(2.0 * dt) * noise\n",
        "            elif mode == 'heun_hop':\n",
        "                A, B = xp.exp(dt), (xp.exp(2 * dt) - 1.0)\n",
        "                C, D = xp.sqrt(B), B / A\n",
        "                y = A * y + h_coeff * B * S_proxy + (1 - h_coeff) * D * S_cur + C * noise\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown mode {mode}\")\n",
        "\n",
        "        # map back to x-space\n",
        "        y_x = apply_linear(y, Winv, xp=xp)\n",
        "        if true_score_func:\n",
        "            # diagnostics in x-space if requested\n",
        "            S_true1 = true_score_func(y_x, t_cur)\n",
        "            S_true2 = true_score_func(apply_linear(y_hat, Winv, xp=xp), t_prev)\n",
        "            rmse_list.append(float(xp_rmse(apply_linear(S_cur, W.T, xp=xp), S_true1)))\n",
        "            rmse_list.append(float(xp_rmse(apply_linear(S_proxy, W.T, xp=xp), S_true2)))\n",
        "            return y_x, rmse_list\n",
        "        else:\n",
        "            return y_x\n",
        "\n",
        "    # ---------- No whitening: fall back to original behavior ---------------\n",
        "    x0_ref = x0_ref_x\n",
        "    s0_ref = s0_ref_x\n",
        "\n",
        "    # (identical to your current implementation below)\n",
        "    for k_step in range(len(time_pts)-1):\n",
        "        t_cur, t_prev = time_pts[k_step], time_pts[k_step+1]\n",
        "        dt = t_cur - t_prev\n",
        "        noise = xp.random.randn(N_part, dim).astype(xp.float64)\n",
        "\n",
        "        if weight_mode == 'snis' or not true_score_func:\n",
        "            S_cur = s_blend(y, t_cur, x0_ref, s0_ref,\n",
        "                            loglik_fn=likelyhood_func,\n",
        "                            loglik_grad_fn=loglik_grad_fn,\n",
        "                            w_correct=w_correct,\n",
        "                            twd_basis=None)\n",
        "            y_hat = y + dt * (y + 2 * S_cur) + xp.sqrt(2.0 * dt) * noise\n",
        "            S_proxy = s_blend(y_hat, t_prev, x0_ref, s0_ref,\n",
        "                              loglik_fn=likelyhood_func,\n",
        "                              loglik_grad_fn=loglik_grad_fn,\n",
        "                              w_correct=w_correct,\n",
        "                              twd_basis=None)\n",
        "        elif weight_mode == 'oracle':\n",
        "            S_cur = true_score_func(y, t_cur)\n",
        "            y_hat = y + dt * (y + 2 * S_cur) + xp.sqrt(2.0 * dt) * noise\n",
        "            S_proxy = true_score_func(y_hat, t_prev)\n",
        "\n",
        "        if mode == 'ou_sde':\n",
        "            y = y_hat\n",
        "        elif mode == 'pf_ode':\n",
        "            y = y + dt * (y + S_cur)\n",
        "        elif mode == 'heun_pc':\n",
        "            drift_avg = 0.5 * ((y + 2 * S_cur) + (y_hat + 2 * S_proxy))\n",
        "            y = y + dt * drift_avg + xp.sqrt(2.0 * dt) * noise\n",
        "        elif mode == 'heun_hop':\n",
        "            A, B = xp.exp(dt), (xp.exp(2 * dt) - 1.0)\n",
        "            C, D = xp.sqrt(B), B / A\n",
        "            y = A * y + h_coeff * B * S_proxy + (1 - h_coeff) * D * S_cur + C * noise\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown mode {mode}\")\n",
        "\n",
        "    if true_score_func:\n",
        "        S_true1 = true_score_func(y, t_cur)\n",
        "        S_true2 = true_score_func(y_hat, t_prev)\n",
        "        rmse_list.append(float(xp_rmse(S_cur,  S_true1)))\n",
        "        rmse_list.append(float(xp_rmse(S_proxy,S_true2)))\n",
        "        return y, rmse_list\n",
        "    else:\n",
        "        return y\n",
        "\n",
        "\n",
        "def tweedie_sampler(\n",
        "    N_part, N_ref, time_pts, batch_size,\n",
        "    sampler_func, mode, likelyhood_func=None,\n",
        "    true_score_func=None, h_coeff=0.5, seed='rand', w_correct_func=None,\n",
        "    # NEW\n",
        "    whiten_scores=False,\n",
        "    whiten_det_one=True, whiten_center=True, whiten_lam_min=1e-8, whiten_lam_max=None\n",
        "):\n",
        "    xp = cp\n",
        "    rmse_list = []\n",
        "    dim = int(sampler_func(5).shape[1])\n",
        "    y = xp.random.randn(N_part, dim).astype(xp.float64)\n",
        "\n",
        "    x0_ref_x = sampler_func(N_ref) if seed == 'rand' else sampler_func(N_ref, seed=seed)\n",
        "\n",
        "    if w_correct_func is None:\n",
        "        w_correct = 1.0\n",
        "    else:\n",
        "        w_correct = w_correct_func(x0_ref_x)\n",
        "\n",
        "    if whiten_scores:\n",
        "        # need any score proxy just to build W — use a cheap local diag proxy if you like;\n",
        "        # here we fall back to a finite-diff guard: if you have a proper proxy, plug it here.\n",
        "        raise NotImplementedError(\"To whiten Tweedie, provide score samples to build W or skip whitening here.\")\n",
        "    else:\n",
        "        x0_ref = x0_ref_x\n",
        "        like_fn = likelyhood_func\n",
        "\n",
        "    for k_step in range(len(time_pts)-1):\n",
        "        t_cur, t_prev = time_pts[k_step], time_pts[k_step+1]\n",
        "        dt = t_cur - t_prev\n",
        "        noise = xp.random.randn(N_part, dim).astype(xp.float64)\n",
        "\n",
        "        S_cur = tweedie_score_t(y, t_cur, x0_ref, loglik_fn=like_fn, w_correct=w_correct)\n",
        "        y_hat = y + dt * (y + 2 * S_cur) + xp.sqrt(2.0 * dt) * noise\n",
        "        S_proxy = tweedie_score_t(y_hat, t_prev, x0_ref, loglik_fn=like_fn, w_correct=w_correct)\n",
        "\n",
        "        if mode == 'ou_sde':\n",
        "            y = y_hat\n",
        "        elif mode == 'pf_ode':\n",
        "            y = y + dt * (y + S_cur)\n",
        "        elif mode == 'heun_pc':\n",
        "            drift_avg = 0.5 * ((y + 2 * S_cur) + (y_hat + 2 * S_proxy))\n",
        "            y = y + dt * drift_avg + xp.sqrt(2.0 * dt) * noise\n",
        "        elif mode == 'heun_hop':\n",
        "            A, B = xp.exp(dt), (xp.exp(2 * dt) - 1.0)\n",
        "            C, D = xp.sqrt(B), B / A\n",
        "            y = A * y + h_coeff * B * S_proxy + (1 - h_coeff) * D * S_cur + C * noise\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown mode {mode}\")\n",
        "\n",
        "    if true_score_func:\n",
        "        S_true1 = true_score_func(y, t_cur)\n",
        "        S_true2 = true_score_func(y_hat, t_prev)\n",
        "        rmse_list.append(float(xp_rmse(S_cur,  S_true1)))\n",
        "        rmse_list.append(float(xp_rmse(S_proxy,S_true2)))\n",
        "        return y, rmse_list\n",
        "    else:\n",
        "        return y\n",
        "\n",
        "\n",
        "def kss_sampler(\n",
        "    N_part, N_ref, time_pts, batch_size,\n",
        "    sampler_func, score_func, mode, likelyhood_func=None, loglik_grad_fn=None,\n",
        "    true_score_func=None, h_coeff=0.5, seed='rand', w_correct_func=1.0,\n",
        "    # NEW\n",
        "    whiten_scores=False,\n",
        "    whiten_mode=\"map_white_scores\",\n",
        "    proxy_score_fit=None,\n",
        "    whiten_center=True, whiten_det_one=True, whiten_lam_min=1e-8, whiten_lam_max=None\n",
        "):\n",
        "    xp = cp\n",
        "    rmse_list = []\n",
        "    dim = int(sampler_func(5).shape[1])\n",
        "    y = xp.random.randn(N_part, dim).astype(xp.float64)\n",
        "\n",
        "    x0_ref_x = sampler_func(N_ref) if seed == 'rand' else sampler_func(N_ref, seed=seed)\n",
        "    s0_ref_x = score_func(x0_ref_x)\n",
        "\n",
        "    if w_correct_func is None:\n",
        "        w_correct = 1.0\n",
        "    else:\n",
        "        w_correct = w_correct_func(x0_ref_x)\n",
        "\n",
        "    if whiten_scores:\n",
        "        W, Winv, WTinv, _ = build_score_whitener(\n",
        "            s0_ref_x, center=whiten_center, det_one=whiten_det_one,\n",
        "            lam_min=whiten_lam_min, lam_max=whiten_lam_max\n",
        "        )\n",
        "        x0_ref = apply_linear(x0_ref_x, W, xp=xp)\n",
        "        if whiten_mode == \"map_white_scores\":\n",
        "            s0_ref = apply_linear(s0_ref_x, WTinv, xp=xp)\n",
        "            score_z_callable = None\n",
        "        else:\n",
        "            if proxy_score_fit is None:\n",
        "                raise ValueError(\"kss_sampler: proxy_score_fit required for recompute mode.\")\n",
        "            s0_ref, score_z_callable = recompute_proxy_scores_in_white(x0_ref, proxy_score_fit)\n",
        "\n",
        "        like_fn_z = wrap_likelihood_for_whiten(likelyhood_func, Winv)\n",
        "        glike_fn_z = wrap_grad_like_for_whiten(loglik_grad_fn, Winv)\n",
        "        true_score_func_z = wrap_blackbox_score_for_whiten(true_score_func, Winv, WTinv)\n",
        "\n",
        "        for k_step in range(len(time_pts)-1):\n",
        "            t_cur, t_prev = time_pts[k_step], time_pts[k_step+1]\n",
        "            dt = t_cur - t_prev\n",
        "            noise = xp.random.randn(N_part, dim).astype(xp.float64)\n",
        "\n",
        "            S_cur = kss_score_t(y, t_cur, x0_ref, s0_ref, loglik_fn=like_fn_z, w_correct=w_correct)\n",
        "\n",
        "            y_hat = y + dt * (y + 2 * S_cur) + xp.sqrt(2.0 * dt) * noise\n",
        "            S_proxy = kss_score_t(y_hat, t_prev, x0_ref, s0_ref, loglik_fn=like_fn_z, w_correct=w_correct)\n",
        "\n",
        "            if mode == 'ou_sde':\n",
        "                y = y_hat\n",
        "            elif mode == 'pf_ode':\n",
        "                y = y + dt * (y + S_cur)\n",
        "            elif mode == 'heun_pc':\n",
        "                drift_avg = 0.5 * ((y + 2 * S_cur) + (y_hat + 2 * S_proxy))\n",
        "                y = y + dt * drift_avg + xp.sqrt(2.0 * dt) * noise\n",
        "            elif mode == 'heun_hop':\n",
        "                A, B = xp.exp(dt), (xp.exp(2 * dt) - 1.0)\n",
        "                C, D = xp.sqrt(B), B / A\n",
        "                y = A * y + h_coeff * B * S_proxy + (1 - h_coeff) * D * S_cur + C * noise\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown mode {mode}\")\n",
        "\n",
        "        y_x = apply_linear(y, Winv, xp=xp)\n",
        "        if true_score_func:\n",
        "            S_true1 = true_score_func(y_x, t_cur)\n",
        "            S_true2 = true_score_func(apply_linear(y_hat, Winv, xp=xp), t_prev)\n",
        "            rmse_list.append(float(xp_rmse(apply_linear(S_cur, W.T, xp=xp), S_true1)))\n",
        "            rmse_list.append(float(xp_rmse(apply_linear(S_proxy, W.T, xp=xp), S_true2)))\n",
        "            return y_x, rmse_list\n",
        "        else:\n",
        "            return y_x\n",
        "\n",
        "    # no whitening → original behavior\n",
        "    x0_ref = x0_ref_x\n",
        "    s0_ref = s0_ref_x\n",
        "    for k_step in range(len(time_pts)-1):\n",
        "        t_cur, t_prev = time_pts[k_step], time_pts[k_step+1]\n",
        "        dt = t_cur - t_prev\n",
        "        noise = xp.random.randn(N_part, dim).astype(xp.float64)\n",
        "\n",
        "        S_cur = kss_score_t(y, t_cur, x0_ref, s0_ref,\n",
        "                            loglik_fn=likelyhood_func,\n",
        "                            w_correct=w_correct)\n",
        "        y_hat = y + dt * (y + 2 * S_cur) + xp.sqrt(2.0 * dt) * noise\n",
        "        S_proxy = kss_score_t(y_hat, t_prev, x0_ref, s0_ref,\n",
        "                              loglik_fn=likelyhood_func,\n",
        "                              w_correct=w_correct)\n",
        "\n",
        "        if mode == 'ou_sde':\n",
        "            y = y_hat\n",
        "        elif mode == 'pf_ode':\n",
        "            y = y + dt * (y + S_cur)\n",
        "        elif mode == 'heun_pc':\n",
        "            drift_avg = 0.5 * ((y + 2 * S_cur) + (y_hat + 2 * S_proxy))\n",
        "            y = y + dt * drift_avg + xp.sqrt(2.0 * dt) * noise\n",
        "        elif mode == 'heun_hop':\n",
        "            A, B = xp.exp(dt), (xp.exp(2 * dt) - 1.0)\n",
        "            C, D = xp.sqrt(B), B / A\n",
        "            y = A * y + h_coeff * B * S_proxy + (1 - h_coeff) * D * S_cur + C * noise\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown mode {mode}\")\n",
        "\n",
        "    if true_score_func:\n",
        "        S_true1 = true_score_func(y, t_cur)\n",
        "        S_true2 = true_score_func(y_hat, t_prev)\n",
        "        rmse_list.append(float(xp_rmse(S_cur,  S_true1)))\n",
        "        rmse_list.append(float(xp_rmse(S_proxy,S_true2)))\n",
        "        return y, rmse_list\n",
        "    else:\n",
        "        return y\n",
        "\n",
        "\n",
        "def blackbox_sampler(\n",
        "    N_part, time_pts, custom_score, sampler_func, *,\n",
        "    mode=\"heun_hop\", h_coeff=0.5, true_score_func=None,\n",
        "    p_prune=0, likelyhood_func=None, loglik_grad_fn=None,\n",
        "    # NEW\n",
        "    whiten_scores=False,\n",
        "    score_anchors=None,          # optional: array of anchor scores to build W\n",
        "    whiten_center=True, whiten_det_one=True, whiten_lam_min=1e-8, whiten_lam_max=None\n",
        "):\n",
        "    xp  = cp\n",
        "    dim = int(sampler_func(5).shape[1])\n",
        "    y   = xp.random.randn(N_part, dim).astype(xp.float64)\n",
        "    rmse_list = []\n",
        "\n",
        "    if not whiten_scores:\n",
        "        # original path\n",
        "        for k in range(len(time_pts)-1):\n",
        "            t_cur, t_prev = time_pts[k], time_pts[k+1]\n",
        "            dt    = t_cur - t_prev\n",
        "            noise = xp.random.randn(N_part, dim).astype(xp.float64)\n",
        "            S_cur  = custom_score(y, t_cur)\n",
        "            y_hat  = y + dt*(y + 2*S_cur) + xp.sqrt(2.0*dt)*noise\n",
        "            S_prev = custom_score(y_hat, t_prev)\n",
        "            if mode == \"heun_hop\":\n",
        "                A = xp.exp(dt); B = xp.exp(2*dt) - 1.0; C = xp.sqrt(B); D = B / A\n",
        "                y = A*y + h_coeff*B*S_prev + (1-h_coeff)*D*S_cur + C*noise\n",
        "            elif mode == \"ou_sde\":  y = y_hat\n",
        "            elif mode == \"pf_ode\":  y = y + dt*(y + S_cur)\n",
        "            elif mode == \"heun_pc\":\n",
        "                drift_avg = 0.5*(y + 2*S_cur) + 0.5*(y_hat + 2*S_prev)\n",
        "                y = y + dt*drift_avg + xp.sqrt(2.0 * dt) * noise\n",
        "            else:\n",
        "                raise ValueError(mode)\n",
        "        if true_score_func is not None:\n",
        "            S_cur_true = true_score_func(y, t_cur)\n",
        "            S_prev_true = true_score_func(y_hat, t_prev)\n",
        "            S_cur_safe, cur_idx_safe = prune_cp_arr(S_cur, p_percent=p_prune)\n",
        "            S_prev_safe, prev_idx_safe = prune_cp_arr(S_prev, p_percent=p_prune)\n",
        "            RMSE_cur = float(xp_rmse(S_cur_safe,  S_cur_true[cur_idx_safe]))\n",
        "            RMSE_prev = float(xp_rmse(S_prev_safe, S_prev_true[prev_idx_safe]))\n",
        "            rmse_list.extend([RMSE_cur, RMSE_prev])\n",
        "            return y, rmse_list\n",
        "        else:\n",
        "            return y\n",
        "\n",
        "    # whitening path for blackbox\n",
        "    if score_anchors is None:\n",
        "        # get a modest anchor set to build W\n",
        "        x0_ref = sampler_func(min(4096, N_part*2))\n",
        "        s0_ref = custom_score(x0_ref, 0.0)  # or at t≈0\n",
        "    else:\n",
        "        s0_ref = score_anchors\n",
        "\n",
        "    W, Winv, WTinv, _ = build_score_whitener(\n",
        "        s0_ref, center=whiten_center, det_one=whiten_det_one,\n",
        "        lam_min=whiten_lam_min, lam_max=whiten_lam_max\n",
        "    )\n",
        "    # wrap blackbox score: s_z(z,t) = W^{-T} s_x(W^{-1} z,t)\n",
        "    custom_score_z = wrap_blackbox_score_for_whiten(custom_score, Winv, WTinv)\n",
        "    true_score_z   = wrap_blackbox_score_for_whiten(true_score_func, Winv, WTinv) if true_score_func else None\n",
        "\n",
        "    # evolve in z-space\n",
        "    for k in range(len(time_pts)-1):\n",
        "        t_cur, t_prev = time_pts[k], time_pts[k+1]\n",
        "        dt    = t_cur - t_prev\n",
        "        noise = xp.random.randn(N_part, dim).astype(xp.float64)\n",
        "\n",
        "        S_cur  = custom_score_z(y, t_cur)\n",
        "        y_hat  = y + dt*(y + 2*S_cur) + xp.sqrt(2.0*dt)*noise\n",
        "        S_prev = custom_score_z(y_hat, t_prev)\n",
        "\n",
        "        if mode == \"heun_hop\":\n",
        "            A = xp.exp(dt); B = xp.exp(2*dt) - 1.0; C = xp.sqrt(B); D = B / A\n",
        "            y = A*y + h_coeff*B*S_prev + (1-h_coeff)*D*S_cur + C*noise\n",
        "        elif mode == \"ou_sde\":  y = y_hat\n",
        "        elif mode == \"pf_ode\":  y = y + dt*(y + S_cur)\n",
        "        elif mode == \"heun_pc\":\n",
        "            drift_avg = 0.5*(y + 2*S_cur) + 0.5*(y_hat + 2*S_prev)\n",
        "            y = y + dt*drift_avg + xp.sqrt(2.0 * dt) * noise\n",
        "        else:\n",
        "            raise ValueError(mode)\n",
        "\n",
        "    # map back\n",
        "    y_x = apply_linear(y, Winv, xp=xp)\n",
        "\n",
        "    if true_score_z is not None:\n",
        "        S_cur_true = true_score_z(y, t_cur)\n",
        "        S_prev_true = true_score_z(y_hat, t_prev)\n",
        "        S_cur_safe, cur_idx_safe = prune_cp_arr(S_cur, p_percent=p_prune)\n",
        "        S_prev_safe, prev_idx_safe = prune_cp_arr(S_prev, p_percent=p_prune)\n",
        "        RMSE_cur = float(xp_rmse(S_cur_safe,  S_cur_true[cur_idx_safe]))\n",
        "        RMSE_prev = float(xp_rmse(S_prev_safe, S_prev_true[prev_idx_safe]))\n",
        "        rmse_list.extend([RMSE_cur, RMSE_prev])\n",
        "        return y_x, rmse_list\n",
        "    else:\n",
        "        return y_x\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "def _to_numpy(arr):\n",
        "    \"\"\"Return a NumPy array, copying from GPU if input is CuPy.\"\"\"\n",
        "    try:\n",
        "        import cupy as cp\n",
        "        if isinstance(arr, cp.ndarray):\n",
        "            return arr.get()\n",
        "    except ImportError:\n",
        "        pass\n",
        "    return np.asarray(arr)\n",
        "\n",
        "\n",
        "def div_unique_pairs(n: int):\n",
        "    pairs, cnt, out = list(combinations(range(1, n + 1), 2)), [0]*(n + 1), []\n",
        "    while pairs:\n",
        "        p = min(pairs, key=lambda t: (cnt[t[0]] + cnt[t[1]], t))\n",
        "        pairs.remove(p); out.append(p)\n",
        "        cnt[p[0]] += 1; cnt[p[1]] += 1\n",
        "    return out\n",
        "\n",
        "\n",
        "def get_row_norm(norm_ref, i,j, bins, norm_mode = \"max\"):\n",
        "    H_true, xedges, yedges = np.histogram2d(\n",
        "          norm_ref[:, i], norm_ref[:, j], bins=bins, density=False)\n",
        "    density_true = H_true / norm_ref.shape[0]\n",
        "    if norm_mode == \"max\":\n",
        "        vmax = density_true.max()\n",
        "    elif norm_mode == \"median\":\n",
        "        vmax = 2.0 * np.median(density_true[density_true > 0])\n",
        "    else:\n",
        "        raise ValueError(\"norm_mode must be 'max' or 'median'.\")\n",
        "    norm = plt.Normalize(vmin=0.0, vmax=vmax)\n",
        "    return norm\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_likelihood_contours(\n",
        "    ax,\n",
        "    likelihood_func,\n",
        "    xlim, ylim,\n",
        "    *,\n",
        "    levels=5,\n",
        "    color=\"white\",\n",
        "    alpha=0.9,\n",
        "    linewidths=1.2,\n",
        "    linestyles=\"solid\",\n",
        "    grid_res=200,\n",
        "    is_log_density=False,\n",
        "    normalize=True,\n",
        "    transform=None,\n",
        "    chunk_size=200_000,\n",
        "    zorder=12,\n",
        "    backend=\"auto\",\n",
        "    full_dim=None,\n",
        "    ij=None,\n",
        "    anchor=None,\n",
        "    levels_mode=\"auto\",\n",
        "    percentile_range=(50, 99.5),\n",
        "    coverage_clip=1/3,\n",
        "    min_level=None,\n",
        "    mass_levels=None,\n",
        "    # --- debug ---\n",
        "    debug_save_path=None,\n",
        "    debug_save_raw=False,\n",
        "    verbose=False,\n",
        "):\n",
        "    xs = np.linspace(xlim[0], xlim[1], int(grid_res))\n",
        "    ys = np.linspace(ylim[0], ylim[1], int(grid_res))\n",
        "    X, Y = np.meshgrid(xs, ys, indexing=\"xy\")\n",
        "    P2 = np.stack([X.ravel(), Y.ravel()], axis=1)\n",
        "\n",
        "    try:\n",
        "        import cupy as cp\n",
        "    except Exception:\n",
        "        cp = None\n",
        "\n",
        "    def is_cupy(a): return (cp is not None) and isinstance(a, cp.ndarray)\n",
        "    def to_backend(a, use_cp):\n",
        "        if use_cp:\n",
        "            if cp is None: raise RuntimeError(\"backend='cupy' but CuPy unavailable.\")\n",
        "            return cp.asarray(a)\n",
        "        return np.asarray(a)\n",
        "\n",
        "    use_cp = (backend == \"cupy\") or (backend == \"auto\" and cp is not None)\n",
        "\n",
        "    # ---- lift 2D -> d if requested ----\n",
        "    if full_dim is not None and ij is not None:\n",
        "        i, j = ij\n",
        "        xp   = cp if use_cp else np\n",
        "        P2xp = to_backend(P2, use_cp)\n",
        "        if anchor is None:\n",
        "            anchor_xp = xp.zeros(full_dim, dtype=P2xp.dtype)\n",
        "        else:\n",
        "            anchor_xp = xp.asarray(anchor, dtype=P2xp.dtype)\n",
        "            if anchor_xp.shape != (full_dim,):\n",
        "                raise ValueError(f\"`anchor` must have shape ({full_dim},), got {anchor_xp.shape}\")\n",
        "        U = xp.broadcast_to(anchor_xp, (P2xp.shape[0], full_dim)).copy()\n",
        "        U[:, i] = P2xp[:, 0]\n",
        "        U[:, j] = P2xp[:, 1]\n",
        "    else:\n",
        "        U = to_backend(transform(P2) if transform is not None else P2, use_cp)\n",
        "\n",
        "    # ---- evaluate likelihood ----\n",
        "    try:\n",
        "        vals = likelihood_func(U)\n",
        "    except Exception:\n",
        "        out = []\n",
        "        N = U.shape[0]\n",
        "        for k in range(0, N, chunk_size):\n",
        "            out.append(likelihood_func(U[k:k+chunk_size]))\n",
        "        vals = (cp.concatenate(out, 0) if use_cp else np.concatenate(out, 0))\n",
        "\n",
        "    Z = cp.asnumpy(vals) if is_cupy(vals) else np.asarray(vals)\n",
        "    Z = Z.reshape(Y.shape)\n",
        "\n",
        "    # convert log-like -> like\n",
        "    if is_log_density:\n",
        "        Z = Z - np.nanmax(Z)\n",
        "        Z = np.exp(Z)\n",
        "\n",
        "    # optional normalization (does not affect mass-level ordering)\n",
        "    if normalize:\n",
        "        m = np.nanmax(Z)\n",
        "        if np.isfinite(m) and m > 0:\n",
        "            Z = Z / m\n",
        "\n",
        "    zfin = Z[np.isfinite(Z)]\n",
        "    if zfin.size == 0:\n",
        "        if verbose: print(\"[contours] all values non-finite; skip\")\n",
        "        return None\n",
        "\n",
        "    # === NEW: HDR mass-based levels ===\n",
        "    if mass_levels is not None:\n",
        "        masses = np.sort(np.clip(np.asarray(mass_levels, float), 0.0, 0.999999))\n",
        "        # compute highest-density thresholds: find t s.t. ∫_{Z>=t} Z dx dy = m * ∫ Z dx dy\n",
        "        dx = (xs[-1] - xs[0]) / (len(xs) - 1) if len(xs) > 1 else 1.0\n",
        "        dy = (ys[-1] - ys[0]) / (len(ys) - 1) if len(ys) > 1 else 1.0\n",
        "        area = dx * dy\n",
        "        vals1d = Z.ravel()\n",
        "        order = np.argsort(vals1d)[::-1]\n",
        "        sorted_vals = vals1d[order]\n",
        "        cums = np.cumsum(sorted_vals) * area\n",
        "        total = cums[-1] if cums.size else 1.0\n",
        "        lev = []\n",
        "        actual = []\n",
        "        for m in masses:\n",
        "            k = np.searchsorted(cums, m * total, side=\"left\")\n",
        "            thr = sorted_vals[k] if k < sorted_vals.size else sorted_vals[-1]\n",
        "            lev.append(thr)\n",
        "            # report actual enclosed mass with this threshold\n",
        "            mask = Z >= thr\n",
        "            actual_mass = (Z[mask].sum() * area) / total\n",
        "            actual.append(actual_mass)\n",
        "        lev = np.array(sorted(lev))  # ascending for contour\n",
        "        if verbose:\n",
        "            print(\"[contours] HDR mass levels requested:\", masses)\n",
        "            print(\"[contours] actual enclosed masses:    \", np.round(actual, 4))\n",
        "    else:\n",
        "        # --- old value-based level selection (kept for compatibility) ---\n",
        "        if isinstance(levels, int):\n",
        "            zmin, zmax = float(np.min(zfin)), float(np.max(zfin))\n",
        "            if levels_mode == \"percentile\":\n",
        "                lo, hi = np.percentile(zfin, percentile_range)\n",
        "            elif levels_mode == \"linear\":\n",
        "                lo, hi = zmin, zmax\n",
        "            else:\n",
        "                p95 = np.percentile(zfin, 95.0)\n",
        "                lo, hi = (np.percentile(zfin, percentile_range)\n",
        "                          if p95 < 0.15 else (zmin, zmax))\n",
        "            eps = 1e-12 * max(1.0, abs(hi))\n",
        "            lev = np.linspace(lo + eps, hi - eps, max(1, levels))\n",
        "        else:\n",
        "            lev = np.asarray(levels, float)\n",
        "\n",
        "        if min_level is not None:\n",
        "            lev = lev[lev >= float(min_level)]\n",
        "\n",
        "        if coverage_clip is not None and lev.size > 0:\n",
        "            keep = []\n",
        "            for lv in lev:\n",
        "                cov = np.mean(Z >= lv)\n",
        "                keep.append(cov <= coverage_clip)\n",
        "            keep = np.array(keep, bool)\n",
        "            if keep.any():\n",
        "                lev = lev[keep]\n",
        "            if lev.size == 0:\n",
        "                lev = np.sort(np.unique(np.percentile(zfin, np.linspace(85, 99.5, max(3, levels if isinstance(levels, int) else 6)))))\n",
        "\n",
        "    if lev.size == 0:\n",
        "        if verbose: print(\"[contours] no levels; skip\")\n",
        "        return None\n",
        "\n",
        "    cs = ax.contour(\n",
        "        X, Y, Z, levels=lev,\n",
        "        colors=color, linewidths=linewidths, linestyles=linestyles,\n",
        "        alpha=alpha, zorder=zorder, antialiased=True,\n",
        "    )\n",
        "\n",
        "    if debug_save_path is not None:\n",
        "        fig, axd = plt.subplots(1, 1, dpi=200)\n",
        "        im = axd.imshow(Z, origin=\"lower\",\n",
        "                        extent=[xs[0], xs[-1], ys[0], ys[-1]],\n",
        "                        aspect=\"equal\", cmap=\"magma\")\n",
        "        fig.colorbar(im, ax=axd, fraction=0.046, pad=0.04).set_label(\"likelihood (scaled)\" if normalize else \"likelihood\")\n",
        "        axd.contour(X, Y, Z, levels=lev, colors=\"w\", linewidths=1.2)\n",
        "        axd.set_xlim(xlim); axd.set_ylim(ylim)\n",
        "        axd.set_title(\"Likelihood contours (HDR mass levels)\" if mass_levels is not None else \"Likelihood contours\")\n",
        "        fig.savefig(debug_save_path, bbox_inches=\"tight\"); plt.close(fig)\n",
        "        if debug_save_raw:\n",
        "            base = debug_save_path.rsplit(\".\", 1)[0]\n",
        "            np.savez(base + \".npz\", X=X, Y=Y, Z=Z, levels=np.asarray(lev),\n",
        "                     xlim=np.asarray(xlim), ylim=np.asarray(ylim))\n",
        "\n",
        "    return cs\n",
        "\n",
        "\n",
        "'''\n",
        "def plot_pair_histograms(\n",
        "        y_finals_dict: dict,\n",
        "        prior_samples,\n",
        "        post_samples,\n",
        "        save_path: str,\n",
        "        dim_pairs: list[tuple[int, int]] = None,\n",
        "        *,\n",
        "        bins: int = 100,\n",
        "        norm_mode: str = \"max\",\n",
        "        hist_norm: float = 1.25,\n",
        "        nrows: int = 3,\n",
        "        mode: str = 'first',\n",
        "        display_mode: str = 'standard',\n",
        "        pre_process_f=None,       # <--- NEW: optional preprocessing callable\n",
        "        colormap: str = 'inferno',\n",
        "        post_ref = True,\n",
        "        likelyhood_func = None,\n",
        "        draw_countours = True,\n",
        "        prior_scale = True,\n",
        "        plot_prior = True,\n",
        "        plot_post = True,\n",
        "        post_hist_norm = .8):\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # prep data & optional preprocessing\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def _to_numpy_local(arr):\n",
        "        try:\n",
        "            import cupy as _cp\n",
        "            if isinstance(arr, _cp.ndarray):\n",
        "                return arr.get()\n",
        "        except Exception:\n",
        "            pass\n",
        "        return np.asarray(arr)\n",
        "\n",
        "\n",
        "    true_np = _to_numpy_local(prior_samples)\n",
        "    if post_samples is not None and plot_post:\n",
        "        post_dict = {'Posterior': _to_numpy_local(post_samples)}\n",
        "        y_finals_dict = {**post_dict, **y_finals_dict}\n",
        "\n",
        "\n",
        "    #true_np = _to_numpy_local(true_samples)\n",
        "    y_finals_np = {k: _to_numpy_local(v) for k, v in y_finals_dict.items()}\n",
        "\n",
        "    # apply preprocessing if provided\n",
        "    if pre_process_f is not None:\n",
        "        true_np = _to_numpy_local(pre_process_f(true_np))\n",
        "        y_finals_np = {k: _to_numpy_local(pre_process_f(v)) for k, v in y_finals_np.items()}\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # choose coordinate pairs on the (possibly transformed) space\n",
        "    # ------------------------------------------------------------------ #\n",
        "    if dim_pairs is None:\n",
        "        d = true_np.shape[1]\n",
        "        pairs = div_unique_pairs(d)\n",
        "        if mode == 'first':\n",
        "            dim_pairs = pairs[: min(nrows, d)]\n",
        "        elif mode == 'rand':\n",
        "            idx = np.random.choice(len(pairs), min(nrows, d), replace=False)\n",
        "            dim_pairs = [pairs[i] for i in idx]\n",
        "        elif mode == 'pca':\n",
        "            pca = PCA(n_components=min(32, d))\n",
        "            pca.fit(true_np)\n",
        "            sorted_idx = np.argsort(-pca.explained_variance_)\n",
        "            dim_pairs = [(sorted_idx[2*k] + 1, sorted_idx[2*k+1] + 1)\n",
        "                        for k in range(min(nrows, d // 2))]\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown mode {mode}\")\n",
        "\n",
        "    # convert to 0-based indices\n",
        "    pairs = [(i - 1, j - 1) for (i, j) in dim_pairs]\n",
        "    max_dim = max(max(p) for p in pairs)\n",
        "    if true_np.shape[1] <= max_dim:\n",
        "        raise ValueError(\"Requested dimension exceeds data dimensionality.\")\n",
        "    for name, gen in y_finals_np.items():\n",
        "        if gen.shape[1] <= max_dim:\n",
        "            raise ValueError(f\"{name!r} has too few dimensions for selected pairs.\")\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # figure grid\n",
        "    # ------------------------------------------------------------------ #\n",
        "    n_rows = len(pairs)\n",
        "    if plot_prior:\n",
        "      n_cols = 1 + len(y_finals_np)\n",
        "    else:\n",
        "      n_cols = len(y_finals_np)\n",
        "\n",
        "    fig_w = 4 * n_cols\n",
        "    fig_h = 4 * n_rows\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(fig_w, fig_h), squeeze=False)\n",
        "\n",
        "    # spacing\n",
        "    if display_mode == 'min_label':\n",
        "        plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
        "    else:\n",
        "        plt.subplots_adjust(wspace=0.25, hspace=0.25)\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # loop over coordinate pairs\n",
        "    # ------------------------------------------------------------------ #\n",
        "\n",
        "    if post_samples is not None and post_ref:\n",
        "        norm_ref = _to_numpy_local(post_samples)\n",
        "        h_ref = post_hist_norm\n",
        "    else:\n",
        "        norm_ref = true_np\n",
        "        h_ref = hist_norm\n",
        "\n",
        "    if not prior_scale:\n",
        "        plot_refs = norm_ref\n",
        "    else:\n",
        "        plot_refs = true_np\n",
        "\n",
        "    for row_idx, (i, j) in enumerate(pairs):\n",
        "\n",
        "        H_true, xedges, yedges = np.histogram2d(\n",
        "            plot_refs[:, i], plot_refs[:, j], bins=bins, density=False\n",
        "        )\n",
        "        norm = get_row_norm(_to_numpy_local(norm_ref), i,j, bins, norm_mode)\n",
        "\n",
        "        def _draw(ax, data_np, n_samples, title, draw_title: bool,\n",
        "                  plt_norm = norm, h_norm = hist_norm,\n",
        "                  likelihood_func=None, like_levels=6, like_color=\"#D3D3D3\",\n",
        "                  like_alpha=.92,like_linewidths=1.5, like_linestyles=\"solid\",\n",
        "                  like_grid_res=200,like_is_log_density=False,\n",
        "                  like_normalize=True, like_transform=None,\n",
        "                  like_backend=\"auto\",like_full_dim=None,\n",
        "                  mass_levels = [.5, .85, .975],\n",
        "                  percentiles = (70, 99.5),\n",
        "                  like_ij=None, like_anchor=None, debug_save_path = None):\n",
        "\n",
        "            d = data_np.shape[1]\n",
        "            H, _, _ = np.histogram2d(\n",
        "                data_np[:, i], data_np[:, j],\n",
        "                bins=[xedges, yedges], density=False)\n",
        "            density = (h_norm * H / n_samples) ** .95\n",
        "            ax.imshow(\n",
        "                density.T,\n",
        "                origin=\"lower\",\n",
        "                cmap=colormap,\n",
        "                norm=plt_norm,\n",
        "                extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]],\n",
        "                aspect=\"auto\",\n",
        "                interpolation=\"nearest\",)\n",
        "\n",
        "            if display_mode == 'min_label':\n",
        "                ax.set_xticks([]); ax.set_yticks([])\n",
        "            else:\n",
        "                ax.set_xlabel(f\"$d_{{{i+1}}}$\", fontsize=15)\n",
        "                ax.set_ylabel(f\"$d_{{{j+1}}}$\", fontsize=15)\n",
        "            if draw_title:\n",
        "                ax.set_title(title, fontsize=16)\n",
        "\n",
        "            if likelihood_func is not None:\n",
        "                xlim = ax.get_xlim()\n",
        "                ylim = ax.get_ylim()\n",
        "                plot_likelihood_contours( ax,likelihood_func,xlim, ylim,\n",
        "                    levels=like_levels,color=like_color,\n",
        "                    alpha=like_alpha, linewidths=like_linewidths,\n",
        "                    linestyles=like_linestyles,\n",
        "                    grid_res=like_grid_res, is_log_density=like_is_log_density,\n",
        "                    normalize=like_normalize,transform=like_transform,\n",
        "                    backend=like_backend,full_dim=d,\n",
        "                    ij=like_ij, anchor=like_anchor,\n",
        "                    percentile_range = percentiles,\n",
        "                    mass_levels = mass_levels,\n",
        "                    debug_save_path= None)\n",
        "\n",
        "        # column 0: ground truth\n",
        "        prior_norm = get_row_norm(true_np, i,j, bins, norm_mode)\n",
        "\n",
        "        if likelyhood_func is not None and draw_countours:\n",
        "           def like_wrapper(U_np):\n",
        "                return cp.asnumpy(likelyhood_func(cp.asarray(U_np)))\n",
        "        else:\n",
        "           like_wrapper = None\n",
        "\n",
        "        if plot_prior:\n",
        "          _draw(\n",
        "              axes[row_idx, 0],\n",
        "              true_np, true_np.shape[0],\n",
        "              True, #\"Prior & Likelyhood\",\n",
        "              draw_title=(row_idx == 0),\n",
        "              plt_norm = prior_norm,\n",
        "              h_norm = hist_norm,\n",
        "              like_full_dim=d,              # lift 2D -> 9D\n",
        "              like_is_log_density=True,\n",
        "              like_ij=(i, j),\n",
        "              like_anchor=None,\n",
        "              debug_save_path= None,\n",
        "              likelihood_func = like_wrapper)\n",
        "\n",
        "        # remaining columns: each sampler\n",
        "        for col_idx, (lab, y_gen) in enumerate(y_finals_np.items(),\n",
        "                                               start=int(plot_prior)):\n",
        "            _draw(\n",
        "                axes[row_idx, col_idx],\n",
        "                y_gen, y_gen.shape[0],\n",
        "                lab,\n",
        "                draw_title=(row_idx == 0),\n",
        "                h_norm = h_ref,\n",
        "                likelihood_func  = None)\n",
        "\n",
        "        # compact y-label column (min_label mode)\n",
        "        if display_mode == 'min_label':\n",
        "            ax0 = axes[row_idx, 0]\n",
        "            label = f\"$d_{{{i+1}}}\\\\,v\\\\,d_{{{j+1}}}$\"\n",
        "            ax0.set_ylabel(label, rotation=90, labelpad=20,\n",
        "                           va='center', ha='center', fontsize=18)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.close(fig)\n",
        "    '''\n",
        "\n",
        "def plot_pair_histograms(\n",
        "        y_finals_dict: dict,\n",
        "        prior_samples,\n",
        "        post_samples,\n",
        "        save_path: str,\n",
        "        dim_pairs: list[tuple[int, int]] = None,\n",
        "        *,\n",
        "        bins: int = 100,\n",
        "        norm_mode: str = \"max\",\n",
        "        hist_norm: float = 1.25,\n",
        "        nrows: int = 3,\n",
        "        mode: str = 'first',\n",
        "        display_mode: str = 'standard',\n",
        "        pre_process_f=None,\n",
        "        colormap: str = 'inferno',\n",
        "        post_ref = True,\n",
        "        likelyhood_func = None,\n",
        "        draw_countours = True,\n",
        "        prior_scale = True,\n",
        "        plot_prior = True,\n",
        "        plot_post = True,\n",
        "        post_hist_norm = .8,\n",
        "        fontsize: int = 20):  # <--- NEW: parameter added here\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # prep data & optional preprocessing\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def _to_numpy_local(arr):\n",
        "        try:\n",
        "            import cupy as _cp\n",
        "            if isinstance(arr, _cp.ndarray):\n",
        "                return arr.get()\n",
        "        except Exception:\n",
        "            pass\n",
        "        return np.asarray(arr)\n",
        "\n",
        "\n",
        "    true_np = _to_numpy_local(prior_samples)\n",
        "    if post_samples is not None and plot_post:\n",
        "        post_dict = {'Posterior': _to_numpy_local(post_samples)}\n",
        "        y_finals_dict = {**post_dict, **y_finals_dict}\n",
        "\n",
        "\n",
        "    #true_np = _to_numpy_local(true_samples)\n",
        "    y_finals_np = {k: _to_numpy_local(v) for k, v in y_finals_dict.items()}\n",
        "\n",
        "    # apply preprocessing if provided\n",
        "    if pre_process_f is not None:\n",
        "        true_np = _to_numpy_local(pre_process_f(true_np))\n",
        "        y_finals_np = {k: _to_numpy_local(pre_process_f(v)) for k, v in y_finals_np.items()}\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # choose coordinate pairs on the (possibly transformed) space\n",
        "    # ------------------------------------------------------------------ #\n",
        "    if dim_pairs is None:\n",
        "        d = true_np.shape[1]\n",
        "        pairs = div_unique_pairs(d)\n",
        "        if mode == 'first':\n",
        "            dim_pairs = pairs[: min(nrows, d)]\n",
        "        elif mode == 'rand':\n",
        "            idx = np.random.choice(len(pairs), min(nrows, d), replace=False)\n",
        "            dim_pairs = [pairs[i] for i in idx]\n",
        "        elif mode == 'pca':\n",
        "            pca = PCA(n_components=min(32, d))\n",
        "            pca.fit(true_np)\n",
        "            sorted_idx = np.argsort(-pca.explained_variance_)\n",
        "            dim_pairs = [(sorted_idx[2*k] + 1, sorted_idx[2*k+1] + 1)\n",
        "                        for k in range(min(nrows, d // 2))]\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown mode {mode}\")\n",
        "\n",
        "    # convert to 0-based indices\n",
        "    pairs = [(i - 1, j - 1) for (i, j) in dim_pairs]\n",
        "    max_dim = max(max(p) for p in pairs)\n",
        "    if true_np.shape[1] <= max_dim:\n",
        "        raise ValueError(\"Requested dimension exceeds data dimensionality.\")\n",
        "    for name, gen in y_finals_np.items():\n",
        "        if gen.shape[1] <= max_dim:\n",
        "            raise ValueError(f\"{name!r} has too few dimensions for selected pairs.\")\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # figure grid\n",
        "    # ------------------------------------------------------------------ #\n",
        "    n_rows = len(pairs)\n",
        "    if plot_prior:\n",
        "      n_cols = 1 + len(y_finals_np)\n",
        "    else:\n",
        "      n_cols = len(y_finals_np)\n",
        "\n",
        "    fig_w = 4 * n_cols\n",
        "    fig_h = 4 * n_rows\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(fig_w, fig_h), squeeze=False)\n",
        "\n",
        "    # spacing\n",
        "    if display_mode == 'min_label':\n",
        "        plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
        "    else:\n",
        "        plt.subplots_adjust(wspace=0.25, hspace=0.25)\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # loop over coordinate pairs\n",
        "    # ------------------------------------------------------------------ #\n",
        "\n",
        "    if post_samples is not None and post_ref:\n",
        "        norm_ref = _to_numpy_local(post_samples)\n",
        "        h_ref = post_hist_norm\n",
        "    else:\n",
        "        norm_ref = true_np\n",
        "        h_ref = hist_norm\n",
        "\n",
        "    if not prior_scale:\n",
        "        plot_refs = norm_ref\n",
        "    else:\n",
        "        plot_refs = true_np\n",
        "\n",
        "    for row_idx, (i, j) in enumerate(pairs):\n",
        "\n",
        "        H_true, xedges, yedges = np.histogram2d(\n",
        "            plot_refs[:, i], plot_refs[:, j], bins=bins, density=False\n",
        "        )\n",
        "        norm = get_row_norm(_to_numpy_local(norm_ref), i,j, bins, norm_mode)\n",
        "\n",
        "        def _draw(ax, data_np, n_samples, title, draw_title: bool,\n",
        "                  plt_norm = norm, h_norm = hist_norm,\n",
        "                  likelihood_func=None, like_levels=6, like_color=\"#D3D3D3\",\n",
        "                  like_alpha=.92,like_linewidths=1.5, like_linestyles=\"solid\",\n",
        "                  like_grid_res=200,like_is_log_density=False,\n",
        "                  like_normalize=True, like_transform=None,\n",
        "                  like_backend=\"auto\",like_full_dim=None,\n",
        "                  mass_levels =  [.5, .85, .975],\n",
        "                  percentiles = [50, 85, 975],\n",
        "                  like_ij=None, like_anchor=None, debug_save_path = None):\n",
        "\n",
        "            d = data_np.shape[1]\n",
        "            H, _, _ = np.histogram2d(\n",
        "                data_np[:, i], data_np[:, j],\n",
        "                bins=[xedges, yedges], density=False)\n",
        "            density = (h_norm * H / n_samples) ** .95\n",
        "            ax.imshow(\n",
        "                density.T,\n",
        "                origin=\"lower\",\n",
        "                cmap=colormap,\n",
        "                norm=plt_norm,\n",
        "                extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]],\n",
        "                aspect=\"auto\",\n",
        "                interpolation=\"nearest\",)\n",
        "\n",
        "            if display_mode == 'min_label':\n",
        "                ax.set_xticks([]); ax.set_yticks([])\n",
        "            else:\n",
        "                # <--- MODIFIED: Uses fontsize variable\n",
        "                ax.set_xlabel(f\"$d_{{{i+1}}}$\", fontsize=fontsize)\n",
        "                ax.set_ylabel(f\"$d_{{{j+1}}}$\", fontsize=fontsize)\n",
        "            if draw_title:\n",
        "                ax.set_title(title, fontsize=fontsize)\n",
        "\n",
        "            if likelihood_func is not None:\n",
        "                xlim = ax.get_xlim()\n",
        "                ylim = ax.get_ylim()\n",
        "                plot_likelihood_contours( ax,likelihood_func,xlim, ylim,\n",
        "                    levels=like_levels,color=like_color,\n",
        "                    alpha=like_alpha, linewidths=like_linewidths,\n",
        "                    linestyles=like_linestyles,\n",
        "                    grid_res=like_grid_res, is_log_density=like_is_log_density,\n",
        "                    normalize=like_normalize,transform=like_transform,\n",
        "                    backend=like_backend,full_dim=d,\n",
        "                    ij=like_ij, anchor=like_anchor,\n",
        "                    percentile_range = percentiles,\n",
        "                    mass_levels = mass_levels,\n",
        "                    debug_save_path= None)\n",
        "\n",
        "        # column 0: ground truth\n",
        "        prior_norm = get_row_norm(true_np, i,j, bins, norm_mode)\n",
        "\n",
        "        if likelyhood_func is not None and draw_countours:\n",
        "           def like_wrapper(U_np):\n",
        "                return cp.asnumpy(likelyhood_func(cp.asarray(U_np)))\n",
        "        else:\n",
        "           like_wrapper = None\n",
        "\n",
        "        if plot_prior:\n",
        "          _draw(\n",
        "              axes[row_idx, 0],\n",
        "              true_np, true_np.shape[0],\n",
        "              True, #\"Prior & Likelyhood\",\n",
        "              draw_title=(row_idx == 0),\n",
        "              plt_norm = prior_norm,\n",
        "              h_norm = hist_norm,\n",
        "              like_full_dim=d,              # lift 2D -> 9D\n",
        "              like_is_log_density=True,\n",
        "              like_ij=(i, j),\n",
        "              like_anchor=None,\n",
        "              debug_save_path= None,\n",
        "              likelihood_func = like_wrapper)\n",
        "\n",
        "        # remaining columns: each sampler\n",
        "        for col_idx, (lab, y_gen) in enumerate(y_finals_np.items(),\n",
        "                                               start=int(plot_prior)):\n",
        "            _draw(\n",
        "                axes[row_idx, col_idx],\n",
        "                y_gen, y_gen.shape[0],\n",
        "                lab,\n",
        "                draw_title=(row_idx == 0),\n",
        "                h_norm = h_ref,\n",
        "                likelihood_func  = None)\n",
        "\n",
        "        # compact y-label column (min_label mode)\n",
        "        if display_mode == 'min_label':\n",
        "            ax0 = axes[row_idx, 0]\n",
        "            label = f\"$d_{{{i+1}}}\\\\,v\\\\,d_{{{j+1}}}$\"\n",
        "            # <--- MODIFIED: Uses fontsize variable\n",
        "            ax0.set_ylabel(label, rotation=90, labelpad=20,\n",
        "                           va='center', ha='center', fontsize=fontsize)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.close(fig)\n",
        "\n",
        "def lambda_two_step(N, T_end, T_target):\n",
        "    # --- first step only ----------------\n",
        "    delta = T_target ** (1.0 / N)\n",
        "    dt0   = T_end * (1.0 - delta)\n",
        "\n",
        "    B0    = math.exp(2*dt0) - 1.0\n",
        "    D0    = B0 / math.exp(dt0)\n",
        "    R0    = (math.exp(dt0) - 1.0 - dt0) / (dt0*math.exp(dt0)-math.exp(dt0)+1)\n",
        "    lam1  = (R0 * D0) / (B0 + R0 * D0)\n",
        "\n",
        "    # --- two-step cancellation ----------\n",
        "    dt1   = dt0 * delta\n",
        "    def R(dt): return (math.exp(dt)-1-dt)/(dt*math.exp(dt)-math.exp(dt)+1)\n",
        "    def B(dt): return math.exp(2*dt)-1\n",
        "    def D(dt): return B(dt)/math.exp(dt)\n",
        "    lam2_num = R(dt0)*D(dt0)*dt0**2 + R(dt1)*D(dt1)*dt1**2\n",
        "    lam2_den = B(dt0)*dt0**2       + B(dt1)*dt1**2\n",
        "    lam2     = lam2_num / lam2_den\n",
        "\n",
        "    return .5 * (lam1+lam2)\n",
        "\n",
        "\n",
        "\n",
        "def power_grid(t0, tT, n, gamma=1.0):  # gamma<1 flattens big steps\n",
        "    frac = (tT / t0)\n",
        "    exps = (np.arange(n+1) / n) ** gamma\n",
        "    return t0 * frac ** exps\n",
        "\n",
        "\n",
        "# --- Get local Gaussian stats (mu_i, var_i+r) used by proxy Tweedie ----\n",
        "def local_gaussian_stats(x_ref, params=None):\n",
        "    \"\"\"\n",
        "    Returns (mu, var_r) from the same local Gaussian fit used in fit_kernel_score.\n",
        "      mu   : (N,D) weighted local means\n",
        "      var_r: (N,D) diagonal variances with ridge added\n",
        "    \"\"\"\n",
        "    if params is None:\n",
        "        params = {}\n",
        "\n",
        "    X = cp.asarray(x_ref)\n",
        "    N, D = X.shape\n",
        "    dtype = params.get(\"dtype\", cp.float32)\n",
        "    X = X.astype(dtype, copy=False)\n",
        "\n",
        "    # kNN with adaptive bandwidths (same as fit_kernel_score)\n",
        "    N0, k0, alpha = 2000, max(4*D, 48), 0.4\n",
        "    k_default = int(min(N-1, max(4*D, round(k0 * (X.shape[0]/N0) ** alpha))))\n",
        "    k = int(params.get(\"k\", k_default))\n",
        "    k = max(1, min(k, N - 1))\n",
        "\n",
        "    x2 = cp.sum(X * X, axis=1, keepdims=True)\n",
        "    dist2 = cp.maximum(x2 + x2.T - 2.0 * (X @ X.T), 0.0)\n",
        "    cp.fill_diagonal(dist2, cp.inf)\n",
        "    idx = cp.argpartition(dist2, kth=k - 1, axis=1)[:, :k]\n",
        "    d2_knn = cp.take_along_axis(dist2, idx, axis=1)\n",
        "    h2 = cp.maximum(cp.max(d2_knn, axis=1), cp.finfo(dtype).tiny)\n",
        "\n",
        "    w   = cp.exp(-d2_knn / (2.0 * h2[:, None]))\n",
        "    Xnb = X[idx]  # (N,k,D)\n",
        "    denom = cp.sum(w, axis=1, keepdims=True) + cp.finfo(dtype).eps\n",
        "    mu   = cp.sum(w[:, :, None] * Xnb, axis=1) / denom  # (N,D)\n",
        "    diff = Xnb - mu[:, None, :]\n",
        "    var  = cp.sum(w[:, :, None] * diff * diff, axis=1) / denom  # (N,D)\n",
        "\n",
        "    ridge_frac = float(params.get(\"ridge_frac\", 1e-3))\n",
        "    tau = ridge_frac * cp.mean(var, axis=1, keepdims=True)       # (N,1)\n",
        "    var_r = var + tau\n",
        "    return mu.astype(cp.float64), var_r.astype(cp.float64)\n",
        "\n",
        "\n",
        "\n",
        "def fit_kernel_score(x_ref, params=None):\n",
        "    \"\"\"\n",
        "    Local Gaussian (diagonal) kernel score fit, with optional GMM/KDE score recomputation.\n",
        "\n",
        "    Inputs:\n",
        "      x_ref   : (N, D) array-like (moved to GPU).\n",
        "      params  : dict (optional)\n",
        "        - k            : int, kNN size for local fit (default ~4*D, clipped to N-1)\n",
        "        - ridge_frac   : float, ridge as fraction of per-point mean var (default: 1e-2)\n",
        "        - dtype        : cp.float32 or cp.float64 (default: float32)\n",
        "        - recompute    : bool, if True return GMM/KDE score at anchors (default: False)\n",
        "        - gmm_batch    : int, component batch size for recompute (default: 1024)\n",
        "\n",
        "    Returns:\n",
        "      s_hat : (N, D) CuPy array.\n",
        "              If recompute=False: local score  s_i = (mu_i - x_i) / (var_i + ridge).\n",
        "              If recompute=True : GMM/KDE score s(x_i) = [∑_j N(x_i|mu_j,Σ_j) Σ_j^{-1}(mu_j-x_i)] / [∑_j N(x_i|mu_j,Σ_j)].\n",
        "    \"\"\"\n",
        "    if params is None:\n",
        "        params = {}\n",
        "\n",
        "    X = cp.asarray(x_ref)\n",
        "    N, D = X.shape\n",
        "    dtype = params.get(\"dtype\", cp.float32)\n",
        "    X = X.astype(dtype, copy=False)\n",
        "\n",
        "    # -------- Local fit (as before) -----------------------------------------\n",
        "    N0, k0, alpha = 2000, max(4*D, 48), 0.4\n",
        "    k_default = int(min(N-1, max(4*D, round(k0 * (X.shape[0]/N0) ** alpha))))\n",
        "    k = int(params.get(\"k\", k_default))\n",
        "    k = max(1, min(k, N - 1))\n",
        "\n",
        "    # Pairwise squared distances (self-excluded by inf on diag)\n",
        "    x2 = cp.sum(X * X, axis=1, keepdims=True)\n",
        "    dist2 = x2 + x2.T - 2.0 * (X @ X.T)\n",
        "    dist2 = cp.maximum(dist2, 0)\n",
        "    cp.fill_diagonal(dist2, cp.inf)\n",
        "\n",
        "    # kNN\n",
        "    idx = cp.argpartition(dist2, kth=k - 1, axis=1)[:, :k]\n",
        "    d2_knn = cp.take_along_axis(dist2, idx, axis=1)\n",
        "\n",
        "    # Adaptive bandwidth per point\n",
        "    h2 = cp.maximum(cp.max(d2_knn, axis=1), cp.finfo(dtype).tiny)\n",
        "\n",
        "    # RBF weights with per-point bandwidth\n",
        "    w = cp.exp(-d2_knn / (2.0 * h2[:, None]))\n",
        "\n",
        "    # Neighbor points\n",
        "    X_nb = X[idx]                            # (N, k, D)\n",
        "\n",
        "    # Weighted mean\n",
        "    denom = cp.sum(w, axis=1, keepdims=True) + cp.finfo(dtype).eps\n",
        "    mu = cp.sum(w[:, :, None] * X_nb, axis=1) / denom   # (N, D)\n",
        "\n",
        "    # Weighted diagonal covariance\n",
        "    diff = X_nb - mu[:, None, :]                        # (N, k, D)\n",
        "    var = cp.sum(w[:, :, None] * diff * diff, axis=1) / denom  # (N, D)\n",
        "\n",
        "    # Ridge\n",
        "    ridge_frac = float(params.get(\"ridge_frac\", 1e-3))\n",
        "    tau = ridge_frac * cp.mean(var, axis=1, keepdims=True)      # (N, 1)\n",
        "    var_r = var + tau                                           # (N, D)\n",
        "\n",
        "    # Local score at anchors (diagonal precision)\n",
        "    s_local = (mu - X) / var_r                                  # (N, D)\n",
        "\n",
        "    # -------- Optional recomputation: KDE/GMM score at each x_i -------------\n",
        "    if not params.get(\"recompute\", False):\n",
        "        return s_local\n",
        "\n",
        "    # We treat {mu_j, var_r_j} as a uniform GMM; compute s(x_i) for all i.\n",
        "    # log N(x | mu_j, diag(var_j)) = -0.5 * [sum_d log(2π var_jd) + sum_d (x_d - mu_jd)^2 / var_jd]\n",
        "    # Use two passes with log-sum-exp for stability and batch over components j.\n",
        "\n",
        "    B = int(params.get(\"gmm_batch\", 1024))\n",
        "    two_pi = dtype(2.0 * np.pi)\n",
        "\n",
        "    # Precompute per-component constants: logdet terms\n",
        "    # (We’ll slice them inside the loop)\n",
        "    logdet = cp.sum(cp.log(two_pi * var_r), axis=1)   # (N,)\n",
        "\n",
        "    # Pass 1: find per-i maxima of log weights over all j\n",
        "    m = cp.full((N,), -cp.inf, dtype=dtype)\n",
        "    for j0 in range(0, N, B):\n",
        "        jb = slice(j0, min(j0 + B, N))\n",
        "        mu_j = mu[jb]          # (B, D)\n",
        "        var_j = var_r[jb]      # (B, D)\n",
        "        logdet_j = logdet[jb]  # (B,)\n",
        "\n",
        "        # (N, B, D)\n",
        "        dX = X[:, None, :] - mu_j[None, :, :]\n",
        "        quad = cp.sum(dX * dX / var_j[None, :, :], axis=2)      # (N, B)\n",
        "        logw = -0.5 * (quad + logdet_j[None, :])                # (N, B)\n",
        "\n",
        "        m = cp.maximum(m, cp.max(logw, axis=1))                 # (N,)\n",
        "\n",
        "    # Pass 2: accumulate numerator and denominator in linear space\n",
        "    den = cp.zeros((N,), dtype=dtype)\n",
        "    num = cp.zeros((N, D), dtype=dtype)\n",
        "    eps = cp.finfo(dtype).eps\n",
        "\n",
        "    for j0 in range(0, N, B):\n",
        "        jb = slice(j0, min(j0 + B, N))\n",
        "        mu_j = mu[jb]             # (B, D)\n",
        "        var_j = var_r[jb]         # (B, D)\n",
        "        logdet_j = logdet[jb]     # (B,)\n",
        "\n",
        "        dX = X[:, None, :] - mu_j[None, :, :]                    # (N, B, D)\n",
        "        quad = cp.sum(dX * dX / var_j[None, :, :], axis=2)       # (N, B)\n",
        "        logw = -0.5 * (quad + logdet_j[None, :])                 # (N, B)\n",
        "        w_nb = cp.exp(logw - m[:, None])                         # (N, B)\n",
        "\n",
        "        # Component scores at x_i: Σ_j^{-1}(μ_j - x_i) = -(x_i - μ_j)/var_j\n",
        "        comp_score = -dX / var_j[None, :, :]                     # (N, B, D)\n",
        "\n",
        "        # Weighted sums over components in this block\n",
        "        num += cp.einsum('nb,nbd->nd', w_nb, comp_score)         # (N, D)\n",
        "        den += cp.sum(w_nb, axis=1)                              # (N,)\n",
        "\n",
        "    s_gmm = num / (den[:, None] + eps)                           # (N, D)\n",
        "\n",
        "    return s_gmm\n",
        "\n",
        "\n",
        "def KSE_diff(X_ref, target_score_func):\n",
        "   proxy_score = fit_kernel_score(X_ref.get())\n",
        "   real_score = target_score_func(X_ref)\n",
        "   diff = cp.square(proxy_score - real_score)\n",
        "   return cp.asnumpy(cp.sqrt(cp.mean(diff)).get())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def fit_kernel_score_SVD(x_ref, params=None):\n",
        "    \"\"\"\n",
        "    Option A: Local low-rank-plus-diag precision (rank-r PCA/Woodbury).\n",
        "\n",
        "    params:\n",
        "      - k              : int, kNN size (default ~4*D, clipped to N-1)\n",
        "      - rank           : int, PCA rank r (default: min(8, D))\n",
        "      - dtype          : cp.float32 or cp.float64 (default: float32)\n",
        "      - lam_clip_mult  : float, cap λ_i ≤ lam_clip_mult * τ (default: 1e3)\n",
        "      - # --- new / clarified ridge controls ---\n",
        "      - ridge_mode     : {\"tail_mean\",\"tail_trimmed\",\"tail_median\"} (default: \"tail_mean\")\n",
        "      - tail_trim_q    : float in [0,0.5], trim frac for \"tail_trimmed\" (default: 0.1)\n",
        "      - ridge_scale    : float, scales τ_i from tail (default: 1.0)\n",
        "      - ridge_floor    : float, min τ_i (default: 1e-6)\n",
        "      - # --- recompute controls ---\n",
        "      - recompute      : bool, if True return KDE/GMM score using low-rank Σ_j (default: False)\n",
        "      - gmm_batch      : int, component batch size for recompute (default: 128)\n",
        "    \"\"\"\n",
        "    if params is None:\n",
        "        params = {}\n",
        "\n",
        "    # -------- defaults (add/ensure these lines) --------\n",
        "    params.setdefault(\"recompute\", False)\n",
        "    params.setdefault(\"gmm_batch\", 128)\n",
        "    params.setdefault(\"ridge_mode\", \"tail_mean\")\n",
        "    params.setdefault(\"tail_trim_q\", 0.1)\n",
        "    params.setdefault(\"ridge_scale\", 1.0)\n",
        "    params.setdefault(\"ridge_floor\", 1e-6)\n",
        "    params.setdefault(\"lam_clip_mult\", 1e3)\n",
        "    params.setdefault(\"rank\", min(8, x_ref.shape[1]))\n",
        "    # ---------------------------------------------------\n",
        "\n",
        "    X = cp.asarray(x_ref)\n",
        "    N, D = X.shape\n",
        "    dtype = params.get(\"dtype\", cp.float32)\n",
        "    X = X.astype(dtype, copy=False)\n",
        "\n",
        "    # ---------------- kNN and weights (same as before) ----------------\n",
        "    N0, k0, alpha = 2000, max(4*D, 48), 0.4\n",
        "    k_default = int(min(N-1, max(4*D, round(k0 * (X.shape[0]/N0) ** alpha))))\n",
        "    k = int(params.get(\"k\", k_default))\n",
        "    k = max(1, min(k, N - 1))\n",
        "\n",
        "    # Pairwise squared distances (self-excluded)\n",
        "    x2 = cp.sum(X * X, axis=1, keepdims=True)\n",
        "    dist2 = x2 + x2.T - 2.0 * (X @ X.T)\n",
        "    dist2 = cp.maximum(dist2, 0)\n",
        "    cp.fill_diagonal(dist2, cp.inf)\n",
        "\n",
        "    # kNN indices + distances\n",
        "    idx = cp.argpartition(dist2, kth=k - 1, axis=1)[:, :k]\n",
        "    d2_knn = cp.take_along_axis(dist2, idx, axis=1)\n",
        "\n",
        "    # Adaptive bandwidth and RBF weights\n",
        "    h2 = cp.maximum(cp.max(d2_knn, axis=1), cp.finfo(dtype).tiny)\n",
        "    w = cp.exp(-d2_knn / (2.0 * h2[:, None]))                   # (N, k)\n",
        "    X_nb = X[idx]                                                # (N, k, D)\n",
        "\n",
        "    # Weighted mean μ_i\n",
        "    denom = cp.sum(w, axis=1, keepdims=True) + cp.finfo(dtype).eps\n",
        "    mu = cp.sum(w[:, :, None] * X_nb, axis=1) / denom           # (N, D)\n",
        "\n",
        "    # Diagonal variances (only to set τ robustly)\n",
        "    diff = X_nb - mu[:, None, :]                                 # (N, k, D)\n",
        "    var = cp.sum(w[:, :, None] * diff * diff, axis=1) / denom    # (N, D)\n",
        "\n",
        "    # Ridge floor τ_i (scalar per point)\n",
        "    ridge_frac = float(params.get(\"ridge_frac\", 1e-3))\n",
        "    tau = ridge_frac * cp.mean(var, axis=1, keepdims=True)       # (N, 1)\n",
        "    eps = cp.finfo(dtype).eps\n",
        "\n",
        "    # ---------------- Local score at anchors via rank-r PCA ----------------\n",
        "    r = int(params.get(\"rank\", min(8, D)))\n",
        "    lam_clip_mult = float(params.get(\"lam_clip_mult\", 1e3))\n",
        "\n",
        "    ridge_mode   = params.get(\"ridge_mode\", \"tail_mean\")  # \"tail_mean\" | \"tail_trimmed\" | \"tail_median\"\n",
        "    tail_trim_q  = float(params.get(\"tail_trim_q\", 0.1))  # top fraction of tail to drop (0.0–0.5)\n",
        "    ridge_scale  = float(params.get(\"ridge_scale\", 1.0))  # multiply the tail estimate\n",
        "    ridge_floor  = float(params.get(\"ridge_floor\", 1e-6)) # absolute min tau\n",
        "    eps = cp.finfo(dtype).eps\n",
        "\n",
        "    s_local = cp.empty_like(X)\n",
        "    def _estimate_tau_from_tail(S, r_eff):\n",
        "        \"\"\"Estimate tau_i from singular values S of R_i (so λ = S^2).\"\"\"\n",
        "        if S.shape[0] <= r_eff:\n",
        "            return ridge_floor\n",
        "        lam_tail = (S[r_eff:] * S[r_eff:])  # λ_j for j>r\n",
        "        if ridge_mode == \"tail_trimmed\":\n",
        "            # drop the largest tail_trim_q fraction of the tail (robust to a few structured dirs)\n",
        "            m = lam_tail.shape[0]\n",
        "            if m > 2:\n",
        "                kdrop = int(max(1, min(m-1, round(tail_trim_q * m))))\n",
        "                lam_tail_sorted = cp.sort(lam_tail)\n",
        "                lam_tail = lam_tail_sorted[:m - kdrop]\n",
        "            tau_i = cp.mean(lam_tail)\n",
        "        elif ridge_mode == \"tail_median\":\n",
        "            tau_i = cp.median(lam_tail)\n",
        "        else:  # \"tail_mean\"\n",
        "            tau_i = cp.mean(lam_tail)\n",
        "        tau_i = float(ridge_scale) * float(tau_i)\n",
        "        tau_i = float(max(tau_i, ridge_floor))\n",
        "        return tau_i\n",
        "\n",
        "    def _apply_precision_single(v, R, r_eff, Vh, S, tau_i):\n",
        "        \"\"\"Compute (tau I + U Λ U^T)^(-1) v using top r_eff of Vh,S and scalar tau_i.\"\"\"\n",
        "        if r_eff == 0 or Vh is None or S is None:\n",
        "            return v / (tau_i + eps)\n",
        "        V_r = Vh[:r_eff, :].T\n",
        "        lam = S[:r_eff] * S[:r_eff]\n",
        "        lam = cp.minimum(lam, lam_clip_mult * tau_i)\n",
        "        coef = V_r.T @ v\n",
        "        scale = lam / (tau_i * (tau_i + lam) + eps)\n",
        "        corr = V_r @ (scale * coef)\n",
        "        return (v / (tau_i + eps)) - corr\n",
        "\n",
        "    # -------- Local score with per-point tau_i from SVD tail --------\n",
        "    taus = cp.empty((N,), dtype=dtype)   # keep if you want to inspect τ_i later\n",
        "    for i in range(N):\n",
        "        v_i = (mu[i] - X[i])                              # (D,)\n",
        "        sw = cp.sqrt(w[i] / (denom[i, 0] + eps))          # (k,)\n",
        "        R_i = sw[:, None] * (X_nb[i] - mu[i])             # (k, D)\n",
        "        try:\n",
        "            _, S, Vh = cp.linalg.svd(R_i, full_matrices=False)\n",
        "            r_eff = int(min(r, Vh.shape[0]))\n",
        "        except cp.linalg.LinAlgError:\n",
        "            S, Vh, r_eff = None, None, 0\n",
        "\n",
        "        # --- NEW: τ_i from the discarded spectrum ---\n",
        "        tau_i = _estimate_tau_from_tail(S, r_eff) if S is not None else ridge_floor\n",
        "        taus[i] = tau_i\n",
        "\n",
        "        s_local[i] = _apply_precision_single(v_i, R_i, r_eff, Vh, S, tau_i)\n",
        "\n",
        "    if not params.get(\"recompute\", False):\n",
        "        return s_local\n",
        "\n",
        "    # ---------------- Recompute path: reuse per-component τ_j and (V, λ) --------------\n",
        "    B = int(params.get(\"gmm_batch\", 128))\n",
        "    two_pi = dtype(2.0 * np.pi)\n",
        "    m = cp.full((N,), -cp.inf, dtype=dtype)\n",
        "    den = cp.zeros((N,), dtype=dtype)\n",
        "    num = cp.zeros((N, D), dtype=dtype)\n",
        "\n",
        "    for j0 in range(0, N, B):\n",
        "        jb = slice(j0, min(j0 + B, N))\n",
        "        Bcur = jb.stop - jb.start\n",
        "\n",
        "        mu_j  = mu[jb]                      # (B, D)\n",
        "        tau_j = taus[jb]                    # (B,)\n",
        "\n",
        "        V_batch = cp.zeros((Bcur, D, r), dtype=dtype)\n",
        "        lam_batch = cp.zeros((Bcur, r), dtype=dtype)\n",
        "        r_effs = np.empty(Bcur, dtype=np.int32)\n",
        "        logdet_const = cp.empty((Bcur,), dtype=dtype)\n",
        "\n",
        "        for b in range(Bcur):\n",
        "            j = j0 + b\n",
        "            sw = cp.sqrt(w[j] / (denom[j, 0] + eps))\n",
        "            R_j = sw[:, None] * (X_nb[j] - mu[j])\n",
        "            try:\n",
        "                _, S, Vh = cp.linalg.svd(R_j, full_matrices=False)\n",
        "                r_eff = int(min(r, Vh.shape[0]))\n",
        "            except cp.linalg.LinAlgError:\n",
        "                S, Vh, r_eff = None, None, 0\n",
        "\n",
        "            # IMPORTANT: recompute τ_j from this component’s tail as well\n",
        "            tau_comp = _estimate_tau_from_tail(S, r_eff) if S is not None else float(tau_j[b])\n",
        "            tau_j[b] = tau_comp\n",
        "\n",
        "            r_effs[b] = r_eff\n",
        "            if r_eff > 0:\n",
        "                V_r = Vh[:r_eff, :].T\n",
        "                lam = S[:r_eff] * S[:r_eff]\n",
        "                lam = cp.minimum(lam, lam_clip_mult * tau_comp)\n",
        "                V_batch[b, :, :r_eff] = V_r\n",
        "                lam_batch[b, :r_eff] = lam\n",
        "                logdet_const[b] = D * cp.log(two_pi) + (D - r_eff) * cp.log(tau_comp + eps) \\\n",
        "                                  + cp.sum(cp.log(tau_comp + lam + eps))\n",
        "            else:\n",
        "                logdet_const[b] = D * cp.log(two_pi) + D * cp.log(tau_comp + eps)\n",
        "\n",
        "        dX = X[:, None, :] - mu_j[None, :, :]\n",
        "        T = cp.einsum('nbd,bdr->nbr', dX, V_batch)\n",
        "        scale = lam_batch / (tau_j[:, None] * (tau_j[:, None] + lam_batch) + eps)\n",
        "        term = cp.einsum('nbr,bdr->nbd', T * scale[None, :, :], V_batch)\n",
        "        u = dX / (tau_j[None, :, None] + eps) - term\n",
        "\n",
        "        quad = cp.sum(dX * u, axis=2)\n",
        "        logw = -0.5 * (quad + logdet_const[None, :])\n",
        "        comp_score = -u\n",
        "\n",
        "        new_m = cp.maximum(m, cp.max(logw, axis=1))\n",
        "        scale_old = cp.exp(m - new_m)\n",
        "        W = cp.exp(logw - new_m[:, None])\n",
        "\n",
        "        num = num * scale_old[:, None] + cp.einsum('nb,nbd->nd', W, comp_score)\n",
        "        den = den * scale_old + cp.sum(W, axis=1)\n",
        "        m = new_m\n",
        "\n",
        "    s_gmm = num / (den[:, None] + eps)\n",
        "    return s_gmm\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "\n",
        "\n",
        "def fit_kernel_score_SVD_diag(x_ref, params=None):\n",
        "    \"\"\"\n",
        "    Option A: Local low-rank-plus-diagonal precision (rank-r PCA/Woodbury).\n",
        "\n",
        "    params:\n",
        "      - k              : int, kNN size (default ~4*D, clipped to N-1)\n",
        "      - rank           : int, PCA rank r (default: min(8, D))\n",
        "      - dtype          : cp.float32 or cp.float64 (default: float32)\n",
        "      - lam_clip_mult  : float, cap λ_i ≤ lam_clip_mult * max(tau_vec) (default: 1e3)\n",
        "      - # --- ridge / tail controls for diagonal tail τ_vec ---\n",
        "      - ridge_mode     : {\"tail_mean\",\"tail_trimmed\",\"tail_median\"} (default: \"tail_mean\")\n",
        "      - tail_trim_q    : float in [0,0.5], trim frac for \"tail_trimmed\" (default: 0.1)\n",
        "      - ridge_scale    : float, scales τ tail (default: 1.0)\n",
        "      - ridge_floor    : float, per-dim min τ_d (default: 1e-6)\n",
        "      - ridge_frac     : float, fallback scalar floor×mean(var) when SVD fails (default: 1e-3)\n",
        "      - # --- recompute controls ---\n",
        "      - recompute      : bool, if True return KDE/GMM score using LR+D Σ_j (default: False)\n",
        "      - gmm_batch      : int, component batch size for recompute (default: 128)\n",
        "    \"\"\"\n",
        "    if params is None:\n",
        "        params = {}\n",
        "\n",
        "    # -------- defaults --------\n",
        "    params.setdefault(\"recompute\", False)\n",
        "    params.setdefault(\"gmm_batch\", 128)\n",
        "    params.setdefault(\"ridge_mode\", \"tail_mean\")\n",
        "    params.setdefault(\"tail_trim_q\", 0.1)\n",
        "    params.setdefault(\"ridge_scale\", 1.0)\n",
        "    params.setdefault(\"ridge_floor\", 1e-6)\n",
        "    params.setdefault(\"lam_clip_mult\", 1e3)\n",
        "    params.setdefault(\"rank\", None)\n",
        "    params.setdefault(\"ridge_frac\", 1e-3)\n",
        "    # --------------------------\n",
        "\n",
        "    X = cp.asarray(x_ref)\n",
        "    N, D = X.shape\n",
        "    dtype = params.get(\"dtype\", cp.float32)\n",
        "    X = X.astype(dtype, copy=False)\n",
        "\n",
        "    # ---------------- kNN and weights ----------------\n",
        "    N0, k0, alpha = 2000, max(4*D, 48), 0.4\n",
        "    k_default = int(min(N-1, max(4*D, round(k0 * (X.shape[0]/N0) ** alpha))))\n",
        "    k = int(params.get(\"k\", k_default))\n",
        "    k = max(1, min(k, N - 1))\n",
        "\n",
        "    # Pairwise squared distances (self-excluded)\n",
        "    x2 = cp.sum(X * X, axis=1, keepdims=True)\n",
        "    dist2 = x2 + x2.T - 2.0 * (X @ X.T)\n",
        "    dist2 = cp.maximum(dist2, 0)\n",
        "    cp.fill_diagonal(dist2, cp.inf)\n",
        "\n",
        "    # kNN indices + distances\n",
        "    idx = cp.argpartition(dist2, kth=k - 1, axis=1)[:, :k]\n",
        "    d2_knn = cp.take_along_axis(dist2, idx, axis=1)\n",
        "\n",
        "    # Adaptive bandwidth and RBF weights\n",
        "    h2 = cp.maximum(cp.max(d2_knn, axis=1), cp.finfo(dtype).tiny)\n",
        "    w = cp.exp(-d2_knn / (2.0 * h2[:, None]))                   # (N, k)\n",
        "    X_nb = X[idx]                                                # (N, k, D)\n",
        "\n",
        "    # Weighted mean μ_i\n",
        "    denom = cp.sum(w, axis=1, keepdims=True) + cp.finfo(dtype).eps\n",
        "    mu = cp.sum(w[:, :, None] * X_nb, axis=1) / denom           # (N, D)\n",
        "\n",
        "    # Per-dim weighted variances (used for fallback scalar floor only)\n",
        "    diff = X_nb - mu[:, None, :]                                 # (N, k, D)\n",
        "    var = cp.sum(w[:, :, None] * diff * diff, axis=1) / denom    # (N, D)\n",
        "\n",
        "    ridge_frac = float(params.get(\"ridge_frac\", 1e-3))\n",
        "    eps = cp.finfo(dtype).eps\n",
        "\n",
        "    # ---------------- Local score at anchors via rank-r PCA ----------------\n",
        "    r_user = params.get(\"rank\", None)\n",
        "    r_default = min(8, D)\n",
        "    lam_clip_mult = float(params.get(\"lam_clip_mult\", 1e3))\n",
        "\n",
        "    ridge_mode   = params.get(\"ridge_mode\", \"tail_mean\")  # \"tail_mean\" | \"tail_trimmed\" | \"tail_median\"\n",
        "    tail_trim_q  = float(params.get(\"tail_trim_q\", 0.1))  # drop largest tail fraction (robust)\n",
        "    ridge_scale  = float(params.get(\"ridge_scale\", 1.0))\n",
        "    ridge_floor  = float(params.get(\"ridge_floor\", 1e-6))\n",
        "\n",
        "    s_local = cp.empty_like(X)\n",
        "\n",
        "    def _tau_vec_from_tail(S, Vh, r_eff):\n",
        "        \"\"\"\n",
        "        Build diagonal tail τ_vec from discarded singular spectrum:\n",
        "          τ_d ≈ sum_{j>r} λ_j * (V_tail[d,j-r])^2\n",
        "        with ridge_mode control.\n",
        "        \"\"\"\n",
        "        if Vh is None or S is None or Vh.shape[0] <= r_eff:\n",
        "            return cp.full((D,), ridge_floor, dtype=dtype)\n",
        "\n",
        "        # Tail singular values/eigenvalues\n",
        "        lam_tail = (S[r_eff:] * S[r_eff:])   # (m_tail,)\n",
        "        V_tail   = Vh[r_eff:, :].T           # (D, m_tail)\n",
        "\n",
        "        if ridge_mode == \"tail_trimmed\":\n",
        "            m = lam_tail.shape[0]\n",
        "            if m > 2:\n",
        "                kdrop = int(max(1, min(m-1, round(tail_trim_q * m))))\n",
        "                lam_tail_sorted = cp.sort(lam_tail)\n",
        "                lam_tail = lam_tail_sorted[:m - kdrop]\n",
        "                V_tail   = V_tail[:, :lam_tail.shape[0]]\n",
        "            # per-dim energy\n",
        "            tau_vec = (V_tail * V_tail) @ lam_tail\n",
        "        elif ridge_mode == \"tail_median\":\n",
        "            # scale median(λ_tail) by per-dim col-sum of V_tail^2 to preserve shape\n",
        "            lam_med = cp.median(lam_tail)\n",
        "            tau_vec = lam_med * cp.sum(V_tail * V_tail, axis=1)\n",
        "        else:  # \"tail_mean\"\n",
        "            lam_mean = cp.mean(lam_tail)\n",
        "            tau_vec = lam_mean * cp.sum(V_tail * V_tail, axis=1)\n",
        "\n",
        "        tau_vec = ridge_scale * cp.maximum(tau_vec.astype(dtype), dtype(ridge_floor))\n",
        "        return tau_vec\n",
        "\n",
        "    def _apply_precision_diagLR(v, Vh, S, r_eff, tau_vec):\n",
        "        \"\"\"\n",
        "        Compute u = (D + V Λ V^T)^(-1) v\n",
        "          D = diag(tau_vec), Λ = diag(lam), V = V_r\n",
        "        using Woodbury with diagonal base.\n",
        "        \"\"\"\n",
        "        if r_eff == 0 or Vh is None or S is None:\n",
        "            return v / (tau_vec + eps)\n",
        "\n",
        "        V_r = Vh[:r_eff, :].T                    # (D, r)\n",
        "        lam = (S[:r_eff] * S[:r_eff])            # (r,)\n",
        "        lam = cp.minimum(lam, lam_clip_mult * cp.max(tau_vec))\n",
        "\n",
        "        invD = 1.0 / (tau_vec + eps)             # (D,)\n",
        "        invDv = invD * v                         # (D,)\n",
        "        # S_r = V^T D^{-1} V   -> (r,r)\n",
        "        S_r = V_r.T @ (invD[:, None] * V_r)\n",
        "        # Solve (Λ^{-1} + S_r) z = V^T D^{-1} v\n",
        "        A = S_r + cp.diag(1.0 / (lam + eps))     # (r,r)\n",
        "        rhs = V_r.T @ invDv                      # (r,)\n",
        "        z = cp.linalg.solve(A, rhs)              # (r,)\n",
        "        u = invDv - (invD[:, None] * V_r) @ z    # (D,)\n",
        "        return u\n",
        "\n",
        "    r_list = cp.empty((N,), dtype=cp.int32)\n",
        "    tau_store = cp.empty((N, D), dtype=dtype)   # keep τ_vec per anchor (re-used in recompute)\n",
        "\n",
        "    for i in range(N):\n",
        "        wi = w[i] / (denom[i, 0] + eps)               # (k,)\n",
        "        sw = cp.sqrt(wi)                               # (k,)\n",
        "        R_i = sw[:, None] * (X_nb[i] - mu[i])          # (k, D)\n",
        "        try:\n",
        "            _, S, Vh = cp.linalg.svd(R_i, full_matrices=False)\n",
        "            r_eff = int(min(r_user if r_user is not None else r_default, Vh.shape[0]))\n",
        "        except cp.linalg.LinAlgError:\n",
        "            S, Vh, r_eff = None, None, 0\n",
        "\n",
        "        if (S is None) or (r_eff == 0):\n",
        "            # pure diagonal fallback from empirical var + scalar ridge\n",
        "            tau_s = ridge_frac * var[i].mean()\n",
        "            tau_vec = cp.maximum(var[i] + tau_s, dtype(ridge_floor))   # still diagonal-ish\n",
        "            v_i = (mu[i] - X[i])\n",
        "            s_local[i] = v_i / (tau_vec + eps)\n",
        "            tau_store[i] = tau_vec\n",
        "            r_list[i] = 0\n",
        "            continue\n",
        "\n",
        "        tau_vec = _tau_vec_from_tail(S, Vh, r_eff)                     # (D,)\n",
        "        v_i = (mu[i] - X[i])\n",
        "        u_i = _apply_precision_diagLR(v_i, Vh, S, r_eff, tau_vec)\n",
        "        s_local[i] = u_i\n",
        "        tau_store[i] = tau_vec\n",
        "        r_list[i] = r_eff\n",
        "\n",
        "    if not params.get(\"recompute\", False):\n",
        "        return s_local\n",
        "\n",
        "    # ---------------- Recompute path: LR + diagonal per component --------------\n",
        "    B = int(params.get(\"gmm_batch\", 128))\n",
        "    two_pi = dtype(2.0 * np.pi)\n",
        "    m = cp.full((N,), -cp.inf, dtype=dtype)\n",
        "    den = cp.zeros((N,), dtype=dtype)\n",
        "    num = cp.zeros((N, D), dtype=dtype)\n",
        "\n",
        "    for j0 in range(0, N, B):\n",
        "        jb = slice(j0, min(j0 + B, N))\n",
        "        Bcur = jb.stop - jb.start\n",
        "\n",
        "        mu_j   = mu[jb]                # (B, D)\n",
        "        tau_j  = tau_store[jb]         # (B, D)\n",
        "        r_effj = r_list[jb].get()      # small ints on host\n",
        "\n",
        "        # pack V and lam per component (pad to max r in batch for convenience)\n",
        "        r_max = int(r_effj.max() if Bcur > 0 else 0)\n",
        "        V_batch   = cp.zeros((Bcur, D, r_max), dtype=dtype)\n",
        "        lam_batch = cp.zeros((Bcur, r_max), dtype=dtype)\n",
        "        logdet_const = cp.empty((Bcur,), dtype=dtype)\n",
        "\n",
        "        for b in range(Bcur):\n",
        "            j = j0 + b\n",
        "            wi = w[j] / (denom[j, 0] + eps)\n",
        "            sw = cp.sqrt(wi)\n",
        "            R_j = sw[:, None] * (X_nb[j] - mu[j])\n",
        "            try:\n",
        "                _, S, Vh = cp.linalg.svd(R_j, full_matrices=False)\n",
        "                r_eff = int(min(int(r_effj[b]), Vh.shape[0])) if r_effj[b] > 0 else 0\n",
        "            except cp.linalg.LinAlgError:\n",
        "                S, Vh, r_eff = None, None, 0\n",
        "\n",
        "            tau_vec = tau_j[b]                     # (D,)\n",
        "            invD = 1.0 / (tau_vec + eps)\n",
        "\n",
        "            if r_eff > 0 and (S is not None) and (Vh is not None):\n",
        "                V_r = Vh[:r_eff, :].T              # (D, r_eff)\n",
        "                lam = (S[:r_eff] * S[:r_eff])      # (r_eff,)\n",
        "                lam = cp.minimum(lam, lam_clip_mult * cp.max(tau_vec))\n",
        "\n",
        "                # S_r = V^T D^{-1} V\n",
        "                S_r = V_r.T @ (invD[:, None] * V_r)            # (r_eff, r_eff)\n",
        "                # M = I + diag(lam) S_r\n",
        "                M = cp.eye(r_eff, dtype=dtype) + (lam[None, :] * S_r.T).T  # diag(lam) @ S_r\n",
        "                sign, logdetM = cp.linalg.slogdet(M)\n",
        "                if sign <= 0:\n",
        "                    # numerical fallback (rare; should not happen)\n",
        "                    logdetM = cp.log(cp.maximum(cp.linalg.det(M), eps))\n",
        "                logdet_const[b] = D * cp.log(two_pi) + cp.sum(cp.log(tau_vec + eps)) + logdetM\n",
        "\n",
        "                V_batch[b, :, :r_eff] = V_r\n",
        "                lam_batch[b, :r_eff]  = lam\n",
        "            else:\n",
        "                # pure diagonal component\n",
        "                logdet_const[b] = D * cp.log(two_pi) + cp.sum(cp.log(tau_vec + eps))\n",
        "\n",
        "        # Now compute u = Σ_j^{-1} (x - μ_j) for all queries x and all batch comps j\n",
        "        dX = X[:, None, :] - mu_j[None, :, :]          # (N, B, D)\n",
        "        invD = 1.0 / (tau_j[None, :, :] + eps)         # (N, B, D) broadcast ok since tau_j is (B, D)\n",
        "        invDv = invD * dX                               # (N, B, D)\n",
        "\n",
        "        # Low-rank correction term where r_eff>0\n",
        "        # Precompute masks\n",
        "        r_mask = (lam_batch.sum(axis=1) > 0)            # (B,)\n",
        "        if r_max > 0 and cp.any(r_mask):\n",
        "            # Gather only active comps to reduce work (optional)\n",
        "            # We'll compute for all and rely on lam=0 where inactive.\n",
        "            T = cp.einsum('nbd,bdr->nbr', invDv, V_batch)        # (N, B, r_max) equals V^T D^{-1} (x-μ)\n",
        "            # For each b, solve (Λ^{-1} + V^T D^{-1} V) z = V^T D^{-1} (x-μ)\n",
        "            # Build A_b = Λ^{-1} + S_r for each component (pad to r_max)\n",
        "            A = cp.zeros((Bcur, r_max, r_max), dtype=dtype)\n",
        "            for b in range(Bcur):\n",
        "                r_eff = int(r_effj[b])\n",
        "                if r_eff == 0:\n",
        "                    continue\n",
        "                V_r = V_batch[b, :, :r_eff]                               # (D,r)\n",
        "                invD_b = 1.0 / (tau_j[b] + eps)                           # (D,)\n",
        "                S_r = V_r.T @ (invD_b[:, None] * V_r)                     # (r,r)\n",
        "                lam = lam_batch[b, :r_eff]\n",
        "                A[b, :r_eff, :r_eff] = S_r + cp.diag(1.0 / (lam + eps))   # (r,r)\n",
        "\n",
        "            # Solve A z = T^T per component b for all N rhs (batched)\n",
        "            # We'll loop over b (r kept small), solve for each b across N.\n",
        "            Z = cp.zeros_like(T)   # (N,B,r_max)\n",
        "            for b in range(Bcur):\n",
        "                r_eff = int(r_effj[b])\n",
        "                if r_eff == 0:\n",
        "                    continue\n",
        "                Ab = A[b, :r_eff, :r_eff]                                # (r,r)\n",
        "                rhs = T[:, b, :r_eff].T                                  # (r,N)\n",
        "                zb = cp.linalg.solve(Ab, rhs)                             # (r,N)\n",
        "                Z[:, b, :r_eff] = zb.T\n",
        "\n",
        "            # u = invDv - D^{-1} V z\n",
        "            corr = cp.einsum('nbr,bdr->nbd', Z, V_batch)                  # (N,B,D)\n",
        "            u = invDv - (invD) * corr                                     # (N,B,D)\n",
        "        else:\n",
        "            u = invDv                                                     # (N,B,D)\n",
        "\n",
        "        quad = cp.sum(dX * u, axis=2)                      # (N,B)\n",
        "        logw = -0.5 * (quad + logdet_const[None, :])       # (N,B)\n",
        "        comp_score = -u                                     # (N,B,D)\n",
        "\n",
        "        new_m = cp.maximum(m, cp.max(logw, axis=1))\n",
        "        scale_old = cp.exp(m - new_m)\n",
        "        W = cp.exp(logw - new_m[:, None])\n",
        "\n",
        "        num = num * scale_old[:, None] + cp.einsum('nb,nbd->nd', W, comp_score)\n",
        "        den = den * scale_old + cp.sum(W, axis=1)\n",
        "        m = new_m\n",
        "\n",
        "    s_gmm = num / (den[:, None] + eps)\n",
        "    return s_gmm\n",
        "\n",
        "\n",
        "def _to_numpy_cpu(x):\n",
        "    \"\"\"\n",
        "    Robustly convert x -> NumPy (CPU) float64, handling:\n",
        "      - CuPy arrays (GPU) -> .get()\n",
        "      - PyTorch tensors (GPU/CPU) -> .detach().cpu().numpy()\n",
        "      - NumPy arrays -> asarray\n",
        "      - Python lists/tuples -> np.asarray\n",
        "    \"\"\"\n",
        "    # Avoid hard dependencies\n",
        "    try:\n",
        "        import cupy as cp\n",
        "        if isinstance(x, cp.ndarray):\n",
        "            x = cp.asnumpy(x)  # GPU->CPU\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        import torch\n",
        "        if torch.is_tensor(x):\n",
        "            x = x.detach().cpu().numpy()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    x = np.asarray(x)  # handles numpy/list/tuple\n",
        "    if x.dtype.kind in ('i','u','b'):\n",
        "        x = x.astype(np.float64, copy=False)\n",
        "    elif x.dtype != np.float64:\n",
        "        # keep float precision stable for distances\n",
        "        x = x.astype(np.float64, copy=False)\n",
        "    return x\n",
        "\n",
        "def _sanitize_points(x, name=\"array\", require_min=3):\n",
        "    \"\"\"\n",
        "    Ensure 2D [N, D], drop non-finite rows, enforce min count.\n",
        "    \"\"\"\n",
        "    x = _to_numpy_cpu(x)\n",
        "    if x.ndim == 1:\n",
        "        x = x.reshape(-1, 1)\n",
        "    elif x.ndim > 2:\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "\n",
        "    # drop rows with NaN/Inf\n",
        "    good = np.isfinite(x).all(axis=1)\n",
        "    if not np.all(good):\n",
        "        x = x[good]\n",
        "    if x.shape[0] < require_min:\n",
        "        raise ValueError(f\"{name}: need at least {require_min} valid points; got {x.shape[0]}.\")\n",
        "    return np.ascontiguousarray(x, dtype=np.float64)\n",
        "\n",
        "def copy_test(output_samples, train_samples, test_samples, label):\n",
        "    \"\"\"\n",
        "    Wrap copying_index + duplicate_rate with robust conversions.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        G = _sanitize_points(output_samples, name=\"gen\")\n",
        "        TR = _sanitize_points(train_samples,  name=\"train\")\n",
        "        TE = _sanitize_points(test_samples,   name=\"test\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] {label}: conversion/sanitize failed: {e}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        copy_stats = copying_index(\n",
        "            gen=G,\n",
        "            train=TR,\n",
        "            test=TE,\n",
        "            k=1,\n",
        "            alphas=(0.9, 0.8),\n",
        "            max_pts=6000\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] {label}: copying_index failed: {e}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        dup_rate = duplicate_rate(G, eps=1e-4, max_pts=6000)\n",
        "        copy_stats[\"dup_rate_eps1e4\"] = dup_rate\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] {label}: duplicate_rate failed: {e}\")\n",
        "        dup_rate = float(\"nan\")\n",
        "\n",
        "    # Print nicely even if some keys are missing\n",
        "    rmean = copy_stats.get(\"copy_ratio_mean\", np.nan)\n",
        "    rmed  = copy_stats.get(\"copy_ratio_median\", np.nan)\n",
        "    f09   = copy_stats.get(\"copy_frac_a09\", np.nan)\n",
        "\n",
        "    print(f\"    [copy] {label}: ratio_mean={rmean:.3f}, \"\n",
        "          f\"ratio_med={rmed:.3f}, frac<0.9={f09:.3f}, \"\n",
        "          f\"dup_rate={dup_rate:.3e}\")\n",
        "\n",
        "\n",
        "\n",
        "def run_comparison(\n",
        "    samplers, steps_list,  true_sampler_func,\n",
        "    prior_sampler_func, prior_score_func=None, prior_score_div_func = None,\n",
        "    likelyhood_func = None, loglik_grad_fn = None,\n",
        "    post_sampler_func = None, post_score_func = None, post_score_div_func = None,\n",
        "    true_score_func=None,               # NEW: oracle score function (y, t) -> score\n",
        "    true_init_score = None,\n",
        "    track_score_rmse=False,             # NEW: whether to compute RMSE vs oracle\n",
        "    N_ref=1000, N_part=2000, batch=1000,\n",
        "    trials=20, nrows=3, trial_name='trial', div='M_KSD',\n",
        "    T_end=1.5, T_target=1e-3, time_split='power',\n",
        "    plot_hists=True, csv_path=None, hist_mode='first', display_mode='standard',\n",
        "    ref_seed='rand', plot_res=False, d_pairs=None, pre_process_f=None, plot_post = False,\n",
        "    mean_bins = 80, best_bins = 60, p_prune = 0, plot_prior = True, hist_norm = 1.25,\n",
        "    draw_countours = False, prior_scale = True, save_tag = '', r = 0, post_hist_norm = 1.0,\n",
        "    w_correct_func = None, comparison_samples = None, comparison_label = None,\n",
        "    rand_seed = False, ridge_frac = 3e-3, cov_rank = 3, do_copy_test = True,\n",
        "    # ----------------- NEW -----------------\n",
        "    whiten_scores: bool = False,\n",
        "):\n",
        "    def _tweedie_white_proxy_fn_factory(prior_score_func, ridge_frac):\n",
        "        # Used ONLY to build/recompute whitened scores for Tweedie.\n",
        "        # Must return an array of shape [N,d] when called on anchors.\n",
        "        if prior_score_func is not None:\n",
        "            return lambda x: prior_score_func(x)\n",
        "        return lambda x: fit_kernel_score(x, params={\"ridge_frac\": ridge_frac, \"recompute\": False})\n",
        "\n",
        "    if len(save_tag):\n",
        "       print_tag = f'{save_tag}, '\n",
        "    else:\n",
        "       print_tag = ''\n",
        "\n",
        "    results = {f'{label}': []  for (_, _, label, *_) in samplers}\n",
        "    samples_dict =  {f'{label}': []  for (_, _, label, *_) in samplers}\n",
        "    divs_dict = {f'{label}': []  for (_, _, label, *_) in samplers}\n",
        "\n",
        "    if loglik_grad_fn is None:                 # Indication we arent doing any posterior\n",
        "        post_sampler_func = true_sampler_func\n",
        "        post_score_func = true_init_score\n",
        "        likelyhood_func = None\n",
        "\n",
        "    if true_init_score == None and div == 'M_KSD':\n",
        "      div = 'W2'\n",
        "      true_init_score = prior_score_func\n",
        "      print(f'No score available for KSD, switching div to W2 ')\n",
        "\n",
        "    clip_plot = False\n",
        "    floors = []\n",
        "\n",
        "    if track_score_rmse and true_score_func is not None:\n",
        "        score_rmse = {label: [] for  (_, _, label, *_) in samplers}\n",
        "\n",
        "    for steps in steps_list:\n",
        "        delta = T_target**(1/steps)\n",
        "\n",
        "        if time_split == 'exp':\n",
        "            times = T_end * (delta**np.arange(steps+1))\n",
        "        elif time_split == 'power':\n",
        "            gamma = min(1.0, 0.15*steps)   # adaptive γ\n",
        "            times = power_grid(T_end, T_target, steps, gamma)\n",
        "        else:\n",
        "            times = np.linspace(T_end, T_target, steps+1)\n",
        "\n",
        "        h_coeff = min(lambda_two_step(steps, T_end, T_target), .5)\n",
        "\n",
        "        output_dict = {}\n",
        "\n",
        "        for k , (sampler_tuple) in enumerate(samplers):\n",
        "            if len(sampler_tuple) == 3:\n",
        "                mode, score_mode, label = sampler_tuple\n",
        "                custom_callable = None\n",
        "            else:\n",
        "                mode, score_mode, label, custom_callable = sampler_tuple\n",
        "\n",
        "            per_trial_rmse = []\n",
        "            print_res = True\n",
        "            divs = []\n",
        "            t0 = time.time()\n",
        "\n",
        "            for j in range(trials):\n",
        "                if rand_seed:\n",
        "                    ref_seed = j\n",
        "\n",
        "                test_samples = post_sampler_func(N_part)\n",
        "                train_samples = prior_sampler_func(N_ref, seed = ref_seed)\n",
        "\n",
        "                # ---------- DISPATCH ----------\n",
        "                if score_mode == 'tweedie':\n",
        "                    out = tweedie_sampler(\n",
        "                        N_part, N_ref, times, batch, prior_sampler_func, mode=mode,\n",
        "                        likelyhood_func=likelyhood_func, true_score_func=true_score_func,\n",
        "                        h_coeff=h_coeff, seed=ref_seed, w_correct_func=w_correct_func,\n",
        "                        # whitening args exactly as in tweedie_sampler signature\n",
        "                        whiten_scores=False,\n",
        "                        whiten_det_one=True,\n",
        "                        whiten_center=True,\n",
        "                        whiten_lam_min=1e-8,\n",
        "                        whiten_lam_max=None,\n",
        "                    )\n",
        "\n",
        "                elif score_mode == 'kss':\n",
        "                    # KSS with true prior score: treat as map_white_scores on anchors\n",
        "                    out = kss_sampler(\n",
        "                        N_part, N_ref, times, batch, prior_sampler_func, prior_score_func, mode=mode,\n",
        "                        likelyhood_func=likelyhood_func, loglik_grad_fn=loglik_grad_fn,\n",
        "                        true_score_func=true_score_func, h_coeff=h_coeff, seed=ref_seed,\n",
        "                        w_correct_func=w_correct_func,\n",
        "                        # whitening:\n",
        "                        whiten_scores=whiten_scores,\n",
        "                        whiten_mode=\"map_white_scores\",\n",
        "                    )\n",
        "\n",
        "                elif score_mode == 'blend_proxy':\n",
        "                    params = {\"ridge_frac\": ridge_frac, \"recompute\": False}\n",
        "                    proxy_prior_score = lambda x: fit_kernel_score(x, params=params)\n",
        "                    out = blend_sampler(\n",
        "                        N_part, N_ref, times, batch, prior_sampler_func, proxy_prior_score, mode=mode,\n",
        "                        likelyhood_func=likelyhood_func, loglik_grad_fn=loglik_grad_fn, weight_mode='snis',\n",
        "                        seed=ref_seed, h_coeff=h_coeff, true_score_func=true_score_func, w_correct_func=w_correct_func,\n",
        "                        # whitening:\n",
        "                        whiten_scores=whiten_scores,\n",
        "                        whiten_mode=\"recompute_white_scores\",\n",
        "                        proxy_score_fit=proxy_prior_score,\n",
        "                    )\n",
        "\n",
        "                elif score_mode == 'blend_proxy_SVD':\n",
        "                    params = dict(rank=cov_rank, ridge_mode=\"tail_median\", tail_trim_q=0.1,\n",
        "                                  ridge_scale=1.0, ridge_floor=1e-6, recompute=False, gmm_batch=128)\n",
        "                    proxy_prior_score = lambda x: fit_kernel_score_SVD(x, params=params)\n",
        "                    out = blend_sampler(\n",
        "                        N_part, N_ref, times, batch, prior_sampler_func, proxy_prior_score, mode=mode,\n",
        "                        likelyhood_func=likelyhood_func, loglik_grad_fn=loglik_grad_fn, weight_mode='snis',\n",
        "                        seed=ref_seed, h_coeff=h_coeff, true_score_func=true_score_func, w_correct_func=w_correct_func,\n",
        "                        # whitening:\n",
        "                        whiten_scores=whiten_scores,\n",
        "                        whiten_mode=\"recompute_white_scores\",\n",
        "                        proxy_score_fit=proxy_prior_score,\n",
        "                    )\n",
        "\n",
        "                elif score_mode == 'blend_proxy_SVD_recomp':\n",
        "                    params = dict(rank=cov_rank, ridge_mode=\"tail_median\", tail_trim_q=0.1,\n",
        "                                  ridge_scale=1.0, ridge_floor=1e-6, recompute=True, gmm_batch=128)\n",
        "                    proxy_prior_score = lambda x: fit_kernel_score_SVD(x, params=params)\n",
        "                    out = blend_sampler(\n",
        "                        N_part, N_ref, times, batch, prior_sampler_func, proxy_prior_score, mode=mode,\n",
        "                        likelyhood_func=likelyhood_func, loglik_grad_fn=loglik_grad_fn, weight_mode='snis',\n",
        "                        seed=ref_seed, h_coeff=h_coeff, true_score_func=true_score_func, w_correct_func=w_correct_func,\n",
        "                        whiten_scores=whiten_scores,\n",
        "                        whiten_mode=\"recompute_white_scores\",\n",
        "                        proxy_score_fit=proxy_prior_score,\n",
        "                    )\n",
        "\n",
        "                elif score_mode == 'blend_proxy_SVD_diag':\n",
        "                    params = dict(rank=cov_rank, ridge_mode=\"tail_median\", tail_trim_q=0.1,\n",
        "                                  ridge_scale=1.0, ridge_floor=1e-6, recompute=False, gmm_batch=128)\n",
        "                    proxy_prior_score = lambda x: fit_kernel_score_SVD(x, params=params)\n",
        "                    out = blend_sampler(\n",
        "                        N_part, N_ref, times, batch, prior_sampler_func, proxy_prior_score, mode=mode,\n",
        "                        likelyhood_func=likelyhood_func, loglik_grad_fn=loglik_grad_fn, weight_mode='snis',\n",
        "                        seed=ref_seed, h_coeff=h_coeff, true_score_func=true_score_func, w_correct_func=w_correct_func,\n",
        "                        whiten_scores=whiten_scores,\n",
        "                        whiten_mode=\"recompute_white_scores\",\n",
        "                        proxy_score_fit=proxy_prior_score,\n",
        "                    )\n",
        "\n",
        "                elif score_mode == 'blend_proxy_SVD_recomp_diag':\n",
        "                    params = dict(rank=cov_rank, ridge_mode=\"tail_median\", tail_trim_q=0.1,\n",
        "                                  ridge_scale=1.0, ridge_floor=1e-6, recompute=True, gmm_batch=128)\n",
        "                    proxy_prior_score = lambda x: fit_kernel_score_SVD(x, params=params)\n",
        "                    out = blend_sampler(\n",
        "                        N_part, N_ref, times, batch, prior_sampler_func, proxy_prior_score, mode=mode,\n",
        "                        likelyhood_func=likelyhood_func, loglik_grad_fn=loglik_grad_fn, weight_mode='snis',\n",
        "                        seed=ref_seed, h_coeff=h_coeff, true_score_func=true_score_func, w_correct_func=w_correct_func,\n",
        "                        whiten_scores=whiten_scores,\n",
        "                        whiten_mode=\"recompute_white_scores\",\n",
        "                        proxy_score_fit=proxy_prior_score,\n",
        "                    )\n",
        "\n",
        "                elif score_mode == 'blend_proxy_recomp':\n",
        "                    params = {\"ridge_frac\": ridge_frac, \"recompute\": False}\n",
        "                    proxy_prior_score = lambda x: fit_kernel_score(x, params=params)\n",
        "                    out = blend_sampler(\n",
        "                        N_part, N_ref, times, batch, prior_sampler_func, proxy_prior_score, mode=mode,\n",
        "                        likelyhood_func=likelyhood_func, loglik_grad_fn=loglik_grad_fn, weight_mode='snis',\n",
        "                        seed=ref_seed, h_coeff=h_coeff, true_score_func=true_score_func, w_correct_func=w_correct_func,\n",
        "                        whiten_scores=whiten_scores,\n",
        "                        whiten_mode=\"recompute_white_scores\",\n",
        "                        proxy_score_fit=proxy_prior_score,\n",
        "                    )\n",
        "\n",
        "                elif score_mode == 'blend':\n",
        "                    # ground-truth/blackbox prior scores → map_white_scores\n",
        "                    out = blend_sampler(\n",
        "                        N_part, N_ref, times, batch, prior_sampler_func, true_init_score, mode=mode,\n",
        "                        likelyhood_func=likelyhood_func, loglik_grad_fn=loglik_grad_fn, weight_mode='snis',\n",
        "                        seed=ref_seed, h_coeff=h_coeff, true_score_func=true_score_func, w_correct_func=w_correct_func,\n",
        "                        # whitening:\n",
        "                        whiten_scores=whiten_scores,\n",
        "                        whiten_mode=\"map_white_scores\",\n",
        "                    )\n",
        "\n",
        "                elif score_mode == 'custom':\n",
        "                    # blackbox sampler → map_white_scores (internally builds W from anchor scores)\n",
        "                    out = blackbox_sampler(\n",
        "                        N_part, times, custom_callable, prior_sampler_func,\n",
        "                        mode=mode, likelyhood_func=likelyhood_func, loglik_grad_fn=loglik_grad_fn,\n",
        "                        h_coeff=h_coeff, true_score_func=true_score_func, p_prune=p_prune,\n",
        "                        # whitening:\n",
        "                        whiten_scores=whiten_scores,\n",
        "                    )\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown score mode {score_mode}\")\n",
        "\n",
        "                # ---------- metrics ----------\n",
        "                if track_score_rmse and true_score_func:\n",
        "                    output_samples, rmse_list = out\n",
        "                    per_trial_rmse.append(np.mean(rmse_list))\n",
        "                else:\n",
        "                    output_samples = out\n",
        "\n",
        "                pruned_output_samples,_ = prune_cp_arr(output_samples, p_percent = p_prune)\n",
        "\n",
        "                if div == 'W2':\n",
        "                    divs.append(sliced_wasserstein2(test_samples,  pruned_output_samples, n_proj=2048, max_pts=5000))\n",
        "                elif div == 'M_KSD':\n",
        "                    divs.append(compute_multiscale_ksd(pruned_output_samples, post_score_func))\n",
        "                elif div == 'M_MMD':\n",
        "                    divs.append(M_MMD(test_samples,  pruned_output_samples))\n",
        "                elif div =='OT_MMD':\n",
        "                    divs.append(OT_MMD(test_samples,  pruned_output_samples))\n",
        "                elif div == 'KSE_diff':\n",
        "                    divs.append(KSE_diff(pruned_output_samples, post_score_func))\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown div {div}\")\n",
        "\n",
        "                if label not in output_dict.keys():\n",
        "                    output_dict[label] = output_samples\n",
        "                else:\n",
        "                    output_dict[label] = np.concatenate((output_dict[label], output_samples))\n",
        "\n",
        "            if do_copy_test:\n",
        "                copy_test(prune_cp_arr(output_dict[label], p_percent = p_prune)[0], train_samples, test_samples, label)\n",
        "\n",
        "            avg_div = np.mean(divs)\n",
        "            divs_dict[label].append(avg_div)\n",
        "            samples_dict[label].append(output_dict[label])\n",
        "\n",
        "            if track_score_rmse and true_score_func:\n",
        "                avg_rmse = np.mean(per_trial_rmse)\n",
        "                score_rmse[label].append(avg_rmse)\n",
        "\n",
        "            floor_samples = post_sampler_func(N_part)\n",
        "            if div == 'W2':\n",
        "                floors.append(sliced_wasserstein2(floor_samples, test_samples))\n",
        "                if comparison_samples is not None:\n",
        "                    compare_div = sliced_wasserstein2(comparison_samples, test_samples)\n",
        "            elif div == 'M_KSD':\n",
        "                floors.append(compute_multiscale_ksd(floor_samples, post_score_func))\n",
        "                if comparison_samples is not None:\n",
        "                    compare_div = compute_multiscale_ksd(comparison_samples, post_score_func)\n",
        "            elif div == 'M_MMD':\n",
        "                floors.append(M_MMD(floor_samples, test_samples))\n",
        "                if comparison_samples is not None:\n",
        "                    compare_div = M_MMD(comparison_samples, test_samples)\n",
        "            elif div == 'OT_MMD':\n",
        "                floors.append(OT_MMD(floor_samples, test_samples))\n",
        "                if comparison_samples is not None:\n",
        "                    compare_div = OT_MMD(comparison_samples, test_samples)\n",
        "            elif div == 'KSE_diff':\n",
        "                floors.append(KSE_diff(floor_samples, post_score_func))\n",
        "                if comparison_samples is not None:\n",
        "                    compare_div = KSE_diff(comparison_samples, post_score_func)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown div {div}\")\n",
        "\n",
        "            if comparison_label is not None:\n",
        "                compare_str = f' {comparison_label}: {div} = {compare_div:.3e},'\n",
        "                output_dict['comparison_label'] = comparison_samples\n",
        "            else:\n",
        "                compare_str = ''\n",
        "\n",
        "            if len(save_tag):\n",
        "                print_tag = f'{save_tag}, '\n",
        "            else:\n",
        "                print_tag = ''\n",
        "\n",
        "            mean_floor = np.mean(floors)\n",
        "            divs_dict['floor'] = mean_floor\n",
        "\n",
        "            if print_res:\n",
        "                if track_score_rmse and true_score_func:\n",
        "                    print(f\"{print_tag}{steps:2d} steps, {label}: {div} ={avg_div:.3e}, {div}_floor = {mean_floor:.3e}, score_RMSE={avg_rmse:.3e}, time={time.time()-t0:.1f}s\")\n",
        "                    print(' ')\n",
        "                else:\n",
        "                    print(f\"{print_tag}{steps:2d} steps, {label}: {div} ={avg_div:.3e},{compare_str} {div}_floor = {np.mean(floors):.3e}, time={time.time()-t0:.1f}s\")\n",
        "                    print(' ')\n",
        "\n",
        "        true_prior_samples = true_sampler_func(trials * N_part)\n",
        "\n",
        "        if plot_hists:\n",
        "            if likelyhood_func is not None:\n",
        "                post_samples = post_sampler_func(trials * N_part)\n",
        "            else:\n",
        "                post_samples = None\n",
        "\n",
        "            plot_pair_histograms(\n",
        "                output_dict, true_prior_samples, post_samples,\n",
        "                save_path=f\"{save_tag}_{steps}_steps_mean_hist.png\",\n",
        "                bins=mean_bins, nrows=nrows, mode=hist_mode, display_mode=display_mode, hist_norm=hist_norm,\n",
        "                dim_pairs=d_pairs, pre_process_f=pre_process_f, likelyhood_func=likelyhood_func,\n",
        "                draw_countours=draw_countours, prior_scale=prior_scale,\n",
        "                plot_prior=plot_prior, post_hist_norm=post_hist_norm, plot_post=plot_post\n",
        "            )\n",
        "\n",
        "        results[f'{steps}_dict'] = (output_dict, divs_dict)\n",
        "\n",
        "    if plot_res:\n",
        "        plt.figure(figsize=(6,4))\n",
        "        colors = ['blue', 'red', 'green', 'black','orange', 'purple','pink', 'yellow', 'grey']\n",
        "        floor = np.mean(floors)\n",
        "        print(f\"Floor={floor:.4e}\")\n",
        "\n",
        "        max_avg = -np.inf\n",
        "        for i, (_, _, sampler, *_) in enumerate(samplers[:len(colors)]):\n",
        "            if ('heun' in sampler.lower()):\n",
        "                NFEs = 2 * np.asarray(steps_list)\n",
        "            else:\n",
        "                NFEs =  np.asarray(steps_list)\n",
        "            plt.plot(NFEs , np.log(results[f'{sampler}']), marker='o',\n",
        "                     label = f'{sampler}', color = colors[i])\n",
        "            if np.max(np.log(results[f'{sampler}'])) > max_avg:\n",
        "                max_avg = np.max(np.log(results[f'{sampler}']))\n",
        "\n",
        "        plt.plot(NFEs, np.log(np.asarray([floor for _ in NFEs])),\n",
        "                 label = 'floor', color = 'black', linestyle = 'dashed')\n",
        "\n",
        "        plt.xlabel(\"NFE\", fontsize=11)\n",
        "        if clip_plot:\n",
        "            plt.xlim(left = 2 *  steps_list[0], right = steps_list[-1])\n",
        "\n",
        "        if div == 'W2':\n",
        "            plt.ylabel(f\" Mean Log Wasserstein\", fontsize=13)\n",
        "        elif div == 'M_KSD':\n",
        "            plt.ylabel(f\" Mean Log M-KSD\", fontsize=13)\n",
        "\n",
        "        plt.title(f\"Sampler Comparison\", fontsize=13)\n",
        "        plt.legend(loc='upper right')\n",
        "        plt.grid(ls='--', alpha=0.4)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{trial_name}.png')\n",
        "        plt.show()\n",
        "\n",
        "    samples_dict['True prior'] = true_prior_samples\n",
        "    try:\n",
        "        samples_dict['True posterior'] = OU_evolve_samples(post_sampler_func(trials * N_part), T_target)\n",
        "    except Exception:\n",
        "        print('There was an issue big dawg')\n",
        "\n",
        "    results['samples_dict'] = samples_dict\n",
        "    results['divs_dict'] = divs_dict\n",
        "    if track_score_rmse and true_score_func:\n",
        "        results['score_rmse'] = score_rmse\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "# ─── 4. MAIN SWEEP ─────────────────────────────────────────────────────────────\n",
        "\n",
        "def main1(N_REF = 500, plot_hists = False, do_inv = False, rand_seed = False,\n",
        "          div = 'W2',save_tag = '', ridge_frac = 1e-6,  trials = 6):\n",
        "\n",
        "    ################################################################################################################################################################################\n",
        "    SEED  = 2\n",
        "    NUM_C, K_DIM, M_DIM, = 2000, 9, 3\n",
        "    VARIANT, STD, SCALE, norm_size = \"helix\", .1, 3.0, 1.0\n",
        "    T_target, T_end = 5e-4, 1.5\n",
        "    EMBED_MODE = 'sine_wiggle'\n",
        "    ################################################################################################################################################################################\n",
        "\n",
        "    # 1) Build your pathological GMM and grab its params for the oracle\n",
        "    params, sampler_func, score_func, density_func, score_div_func = get_gmm_funcs(num_c = NUM_C, variant = VARIANT, embedding_mode=EMBED_MODE,\n",
        "                                                     overall_scale = SCALE, comp_std=STD, k_dim =K_DIM, m_dim = M_DIM, normalize = True)\n",
        "\n",
        "    proxy_score_func = fit_kernel_score\n",
        "\n",
        "\n",
        "    test_sample = sampler_func(1)\n",
        "    test_density = density_func(test_sample)\n",
        "\n",
        "    #params, sampler_func, score_func = make_pathological_gmm(\n",
        "        #C=500, d=39, sep=2, cov_scale=0.01, seed=0)\n",
        "    means0, covars0, w0 = params   # these are NumPy arrays\n",
        "\n",
        "\n",
        "\n",
        "    # **Convert to Cupy once** so all downstream calls get Cupy arrays\n",
        "    means0_cp = cp.asarray(means0, dtype=cp.float64)\n",
        "    covars0_cp  = cp.asarray(covars0,  dtype=cp.float64)\n",
        "    w0_cp  = cp.asarray(w0,dtype=cp.float64)\n",
        "\n",
        "\n",
        "    ################################################################################################################################################################################\n",
        "    if do_inv:\n",
        "      D = means0.shape[1]\n",
        "      rank_k = 2         # e.g., project down to 3 observed dims\n",
        "      obs_sigma = .1\n",
        "\n",
        "      #y_obs = sampler_func(1, seed=SEED).reshape(-1)\n",
        "      A, y_obs_k, likelyhood_func, log_likelyhood_func, loglik_grad_fn = make_rank_k_likelihood(\n",
        "        D=D, rank_k=rank_k, obs_sigma=obs_sigma,\n",
        "        sampler_func=sampler_func, seed=3)\n",
        "\n",
        "      def log_density_func(x_ref):\n",
        "        return cp.log(density_func(x_ref))  # unchanged\n",
        "\n",
        "      Rk = (obs_sigma**2) * cp.eye(rank_k, dtype=cp.float64)\n",
        "\n",
        "      w_post, means_post, covars_post = gmm_posterior_params(y_obs_k, w0, means0, covars0, A=A, b=None, R=Rk)\n",
        "\n",
        "      post_params = (means_post, covars_post, w_post)\n",
        "      _, post_sampler_func, post_score_func, post_density_func, post_score_div_func = get_gmm_funcs(num_c = NUM_C, variant = 'custom',\n",
        "                                                                               preset_params = post_params,  normalize = False)\n",
        "\n",
        "      means_post_cp = cp.asarray(means_post, dtype=cp.float64)\n",
        "      covars_post_cp  = cp.asarray(covars_post,  dtype=cp.float64)\n",
        "      w_post_cp     = cp.asarray(w_post,dtype=cp.float64)\n",
        "\n",
        "    else:\n",
        "      post_sampler_func = None\n",
        "      post_score_func = score_func\n",
        "      post_score_div_func = score_div_func\n",
        "      likelyhood_func = None\n",
        "      loglik_grad_fn = None\n",
        "      means_post_cp = means0_cp\n",
        "      covars_post_cp  = covars0_cp\n",
        "      w_post_cp     = w0_cp\n",
        "\n",
        "    ################################################################################################################################################################################\n",
        "\n",
        "\n",
        "    # 2) Wrap the true‐score calculator so it matches run_comparison’s API:\n",
        "    #    true_score_func(y_batch, t_cur) -> [B, d] cupy array\n",
        "    def true_score_func(y_batch, t_cur):\n",
        "        # here batch_size is only used for micro‐batching inside calculate_true_score_at_t,\n",
        "        # but since y_batch is already a batch we just pass len(y_batch)\n",
        "        return calculate_true_score_at_t(\n",
        "            y_batch,\n",
        "            t_cur,\n",
        "            means_post_cp, covars_post_cp, w_post_cp,\n",
        "            batch_size=y_batch.shape[0])\n",
        "\n",
        "\n",
        "    # 3) Define your samplers\n",
        "    samplers = [\n",
        "        ('heun_pc', 'blend',        'Blend score( True )'),\n",
        "        ('heun_pc', 'blend_proxy_SVD_recomp',  'Blend score( Proxy )'),\n",
        "        ('heun_pc', 'tweedie',      'Tweedie score'),\n",
        "        ('heun_pc', 'kss',          'CSEM score'),\n",
        "        #('heun_pc', 'blend_proxy_recomp',  'Blend score( Proxy )' ),\n",
        "        #('heun_pc', 'blend_proxy_SVD',  'Blend score( Proxy SVD )' ),\n",
        "        #('heun_pc', 'blend_proxy_SVD_diag',  'Blend score( Proxy SVD_diag)' ),\n",
        "        #('heun_pc', 'blend_proxy_SVD_recomp_diag',  'Blend score( Proxy SVD_diag recomp )' ),\n",
        "    ]\n",
        "    # 4) Call run_comparison with the new arguments:\n",
        "\n",
        "\n",
        "    results = run_comparison(\n",
        "        samplers              = samplers,\n",
        "        steps_list            = [10],\n",
        "        true_sampler_func     = sampler_func,\n",
        "        prior_sampler_func    = sampler_func,\n",
        "        prior_score_func      = score_func,\n",
        "        prior_score_div_func  = score_div_func,\n",
        "        likelyhood_func       = likelyhood_func,\n",
        "        loglik_grad_fn        = loglik_grad_fn,\n",
        "        post_sampler_func     = post_sampler_func,\n",
        "        post_score_func       = post_score_func,\n",
        "        post_score_div_func   = post_score_div_func,\n",
        "        true_init_score       = score_func,\n",
        "        ref_seed              = 0,\n",
        "        trials                = trials,\n",
        "        N_part                = 5000,\n",
        "        nrows                 = 2,\n",
        "        time_split            = 'power',\n",
        "        trial_name            = 'test',\n",
        "        div                   = div,\n",
        "        plot_hists            = plot_hists,\n",
        "        hist_mode             = 'pca',\n",
        "        display_mode          =  'min_label',\n",
        "        d_pairs               = None,\n",
        "        plot_res              = False,\n",
        "        N_ref                 = N_REF,\n",
        "        T_end                 = T_end,\n",
        "        T_target              = T_target,\n",
        "        true_score_func       = true_score_func,\n",
        "        track_score_rmse      = True,\n",
        "        mean_bins             = 120,\n",
        "        prior_scale           = True,\n",
        "        plot_prior            = True,\n",
        "        plot_post             = do_inv,\n",
        "        draw_countours        = do_inv,\n",
        "        hist_norm             = 1.6,\n",
        "        post_hist_norm        = 1.0,\n",
        "        save_tag              = save_tag,\n",
        "        ridge_frac            = ridge_frac,\n",
        "        rand_seed             = rand_seed,\n",
        "        cov_rank              = M_DIM ,\n",
        "        do_copy_test          = False,\n",
        "        whiten_scores         = False,\n",
        "        )\n",
        "\n",
        "    divs_dict = results['divs_dict']\n",
        "    samples_dict = results['samples_dict']\n",
        "\n",
        "    return results\n",
        "\n",
        "    # 5) results will now include keys like\n",
        "    #    'HHop, Tweedie score_score_rmse' and 'HHop, Blend score_score_rmse'\n",
        "    #print(results)\n",
        "\n",
        "\n",
        "def negate(arg):\n",
        "  if arg:\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pass\n",
        "    main1(N_REF = 700, plot_hists = True, do_inv = False, rand_seed = False,\n",
        "         div = 'W2',save_tag = '', ridge_frac = 1e-6,  trials = 7)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IssiKhwKmqIY"
      },
      "outputs": [],
      "source": [
        "def pick_key(d, must_contain, exclude=None):\n",
        "    \"\"\"\n",
        "    Find first key in d that contains all substrings in must_contain\n",
        "    and does not contain any substring in exclude (case-insensitive).\n",
        "    \"\"\"\n",
        "    if exclude is None:\n",
        "        exclude = []\n",
        "    must_contain = [s.lower() for s in must_contain]\n",
        "    exclude      = [s.lower() for s in exclude]\n",
        "\n",
        "    for k in d.keys():\n",
        "        kl = k.lower()\n",
        "        if all(s in kl for s in must_contain) and all(e not in kl for e in exclude):\n",
        "            return k\n",
        "    raise KeyError(f\"No key found in {list(d.keys())} containing {must_contain} and excluding {exclude}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "\n",
        "    # --- global plotting style for publication -----------------------------\n",
        "    plt.rcParams.update({\n",
        "        \"font.size\": 9,\n",
        "        \"axes.spines.top\": False,\n",
        "        \"axes.spines.right\": False,\n",
        "    })\n",
        "\n",
        "    # reference sizes to sweep\n",
        "    N_REFS = [25, 50, 75, 100, 150, 200, 300, 400, 500,\n",
        "              600, 800, 1000, 1500, 2000, 2500, 3000, 4000, 5000]\n",
        "\n",
        "    blend_divs       = []\n",
        "    blend_proxy_divs = []\n",
        "    tweedie_divs     = []\n",
        "    csem_divs        = []\n",
        "    floors           = []\n",
        "\n",
        "    for N_REF in N_REFS:\n",
        "        print(f\"\\n=== MMD sweep: N_ref = {N_REF} ===\")\n",
        "        res   = main1(\n",
        "            N_REF      = N_REF,\n",
        "            plot_hists = False,\n",
        "            do_inv     = False,\n",
        "            rand_seed  = True,\n",
        "            div        = 'M_MMD',\n",
        "            save_tag   = f'N_REF_{N_REF}',\n",
        "            ridge_frac = 1e-6,\n",
        "            trials     = 5,\n",
        "        )\n",
        "        ddivs = res['divs_dict']  # {label: [avg_div], 'floor': float}\n",
        "\n",
        "        # robustly grab by substrings, and avoid any SVD proxy label\n",
        "        k_blend_true  = pick_key(ddivs, [\"blend\"], exclude=[\"proxy\"])\n",
        "        k_blend_proxy = pick_key(ddivs, [\"blend\", \"proxy\",\"svd\"])\n",
        "        k_tweedie     = pick_key(ddivs, [\"tweedie\"])\n",
        "        k_csem        = pick_key(ddivs, [\"csem\"])\n",
        "\n",
        "        blend_divs.append(      float(np.asarray(ddivs[k_blend_true]) [-1]))\n",
        "        blend_proxy_divs.append(float(np.asarray(ddivs[k_blend_proxy])[-1]))\n",
        "        tweedie_divs.append(    float(np.asarray(ddivs[k_tweedie])    [-1]))\n",
        "        csem_divs.append(       float(np.asarray(ddivs[k_csem])       [-1]))\n",
        "        floors.append(          float(ddivs[\"floor\"]))\n",
        "\n",
        "    # use a single averaged ground-truth floor across N_REF\n",
        "    floor_mean = float(np.mean(floors))\n",
        "    floors_plot = [floor_mean] * len(N_REFS)\n",
        "\n",
        "    # --- make the plot (log–log) ------------------------------------------\n",
        "    fig, ax = plt.subplots(figsize=(3.6, 2.7))\n",
        "\n",
        "    ax.plot(N_REFS, blend_divs,\n",
        "            color=\"tab:blue\", lw=1.5, label=\"Blend (ground-truth $s_0$)\")\n",
        "    ax.plot(N_REFS, blend_proxy_divs,\n",
        "            color=\"tab:blue\", lw=1.5, ls=\"--\", label=\"Blend (diagonal proxy $\\\\hat s_0$)\")\n",
        "    ax.plot(N_REFS, tweedie_divs,\n",
        "            color=\"tab:red\", lw=1.5, label=\"Tweedie\")\n",
        "    ax.plot(N_REFS, csem_divs,\n",
        "            color=\"tab:green\", lw=1.5, label=\"CSEM\")\n",
        "    ax.plot(N_REFS, floors_plot,\n",
        "            color=\"black\", lw=1.2, ls=\"--\", label=\"Ground-truth floor\")\n",
        "\n",
        "    ax.set_xscale(\"log\")\n",
        "    ax.set_yscale(\"log\")\n",
        "    ax.set_xlabel(r\"$N_{\\text{ref}}$\")\n",
        "    ax.set_ylabel(\"MMD\")\n",
        "\n",
        "    ax.grid(ls=\"--\", alpha=0.4)\n",
        "\n",
        "    # legend above axes so it doesn't cover curves\n",
        "    ax.legend(\n",
        "        loc=\"upper center\",\n",
        "        bbox_to_anchor=(0.5, 1.18),\n",
        "        ncol=2,\n",
        "        fontsize=8,\n",
        "        frameon=False,\n",
        "    )\n",
        "\n",
        "    fig.tight_layout(rect=[0.0, 0.0, 1.0, 0.95])\n",
        "    fig.savefig(\"figs/blend_mmd_vs_Nref.png\", dpi=300)\n",
        "    print(\"Saved figs/blend_mmd_vs_Nref.png\")\n",
        "\n",
        "\n",
        "'''\n",
        "=== MMD sweep: N_ref = 25 ===\n",
        "N_REF_25, 10 steps, Blend score: M_MMD =1.332e-01, M_MMD_floor = 1.360e-02, score_RMSE=7.133e+01, time=15.0s\n",
        "\n",
        "N_REF_25, 10 steps, Tweedie score: M_MMD =1.311e-01, M_MMD_floor = 6.799e-03, score_RMSE=2.490e+01, time=14.6s\n",
        "\n",
        "N_REF_25, 10 steps, CSEM score: M_MMD =8.754e-01, M_MMD_floor = 6.421e-03, score_RMSE=3.191e+02, time=14.6s\n",
        "\n",
        "N_REF_25, 10 steps, Blend score( Proxy ): M_MMD =2.171e-01, M_MMD_floor = 4.816e-03, score_RMSE=1.281e+02, time=15.0s\n",
        "\n",
        "N_REF_25, 10 steps, Blend score( Proxy SVD recomp ): M_MMD =1.389e-01, M_MMD_floor = 3.853e-03, score_RMSE=7.474e+01, time=15.5s\n",
        "\n",
        "\n",
        "=== MMD sweep: N_ref = 50 ===\n",
        "N_REF_50, 10 steps, Blend score: M_MMD =9.085e-02, M_MMD_floor = 3.233e-03, score_RMSE=5.906e+01, time=15.1s\n",
        "\n",
        "N_REF_50, 10 steps, Tweedie score: M_MMD =8.142e-02, M_MMD_floor = 1.617e-03, score_RMSE=2.486e+01, time=14.8s\n",
        "\n",
        "N_REF_50, 10 steps, CSEM score: M_MMD =8.852e-01, M_MMD_floor = 1.078e-03, score_RMSE=2.102e+02, time=14.6s\n",
        "\n",
        "N_REF_50, 10 steps, Blend score( Proxy ): M_MMD =1.376e-01, M_MMD_floor = 8.083e-04, score_RMSE=1.075e+02, time=15.1s\n",
        "\n",
        "N_REF_50, 10 steps, Blend score( Proxy SVD recomp ): M_MMD =9.201e-02, M_MMD_floor = 6.466e-04, score_RMSE=6.242e+01, time=16.0s\n",
        "\n",
        "\n",
        "=== MMD sweep: N_ref = 75 ===\n",
        "N_REF_75, 10 steps, Blend score: M_MMD =7.688e-02, M_MMD_floor = 1.007e-02, score_RMSE=5.296e+01, time=15.4s\n",
        "\n",
        "N_REF_75, 10 steps, Tweedie score: M_MMD =7.517e-02, M_MMD_floor = 5.034e-03, score_RMSE=2.482e+01, time=14.9s\n",
        "\n",
        "N_REF_75, 10 steps, CSEM score: M_MMD =6.565e-01, M_MMD_floor = 7.541e-03, score_RMSE=1.037e+02, time=14.9s\n",
        "\n",
        "N_REF_75, 10 steps, Blend score( Proxy ): M_MMD =1.237e-01, M_MMD_floor = 8.947e-03, score_RMSE=8.601e+01, time=15.4s\n",
        "\n",
        "N_REF_75, 10 steps, Blend score( Proxy SVD recomp ): M_MMD =8.413e-02, M_MMD_floor = 7.158e-03, score_RMSE=5.344e+01, time=16.6s\n",
        "\n",
        "\n",
        "=== MMD sweep: N_ref = 100 ===\n",
        "N_REF_100, 10 steps, Blend score: M_MMD =6.367e-02, M_MMD_floor = 0.000e+00, score_RMSE=5.001e+01, time=15.9s\n",
        "\n",
        "N_REF_100, 10 steps, Tweedie score: M_MMD =6.520e-02, M_MMD_floor = 1.027e-02, score_RMSE=2.489e+01, time=15.1s\n",
        "\n",
        "N_REF_100, 10 steps, CSEM score: M_MMD =6.109e-01, M_MMD_floor = 1.203e-02, score_RMSE=8.263e+01, time=15.1s\n",
        "\n",
        "N_REF_100, 10 steps, Blend score( Proxy ): M_MMD =1.065e-01, M_MMD_floor = 9.020e-03, score_RMSE=7.280e+01, time=15.7s\n",
        "\n",
        "N_REF_100, 10 steps, Blend score( Proxy SVD recomp ): M_MMD =7.011e-02, M_MMD_floor = 1.015e-02, score_RMSE=4.928e+01, time=17.4s\n",
        "\n",
        "\n",
        "=== MMD sweep: N_ref = 150 ===\n",
        "N_REF_150, 10 steps, Blend score: M_MMD =5.127e-02, M_MMD_floor = 1.843e-02, score_RMSE=4.604e+01, time=16.5s\n",
        "\n",
        "N_REF_150, 10 steps, Tweedie score: M_MMD =6.887e-02, M_MMD_floor = 9.215e-03, score_RMSE=2.500e+01, time=15.5s\n",
        "\n",
        "N_REF_150, 10 steps, CSEM score: M_MMD =3.821e-01, M_MMD_floor = 6.143e-03, score_RMSE=3.897e+01, time=15.5s\n",
        "\n",
        "N_REF_150, 10 steps, Blend score( Proxy ): M_MMD =7.824e-02, M_MMD_floor = 8.743e-03, score_RMSE=5.715e+01, time=16.3s\n",
        "\n",
        "N_REF_150, 10 steps, Blend score( Proxy SVD recomp ): M_MMD =7.013e-02, M_MMD_floor = 6.995e-03, score_RMSE=4.506e+01, time=19.0s\n",
        "\n",
        "\n",
        "=== MMD sweep: N_ref = 200 ===\n",
        "N_REF_200, 10 steps, Blend score: M_MMD =3.884e-02, M_MMD_floor = 4.721e-03, score_RMSE=4.131e+01, time=17.0s\n",
        "\n",
        "N_REF_200, 10 steps, Tweedie score: M_MMD =4.701e-02, M_MMD_floor = 2.360e-03, score_RMSE=2.495e+01, time=15.8s\n",
        "\n",
        "N_REF_200, 10 steps, CSEM score: M_MMD =4.995e-01, M_MMD_floor = 1.574e-03, score_RMSE=4.800e+01, time=15.8s\n",
        "\n",
        "N_REF_200, 10 steps, Blend score( Proxy ): M_MMD =5.701e-02, M_MMD_floor = 4.041e-03, score_RMSE=4.811e+01, time=17.0s\n",
        "\n",
        "N_REF_200, 10 steps, Blend score( Proxy SVD recomp ): M_MMD =5.199e-02, M_MMD_floor = 4.555e-03, score_RMSE=3.954e+01, time=20.4s\n",
        "\n",
        "\n",
        "=== MMD sweep: N_ref = 300 ===\n",
        "N_REF_300, 10 steps, Blend score: M_MMD =2.503e-02, M_MMD_floor = 0.000e+00, score_RMSE=3.714e+01, time=18.3s\n",
        "\n",
        "N_REF_300, 10 steps, Tweedie score: M_MMD =2.904e-02, M_MMD_floor = 0.000e+00, score_RMSE=2.497e+01, time=16.5s\n",
        "\n",
        "N_REF_300, 10 steps, CSEM score: M_MMD =2.852e-01, M_MMD_floor = 0.000e+00, score_RMSE=1.218e+01, time=16.5s\n",
        "\n",
        "N_REF_300, 10 steps, Blend score( Proxy ): M_MMD =3.931e-02, M_MMD_floor = 0.000e+00, score_RMSE=3.880e+01, time=18.2s\n",
        "\n",
        "N_REF_300, 10 steps, Blend score( Proxy SVD recomp ): M_MMD =3.704e-02, M_MMD_floor = 0.000e+00, score_RMSE=3.501e+01, time=23.4s\n",
        "\n",
        "\n",
        "=== MMD sweep: N_ref = 400 ===\n",
        "N_REF_400, 10 steps, Blend score: M_MMD =2.217e-02, M_MMD_floor = 1.893e-02, score_RMSE=3.433e+01, time=19.6s\n",
        "\n",
        "N_REF_400, 10 steps, Tweedie score: M_MMD =2.710e-02, M_MMD_floor = 9.464e-03, score_RMSE=2.509e+01, time=17.1s\n",
        "\n",
        "N_REF_400, 10 steps, CSEM score: M_MMD =2.742e-01, M_MMD_floor = 7.810e-03, score_RMSE=1.356e+01, time=17.2s\n",
        "\n",
        "N_REF_400, 10 steps, Blend score( Proxy ): M_MMD =3.246e-02, M_MMD_floor = 9.572e-03, score_RMSE=3.311e+01, time=19.4s\n",
        "\n",
        "N_REF_400, 10 steps, Blend score( Proxy SVD recomp ): M_MMD =3.334e-02, M_MMD_floor = 9.498e-03, score_RMSE=3.201e+01, time=26.4s\n",
        "\n",
        "\n",
        "=== MMD sweep: N_ref = 500 ===\n",
        "N_REF_500, 10 steps, Blend score: M_MMD =1.816e-02, M_MMD_floor = 0.000e+00, score_RMSE=3.191e+01, time=20.9s\n",
        "\n",
        "N_REF_500, 10 steps, Tweedie score: M_MMD =2.586e-02, M_MMD_floor = 0.000e+00, score_RMSE=2.524e+01, time=17.8s\n",
        "\n",
        "N_REF_500, 10 steps, CSEM score: M_MMD =2.436e-01, M_MMD_floor = 0.000e+00, score_RMSE=1.042e+01, time=17.9s\n",
        "\n",
        "N_REF_500, 10 steps, Blend score( Proxy ): M_MMD =3.312e-02, M_MMD_floor = 0.000e+00, score_RMSE=3.084e+01, time=20.8s\n",
        "\n",
        "N_REF_500, 10 steps, Blend score( Proxy SVD recomp ): M_MMD =2.669e-02, M_MMD_floor = 3.639e-03, score_RMSE=2.885e+01, time=29.2s\n",
        "\n",
        "\n",
        "=== MMD sweep: N_ref = 600 ===\n",
        "N_REF_600, 10 steps, Blend score: M_MMD =2.096e-02, M_MMD_floor = 0.000e+00, score_RMSE=3.000e+01, time=22.3s\n",
        "\n",
        "N_REF_600, 10 steps, Tweedie score: M_MMD =1.979e-02, M_MMD_floor = 0.000e+00, score_RMSE=2.524e+01, time=18.8s\n",
        "\n",
        "N_REF_600, 10 steps, CSEM score: M_MMD =1.897e-01, M_MMD_floor = 0.000e+00, score_RMSE=7.634e+00, time=18.7s\n",
        "\n",
        "N_REF_600, 10 steps, Blend score( Proxy ): M_MMD =3.557e-02, M_MMD_floor = 0.000e+00, score_RMSE=3.037e+01, time=22.1s\n",
        "\n",
        "N_REF_600, 10 steps, Blend score( Proxy SVD recomp ): M_MMD =2.253e-02, M_MMD_floor = 0.000e+00, score_RMSE=2.828e+01, time=32.3s\n",
        "\n",
        "\n",
        "=== MMD sweep: N_ref = 800 ===\n",
        "N_REF_800, 10 steps, Blend score: M_MMD =2.154e-02, M_MMD_floor = 0.000e+00, score_RMSE=2.689e+01, time=24.8s\n",
        "\n",
        "N_REF_800, 10 steps, Tweedie score: M_MMD =2.022e-02, M_MMD_floor = 1.377e-02, score_RMSE=2.526e+01, time=20.1s\n",
        "\n",
        "N_REF_800, 10 steps, CSEM score: M_MMD =1.490e-01, M_MMD_floor = 1.456e-02, score_RMSE=5.952e+00, time=20.1s\n",
        "\n",
        "N_REF_800, 10 steps, Blend score( Proxy ): M_MMD =3.079e-02, M_MMD_floor = 1.092e-02, score_RMSE=2.616e+01, time=24.6s\n",
        "\n",
        "N_REF_800, 10 steps, Blend score( Proxy SVD recomp ): M_MMD =2.290e-02, M_MMD_floor = 1.125e-02, score_RMSE=2.539e+01, time=38.1s\n",
        "\n",
        "\n",
        "=== MMD sweep: N_ref = 1000 ===\n",
        "N_REF_1000, 10 steps, Blend score: M_MMD =1.938e-02, M_MMD_floor = 0.000e+00, score_RMSE=2.447e+01, time=27.1s\n",
        "\n",
        "N_REF_1000, 10 steps, Tweedie score: M_MMD =2.086e-02, M_MMD_floor = 0.000e+00, score_RMSE=2.541e+01, time=21.3s\n",
        "\n",
        "N_REF_1000, 10 steps, CSEM score: M_MMD =1.984e-01, M_MMD_floor = 0.000e+00, score_RMSE=5.848e+00, time=21.4s\n",
        "\n",
        "N_REF_1000, 10 steps, Blend score( Proxy ): M_MMD =2.623e-02, M_MMD_floor = 0.000e+00, score_RMSE=2.299e+01, time=26.9s\n",
        "\n",
        "N_REF_1000, 10 steps, Blend score( Proxy SVD recomp ): M_MMD =2.520e-02, M_MMD_floor = 1.449e-03, score_RMSE=2.286e+01, time=43.9s\n",
        "\n",
        "\n",
        "=== MMD sweep: N_ref = 1500 ===\n",
        "N_REF_1500, 10 steps, Blend score: M_MMD =1.502e-02, M_MMD_floor = 0.000e+00, score_RMSE=2.113e+01, time=32.8s\n",
        "\n",
        "N_REF_1500, 10 steps, Tweedie score: M_MMD =1.943e-02, M_MMD_floor = 0.000e+00, score_RMSE=2.537e+01, time=24.2s\n",
        "\n",
        "N_REF_1500, 10 steps, CSEM score: M_MMD =1.313e-01, M_MMD_floor = 0.000e+00, score_RMSE=3.875e+00, time=24.3s\n",
        "\n",
        "N_REF_1500, 10 steps, Blend score( Proxy ): M_MMD =2.625e-02, M_MMD_floor = 1.887e-03, score_RMSE=2.024e+01, time=32.5s\n",
        "\n",
        "N_REF_1500, 10 steps, Blend score( Proxy SVD recomp ): M_MMD =2.515e-02, M_MMD_floor = 1.510e-03, score_RMSE=1.955e+01, time=58.4s\n",
        "\n",
        "\n",
        "=== MMD sweep: N_ref = 2000 ===\n",
        "N_REF_2000, 10 steps, Blend score: M_MMD =1.714e-02, M_MMD_floor = 0.000e+00, score_RMSE=1.855e+01, time=38.5s\n",
        "\n",
        "N_REF_2000, 10 steps, Tweedie score: M_MMD =8.542e-03, M_MMD_floor = 6.462e-03, score_RMSE=2.548e+01, time=27.0s\n",
        "\n",
        "N_REF_2000, 10 steps, CSEM score: M_MMD =1.733e-01, M_MMD_floor = 5.051e-03, score_RMSE=3.420e+00, time=27.1s\n",
        "\n",
        "N_REF_2000, 10 steps, Blend score( Proxy ): M_MMD =2.316e-02, M_MMD_floor = 3.788e-03, score_RMSE=1.827e+01, time=38.2s\n",
        "\n",
        "N_REF_2000, 10 steps, Blend score( Proxy SVD recomp ): M_MMD =1.669e-02, M_MMD_floor = 3.031e-03, score_RMSE=1.714e+01, time=72.4s\n",
        "\n",
        "\n",
        "=== MMD sweep: N_ref = 2500 ===\n",
        "N_REF_2500, 10 steps, Blend score: M_MMD =3.604e-03, M_MMD_floor = 0.000e+00, score_RMSE=1.768e+01, time=43.8s\n",
        "\n",
        "N_REF_2500, 10 steps, Tweedie score: M_MMD =7.671e-03, M_MMD_floor = 0.000e+00, score_RMSE=2.546e+01, time=29.3s\n",
        "\n",
        "N_REF_2500, 10 steps, CSEM score: M_MMD =1.228e-01, M_MMD_floor = 0.000e+00, score_RMSE=3.204e+00, time=29.6s\n",
        "\n",
        "N_REF_2500, 10 steps, Blend score( Proxy ): M_MMD =1.751e-02, M_MMD_floor = 0.000e+00, score_RMSE=1.691e+01, time=43.2s\n",
        "\n",
        "N_REF_2500, 10 steps, Blend score( Proxy SVD recomp ): M_MMD =6.972e-03, M_MMD_floor = 0.000e+00, score_RMSE=1.549e+01, time=86.3s\n",
        "\n",
        "\n",
        "=== MMD sweep: N_ref = 3000 ===\n",
        "N_REF_3000, 10 steps, Blend score: M_MMD =9.086e-03, M_MMD_floor = 0.000e+00, score_RMSE=1.569e+01, time=49.2s\n",
        "\n",
        "N_REF_3000, 10 steps, Tweedie score: M_MMD =2.085e-02, M_MMD_floor = 0.000e+00, score_RMSE=2.549e+01, time=31.8s\n",
        "\n",
        "N_REF_3000, 10 steps, CSEM score: M_MMD =1.037e-01, M_MMD_floor = 0.000e+00, score_RMSE=2.947e+00, time=31.9s\n",
        "\n",
        "N_REF_3000, 10 steps, Blend score( Proxy ): M_MMD =1.594e-02, M_MMD_floor = 2.797e-03, score_RMSE=1.535e+01, time=48.5s\n",
        "\n",
        "N_REF_3000, 10 steps, Blend score( Proxy SVD recomp ): M_MMD =7.769e-03, M_MMD_floor = 2.238e-03, score_RMSE=1.409e+01, time=100.3s\n",
        "\n",
        "\n",
        "=== MMD sweep: N_ref = 4000 ===\n",
        "N_REF_4000, 10 steps, Blend score: M_MMD =8.309e-03, M_MMD_floor = 1.284e-02, score_RMSE=1.437e+01, time=59.2s\n",
        "\n",
        "N_REF_4000, 10 steps, Tweedie score: M_MMD =9.104e-03, M_MMD_floor = 1.499e-02, score_RMSE=2.549e+01, time=35.9s\n",
        "\n",
        "N_REF_4000, 10 steps, CSEM score: M_MMD =1.243e-01, M_MMD_floor = 1.302e-02, score_RMSE=2.647e+00, time=36.3s\n",
        "\n",
        "N_REF_4000, 10 steps, Blend score( Proxy ): M_MMD =5.876e-03, M_MMD_floor = 9.763e-03, score_RMSE=1.341e+01, time=58.5s\n",
        "\n",
        "N_REF_4000, 10 steps, Blend score( Proxy SVD recomp ): M_MMD =1.012e-02, M_MMD_floor = 7.810e-03, score_RMSE=1.218e+01, time=127.2s\n",
        "\n",
        "\n",
        "=== MMD sweep: N_ref = 5000 ===\n",
        "N_REF_5000, 10 steps, Blend score: M_MMD =5.254e-03, M_MMD_floor = 0.000e+00, score_RMSE=1.254e+01, time=69.0s\n",
        "\n",
        "N_REF_5000, 10 steps, Tweedie score: M_MMD =1.821e-02, M_MMD_floor = 0.000e+00, score_RMSE=2.547e+01, time=40.0s\n",
        "\n",
        "N_REF_5000, 10 steps, CSEM score: M_MMD =8.747e-02, M_MMD_floor = 0.000e+00, score_RMSE=2.475e+00, time=40.3s\n",
        "\n",
        "N_REF_5000, 10 steps, Blend score( Proxy ): M_MMD =1.107e-02, M_MMD_floor = 5.375e-03, score_RMSE=1.231e+01, time=68.2s\n",
        "\n",
        "N_REF_5000, 10 steps, Blend score( Proxy SVD recomp ): M_MMD =5.410e-03, M_MMD_floor = 4.300e-03, score_RMSE=1.098e+01, time=154.1s\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "\n",
        "    plt.rcParams.update({\n",
        "        \"font.size\": 9,\n",
        "        \"axes.spines.top\": False,\n",
        "        \"axes.spines.right\": False,\n",
        "    })\n",
        "\n",
        "    N_REFS = [25, 50, 75, 100, 150, 200, 300, 400, 500,\n",
        "              600, 800, 1000, 1500, 2000, 2500, 3000, 4000, 5000]\n",
        "\n",
        "\n",
        "    def pick_key(d, must_contain, exclude=None):\n",
        "        \"\"\"\n",
        "        Find first key in d that contains all substrings in must_contain\n",
        "        and does not contain any substring in exclude (case-insensitive).\n",
        "        \"\"\"\n",
        "        if exclude is None:\n",
        "            exclude = []\n",
        "        must_contain = [s.lower() for s in must_contain]\n",
        "        exclude      = [s.lower() for s in exclude]\n",
        "\n",
        "        for k in d.keys():\n",
        "            kl = k.lower()\n",
        "            if all(s in kl for s in must_contain) and all(e not in kl for e in exclude):\n",
        "                return k\n",
        "        raise KeyError(f\"No key found in {list(d.keys())} containing {must_contain} and excluding {exclude}\")\n",
        "\n",
        "\n",
        "    blend_divs       = []\n",
        "    blend_proxy_divs = []\n",
        "    tweedie_divs     = []\n",
        "    csem_divs        = []\n",
        "    floors           = []\n",
        "\n",
        "    for N_REF in N_REFS:\n",
        "        print(f\"\\n=== KSD sweep: N_ref = {N_REF} ===\")\n",
        "        res   = main1(\n",
        "            N_REF      = N_REF,\n",
        "            plot_hists = False,\n",
        "            do_inv     = False,\n",
        "            rand_seed  = True,\n",
        "            div        = 'M_KSD',\n",
        "            save_tag   = f'N_REF_{N_REF}',\n",
        "            ridge_frac = 1e-6,\n",
        "            trials     = 1,\n",
        "        )\n",
        "        ddivs = res['divs_dict']\n",
        "\n",
        "        k_blend_true  = pick_key(ddivs, [\"blend\"], exclude=[\"proxy\"])\n",
        "        k_blend_proxy = pick_key(ddivs, [\"blend\", \"proxy\",\"svd\"])\n",
        "        k_tweedie     = pick_key(ddivs, [\"tweedie\"])\n",
        "        k_csem        = pick_key(ddivs, [\"csem\"])\n",
        "\n",
        "        blend_divs.append(      float(np.asarray(ddivs[k_blend_true]) [-1]))\n",
        "        blend_proxy_divs.append(float(np.asarray(ddivs[k_blend_proxy])[-1]))\n",
        "        tweedie_divs.append(    float(np.asarray(ddivs[k_tweedie])    [-1]))\n",
        "        csem_divs.append(       float(np.asarray(ddivs[k_csem])       [-1]))\n",
        "        floors.append(          float(ddivs[\"floor\"]))\n",
        "\n",
        "    floor_mean  = float(np.mean(floors))\n",
        "    floors_plot = [floor_mean] * len(N_REFS)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(3.6, 2.7))\n",
        "\n",
        "    ax.plot(N_REFS, blend_divs,\n",
        "            color=\"tab:blue\", lw=1.5, label=\"Blend (ground-truth $s_0$)\")\n",
        "    ax.plot(N_REFS, blend_proxy_divs,\n",
        "            color=\"tab:blue\", lw=1.5, ls=\"--\", label=\"Blend (diagonal proxy $\\\\hat s_0$)\")\n",
        "    ax.plot(N_REFS, tweedie_divs,\n",
        "            color=\"tab:red\", lw=1.5, label=\"Tweedie\")\n",
        "    ax.plot(N_REFS, csem_divs,\n",
        "            color=\"tab:green\", lw=1.5, label=\"CSEM\")\n",
        "    ax.plot(N_REFS, floors_plot,\n",
        "            color=\"black\", lw=1.2, ls=\"--\", label=\"Ground-truth floor\")\n",
        "\n",
        "    ax.set_xscale(\"log\")\n",
        "    ax.set_yscale(\"log\")\n",
        "    ax.set_xlabel(r\"$N_{\\text{ref}}$\")\n",
        "    ax.set_ylabel(\"Multiscale KSD\")\n",
        "\n",
        "    ax.grid(ls=\"--\", alpha=0.4)\n",
        "    ax.legend(\n",
        "        loc=\"upper center\",\n",
        "        bbox_to_anchor=(0.5, 1.18),\n",
        "        ncol=2,\n",
        "        fontsize=8,\n",
        "        frameon=False,\n",
        "    )\n",
        "\n",
        "    fig.tight_layout(rect=[0.0, 0.0, 1.0, 0.95])\n",
        "    fig.savefig(\"figs/blend_ksd_vs_Nref.png\", dpi=300)\n",
        "    print(\"Saved figs/blend_ksd_vs_Nref.png\")\n"
      ],
      "metadata": {
        "id": "usMfYRGS0ZO0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b631da47-01c6-4492-d3f5-d891472f108a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== KSD sweep: N_ref = 25 ===\n",
            "N_REF_25, 10 steps, Blend score: M_KSD =7.401e-01, M_KSD_floor = 1.279e-01, score_RMSE=7.458e+01, time=4.3s\n",
            " \n",
            "N_REF_25, 10 steps, Tweedie score: M_KSD =1.113e+01, M_KSD_floor = 1.281e-01, score_RMSE=2.481e+01, time=4.2s\n",
            " \n",
            "N_REF_25, 10 steps, CSEM score: M_KSD =2.328e+02, M_KSD_floor = 1.280e-01, score_RMSE=3.008e+02, time=4.2s\n",
            " \n",
            "N_REF_25, 10 steps, Blend score( Proxy ): M_KSD =2.881e+00, M_KSD_floor = 1.273e-01, score_RMSE=1.210e+02, time=4.4s\n",
            " \n",
            "N_REF_25, 10 steps, Blend score( Proxy SVD recomp ): M_KSD =2.085e+00, M_KSD_floor = 1.267e-01, score_RMSE=7.341e+01, time=4.4s\n",
            " \n",
            "\n",
            "=== KSD sweep: N_ref = 50 ===\n",
            "N_REF_50, 10 steps, Blend score: M_KSD =3.213e-01, M_KSD_floor = 1.260e-01, score_RMSE=6.094e+01, time=4.4s\n",
            " \n",
            "N_REF_50, 10 steps, Tweedie score: M_KSD =5.047e+00, M_KSD_floor = 1.254e-01, score_RMSE=2.488e+01, time=4.3s\n",
            " \n",
            "N_REF_50, 10 steps, CSEM score: M_KSD =9.229e+01, M_KSD_floor = 1.266e-01, score_RMSE=1.836e+02, time=4.2s\n",
            " \n",
            "N_REF_50, 10 steps, Blend score( Proxy ): M_KSD =1.551e+00, M_KSD_floor = 1.265e-01, score_RMSE=1.088e+02, time=7.2s\n",
            " \n",
            "N_REF_50, 10 steps, Blend score( Proxy SVD recomp ): M_KSD =6.536e-01, M_KSD_floor = 1.259e-01, score_RMSE=6.052e+01, time=4.5s\n",
            " \n",
            "\n",
            "=== KSD sweep: N_ref = 75 ===\n",
            "N_REF_75, 10 steps, Blend score: M_KSD =2.490e-01, M_KSD_floor = 1.316e-01, score_RMSE=5.290e+01, time=4.4s\n",
            " \n",
            "N_REF_75, 10 steps, Tweedie score: M_KSD =3.546e+00, M_KSD_floor = 1.283e-01, score_RMSE=2.494e+01, time=4.3s\n",
            " \n",
            "N_REF_75, 10 steps, CSEM score: M_KSD =3.268e+01, M_KSD_floor = 1.280e-01, score_RMSE=1.043e+02, time=4.3s\n",
            " \n",
            "N_REF_75, 10 steps, Blend score( Proxy ): M_KSD =1.330e+00, M_KSD_floor = 1.273e-01, score_RMSE=8.517e+01, time=4.4s\n",
            " \n",
            "N_REF_75, 10 steps, Blend score( Proxy SVD recomp ): M_KSD =5.529e-01, M_KSD_floor = 1.275e-01, score_RMSE=5.323e+01, time=4.7s\n",
            " \n",
            "\n",
            "=== KSD sweep: N_ref = 100 ===\n",
            "N_REF_100, 10 steps, Blend score: M_KSD =2.268e-01, M_KSD_floor = 1.238e-01, score_RMSE=4.656e+01, time=4.5s\n",
            " \n",
            "N_REF_100, 10 steps, Tweedie score: M_KSD =2.626e+00, M_KSD_floor = 1.246e-01, score_RMSE=2.490e+01, time=4.3s\n",
            " \n",
            "N_REF_100, 10 steps, CSEM score: M_KSD =4.039e+01, M_KSD_floor = 1.241e-01, score_RMSE=1.113e+02, time=4.3s\n",
            " \n",
            "N_REF_100, 10 steps, Blend score( Proxy ): M_KSD =7.377e-01, M_KSD_floor = 1.247e-01, score_RMSE=6.789e+01, time=4.4s\n",
            " \n",
            "N_REF_100, 10 steps, Blend score( Proxy SVD recomp ): M_KSD =4.266e-01, M_KSD_floor = 1.251e-01, score_RMSE=4.929e+01, time=4.8s\n",
            " \n",
            "\n",
            "=== KSD sweep: N_ref = 150 ===\n",
            "N_REF_150, 10 steps, Blend score: M_KSD =1.733e-01, M_KSD_floor = 1.253e-01, score_RMSE=4.535e+01, time=7.4s\n",
            " \n",
            "N_REF_150, 10 steps, Tweedie score: M_KSD =1.796e+00, M_KSD_floor = 1.258e-01, score_RMSE=2.506e+01, time=4.4s\n",
            " \n",
            "N_REF_150, 10 steps, CSEM score: M_KSD =7.245e+00, M_KSD_floor = 1.257e-01, score_RMSE=4.127e+01, time=4.4s\n",
            " \n",
            "N_REF_150, 10 steps, Blend score( Proxy ): M_KSD =5.308e-01, M_KSD_floor = 1.255e-01, score_RMSE=5.580e+01, time=4.6s\n",
            " \n",
            "N_REF_150, 10 steps, Blend score( Proxy SVD recomp ): M_KSD =3.980e-01, M_KSD_floor = 1.271e-01, score_RMSE=4.339e+01, time=5.1s\n",
            " \n",
            "\n",
            "=== KSD sweep: N_ref = 200 ===\n",
            "N_REF_200, 10 steps, Blend score: M_KSD =1.734e-01, M_KSD_floor = 1.247e-01, score_RMSE=4.206e+01, time=4.7s\n",
            " \n",
            "N_REF_200, 10 steps, Tweedie score: M_KSD =1.465e+00, M_KSD_floor = 1.274e-01, score_RMSE=2.490e+01, time=4.5s\n",
            " \n",
            "N_REF_200, 10 steps, CSEM score: M_KSD =1.181e+01, M_KSD_floor = 1.275e-01, score_RMSE=3.236e+01, time=4.5s\n",
            " \n",
            "N_REF_200, 10 steps, Blend score( Proxy ): M_KSD =3.999e-01, M_KSD_floor = 1.271e-01, score_RMSE=4.861e+01, time=5.2s\n",
            " \n",
            "N_REF_200, 10 steps, Blend score( Proxy SVD recomp ): M_KSD =3.553e-01, M_KSD_floor = 1.271e-01, score_RMSE=4.041e+01, time=5.4s\n",
            " \n",
            "\n",
            "=== KSD sweep: N_ref = 300 ===\n",
            "N_REF_300, 10 steps, Blend score: M_KSD =1.437e-01, M_KSD_floor = 1.262e-01, score_RMSE=3.585e+01, time=7.8s\n",
            " \n",
            "N_REF_300, 10 steps, Tweedie score: M_KSD =9.841e-01, M_KSD_floor = 1.266e-01, score_RMSE=2.530e+01, time=4.6s\n",
            " \n",
            "N_REF_300, 10 steps, CSEM score: M_KSD =2.539e+00, M_KSD_floor = 1.270e-01, score_RMSE=1.047e+01, time=4.6s\n",
            " \n",
            "N_REF_300, 10 steps, Blend score( Proxy ): M_KSD =2.770e-01, M_KSD_floor = 1.268e-01, score_RMSE=4.018e+01, time=5.0s\n",
            " \n",
            "N_REF_300, 10 steps, Blend score( Proxy SVD recomp ): M_KSD =2.619e-01, M_KSD_floor = 1.268e-01, score_RMSE=3.563e+01, time=6.0s\n",
            " \n",
            "\n",
            "=== KSD sweep: N_ref = 400 ===\n",
            "N_REF_400, 10 steps, Blend score: M_KSD =1.395e-01, M_KSD_floor = 1.256e-01, score_RMSE=3.447e+01, time=5.2s\n",
            " \n",
            "N_REF_400, 10 steps, Tweedie score: M_KSD =7.567e-01, M_KSD_floor = 1.278e-01, score_RMSE=2.522e+01, time=4.7s\n",
            " \n",
            "N_REF_400, 10 steps, CSEM score: M_KSD =3.695e+00, M_KSD_floor = 1.285e-01, score_RMSE=1.679e+01, time=4.8s\n",
            " \n",
            "N_REF_400, 10 steps, Blend score( Proxy ): M_KSD =2.435e-01, M_KSD_floor = 1.283e-01, score_RMSE=3.395e+01, time=5.2s\n",
            " \n",
            "N_REF_400, 10 steps, Blend score( Proxy SVD recomp ): M_KSD =2.506e-01, M_KSD_floor = 1.278e-01, score_RMSE=3.190e+01, time=6.6s\n",
            " \n",
            "\n",
            "=== KSD sweep: N_ref = 500 ===\n",
            "N_REF_500, 10 steps, Blend score: M_KSD =1.382e-01, M_KSD_floor = 1.241e-01, score_RMSE=3.290e+01, time=5.5s\n",
            " \n",
            "N_REF_500, 10 steps, Tweedie score: M_KSD =6.240e-01, M_KSD_floor = 1.240e-01, score_RMSE=2.525e+01, time=4.9s\n",
            " \n",
            "N_REF_500, 10 steps, CSEM score: M_KSD =1.239e+00, M_KSD_floor = 1.236e-01, score_RMSE=8.068e+00, time=4.9s\n",
            " \n",
            "N_REF_500, 10 steps, Blend score( Proxy ): M_KSD =2.104e-01, M_KSD_floor = 1.249e-01, score_RMSE=3.135e+01, time=5.5s\n",
            " \n",
            "N_REF_500, 10 steps, Blend score( Proxy SVD recomp ): M_KSD =2.100e-01, M_KSD_floor = 1.251e-01, score_RMSE=2.973e+01, time=7.2s\n",
            " \n",
            "\n",
            "=== KSD sweep: N_ref = 600 ===\n",
            "N_REF_600, 10 steps, Blend score: M_KSD =1.352e-01, M_KSD_floor = 1.250e-01, score_RMSE=3.318e+01, time=8.6s\n",
            " \n",
            "N_REF_600, 10 steps, Tweedie score: M_KSD =5.532e-01, M_KSD_floor = 1.249e-01, score_RMSE=2.539e+01, time=5.0s\n",
            " \n",
            "N_REF_600, 10 steps, CSEM score: M_KSD =9.279e-01, M_KSD_floor = 1.249e-01, score_RMSE=6.741e+00, time=5.1s\n",
            " \n",
            "N_REF_600, 10 steps, Blend score( Proxy ): M_KSD =2.084e-01, M_KSD_floor = 1.253e-01, score_RMSE=3.109e+01, time=5.7s\n",
            " \n",
            "N_REF_600, 10 steps, Blend score( Proxy SVD recomp ): M_KSD =2.060e-01, M_KSD_floor = 1.251e-01, score_RMSE=2.648e+01, time=7.8s\n",
            " \n",
            "\n",
            "=== KSD sweep: N_ref = 800 ===\n",
            "N_REF_800, 10 steps, Blend score: M_KSD =1.347e-01, M_KSD_floor = 1.276e-01, score_RMSE=2.644e+01, time=6.3s\n",
            " \n",
            "N_REF_800, 10 steps, Tweedie score: M_KSD =4.434e-01, M_KSD_floor = 1.263e-01, score_RMSE=2.508e+01, time=5.3s\n",
            " \n",
            "N_REF_800, 10 steps, CSEM score: M_KSD =1.125e+00, M_KSD_floor = 1.258e-01, score_RMSE=7.009e+00, time=5.4s\n",
            " \n",
            "N_REF_800, 10 steps, Blend score( Proxy ): M_KSD =1.870e-01, M_KSD_floor = 1.246e-01, score_RMSE=2.554e+01, time=6.3s\n",
            " \n",
            "N_REF_800, 10 steps, Blend score( Proxy SVD recomp ): M_KSD =1.889e-01, M_KSD_floor = 1.250e-01, score_RMSE=2.590e+01, time=9.0s\n",
            " \n",
            "\n",
            "=== KSD sweep: N_ref = 1000 ===\n",
            "N_REF_1000, 10 steps, Blend score: M_KSD =1.309e-01, M_KSD_floor = 1.251e-01, score_RMSE=2.432e+01, time=6.8s\n",
            " \n",
            "N_REF_1000, 10 steps, Tweedie score: M_KSD =3.897e-01, M_KSD_floor = 1.255e-01, score_RMSE=2.537e+01, time=5.6s\n",
            " \n",
            "N_REF_1000, 10 steps, CSEM score: M_KSD =2.316e+00, M_KSD_floor = 1.246e-01, score_RMSE=8.125e+00, time=5.6s\n",
            " \n",
            "N_REF_1000, 10 steps, Blend score( Proxy ): M_KSD =1.780e-01, M_KSD_floor = 1.253e-01, score_RMSE=2.412e+01, time=6.7s\n",
            " \n",
            "N_REF_1000, 10 steps, Blend score( Proxy SVD recomp ): M_KSD =1.900e-01, M_KSD_floor = 1.257e-01, score_RMSE=2.293e+01, time=10.1s\n",
            " \n",
            "\n",
            "=== KSD sweep: N_ref = 1500 ===\n",
            "N_REF_1500, 10 steps, Blend score: M_KSD =1.317e-01, M_KSD_floor = 1.251e-01, score_RMSE=2.073e+01, time=7.9s\n",
            " \n",
            "N_REF_1500, 10 steps, Tweedie score: M_KSD =2.949e-01, M_KSD_floor = 1.244e-01, score_RMSE=2.538e+01, time=6.2s\n",
            " \n",
            "N_REF_1500, 10 steps, CSEM score: M_KSD =3.529e-01, M_KSD_floor = 1.247e-01, score_RMSE=4.038e+00, time=6.2s\n",
            " \n",
            "N_REF_1500, 10 steps, Blend score( Proxy ): M_KSD =1.614e-01, M_KSD_floor = 1.252e-01, score_RMSE=2.004e+01, time=7.8s\n",
            " \n",
            "N_REF_1500, 10 steps, Blend score( Proxy SVD recomp ): M_KSD =1.734e-01, M_KSD_floor = 1.262e-01, score_RMSE=1.847e+01, time=12.9s\n",
            " \n",
            "\n",
            "=== KSD sweep: N_ref = 2000 ===\n",
            "N_REF_2000, 10 steps, Blend score: M_KSD =1.276e-01, M_KSD_floor = 1.264e-01, score_RMSE=1.968e+01, time=9.0s\n",
            " \n",
            "N_REF_2000, 10 steps, Tweedie score: M_KSD =2.599e-01, M_KSD_floor = 1.253e-01, score_RMSE=2.552e+01, time=6.7s\n",
            " \n",
            "N_REF_2000, 10 steps, CSEM score: M_KSD =2.859e-01, M_KSD_floor = 1.256e-01, score_RMSE=3.342e+00, time=6.7s\n",
            " \n",
            "N_REF_2000, 10 steps, Blend score( Proxy ): M_KSD =1.602e-01, M_KSD_floor = 1.254e-01, score_RMSE=1.812e+01, time=8.9s\n",
            " \n",
            "N_REF_2000, 10 steps, Blend score( Proxy SVD recomp ): M_KSD =1.574e-01, M_KSD_floor = 1.251e-01, score_RMSE=1.786e+01, time=15.7s\n",
            " \n",
            "\n",
            "=== KSD sweep: N_ref = 2500 ===\n",
            "N_REF_2500, 10 steps, Blend score: M_KSD =1.272e-01, M_KSD_floor = 1.284e-01, score_RMSE=1.581e+01, time=10.1s\n",
            " \n",
            "N_REF_2500, 10 steps, Tweedie score: M_KSD =2.286e-01, M_KSD_floor = 1.278e-01, score_RMSE=2.585e+01, time=7.2s\n",
            " \n",
            "N_REF_2500, 10 steps, CSEM score: M_KSD =1.894e-01, M_KSD_floor = 1.258e-01, score_RMSE=3.165e+00, time=7.2s\n",
            " \n",
            "N_REF_2500, 10 steps, Blend score( Proxy ): M_KSD =1.549e-01, M_KSD_floor = 1.256e-01, score_RMSE=1.665e+01, time=10.0s\n",
            " \n",
            "N_REF_2500, 10 steps, Blend score( Proxy SVD recomp ): M_KSD =1.585e-01, M_KSD_floor = 1.254e-01, score_RMSE=1.699e+01, time=18.5s\n",
            " \n",
            "\n",
            "=== KSD sweep: N_ref = 3000 ===\n",
            "N_REF_3000, 10 steps, Blend score: M_KSD =1.298e-01, M_KSD_floor = 1.245e-01, score_RMSE=1.564e+01, time=11.1s\n",
            " \n",
            "N_REF_3000, 10 steps, Tweedie score: M_KSD =2.127e-01, M_KSD_floor = 1.250e-01, score_RMSE=2.534e+01, time=7.7s\n",
            " \n",
            "N_REF_3000, 10 steps, CSEM score: M_KSD =2.339e-01, M_KSD_floor = 1.252e-01, score_RMSE=2.970e+00, time=7.7s\n",
            " \n",
            "N_REF_3000, 10 steps, Blend score( Proxy ): M_KSD =1.572e-01, M_KSD_floor = 1.255e-01, score_RMSE=1.445e+01, time=11.0s\n",
            " \n",
            "N_REF_3000, 10 steps, Blend score( Proxy SVD recomp ): M_KSD =1.551e-01, M_KSD_floor = 1.252e-01, score_RMSE=1.442e+01, time=21.2s\n",
            " \n",
            "\n",
            "=== KSD sweep: N_ref = 4000 ===\n",
            "N_REF_4000, 10 steps, Blend score: M_KSD =1.301e-01, M_KSD_floor = 1.236e-01, score_RMSE=1.357e+01, time=13.2s\n",
            " \n",
            "N_REF_4000, 10 steps, Tweedie score: M_KSD =1.903e-01, M_KSD_floor = 1.252e-01, score_RMSE=2.532e+01, time=8.5s\n",
            " \n",
            "N_REF_4000, 10 steps, CSEM score: M_KSD =1.767e-01, M_KSD_floor = 1.257e-01, score_RMSE=2.682e+00, time=8.6s\n",
            " \n",
            "N_REF_4000, 10 steps, Blend score( Proxy ): M_KSD =1.545e-01, M_KSD_floor = 1.258e-01, score_RMSE=1.244e+01, time=13.0s\n",
            " \n",
            "N_REF_4000, 10 steps, Blend score( Proxy SVD recomp ): M_KSD =1.530e-01, M_KSD_floor = 1.256e-01, score_RMSE=1.125e+01, time=26.6s\n",
            " \n",
            "\n",
            "=== KSD sweep: N_ref = 5000 ===\n",
            "N_REF_5000, 10 steps, Blend score: M_KSD =1.269e-01, M_KSD_floor = 1.261e-01, score_RMSE=1.265e+01, time=15.1s\n",
            " \n",
            "N_REF_5000, 10 steps, Tweedie score: M_KSD =1.815e-01, M_KSD_floor = 1.253e-01, score_RMSE=2.535e+01, time=9.3s\n",
            " \n",
            "N_REF_5000, 10 steps, CSEM score: M_KSD =1.749e-01, M_KSD_floor = 1.250e-01, score_RMSE=2.492e+00, time=9.3s\n",
            " \n",
            "N_REF_5000, 10 steps, Blend score( Proxy ): M_KSD =1.446e-01, M_KSD_floor = 1.255e-01, score_RMSE=1.144e+01, time=14.9s\n",
            " \n",
            "N_REF_5000, 10 steps, Blend score( Proxy SVD recomp ): M_KSD =1.541e-01, M_KSD_floor = 1.249e-01, score_RMSE=1.088e+01, time=32.0s\n",
            " \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'figs/blend_ksd_vs_Nref.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1568996329.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.95\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"figs/blend_ksd_vs_Nref.png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saved figs/blend_ksd_vs_Nref.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3488\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3489\u001b[0m                     \u001b[0m_recursively_make_axes_transparent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3490\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3492\u001b[0m     def ginput(self, n=1, timeout=30, show_clicks=True,\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2182\u001b[0m                 \u001b[0;31m# force the figure dpi to 72), so we need to set it again here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2183\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2184\u001b[0;31m                     result = print_method(\n\u001b[0m\u001b[1;32m   2185\u001b[0m                         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2186\u001b[0m                         \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2038\u001b[0m                 \"bbox_inches_restore\"}\n\u001b[1;32m   2039\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptional_kws\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2040\u001b[0;31m             print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(\n\u001b[0m\u001b[1;32m   2041\u001b[0m                 *args, **{k: v for k, v in kwargs.items() if k not in skip}))\n\u001b[1;32m   2042\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Let third-parties do as they see fit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincluding\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m'Software'\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \"\"\"\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m_print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \"\"\"\n\u001b[1;32m    429\u001b[0m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         mpl.image.imsave(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             dpi=self.figure.dpi, metadata=metadata, pil_kwargs=pil_kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimsave\u001b[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dpi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1634\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpil_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2581\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2583\u001b[0;31m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2584\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2585\u001b[0m             \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'figs/blend_ksd_vs_Nref.png'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x270 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAADvCAYAAAAEnOQjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAYTBJREFUeJztnXd4VEXXwH93d9N7IxBSaKFJCR0joSNVpGMBFbs0BWyvoKAfgu+rgqKgiAIWUIpSRJAeCF1KpENCDYRAei9b5vsjZMmSQhI2ySbM73n2SXbu3Dtn7uyeO3vmzDmKEEIgkUgkkiqJqrIFkEgkEknZkUpcIpFIqjBSiUskEkkVRipxiUQiqcJIJS6RSCRVGKnEJRKJpAojlbhEIpFUYaQSl0gkkiqMVOISiURShZFKXCKRSKowUolLJBJJFUYqcYlEIqnCSCUukUgkVRipxCUSiaQKI5W4RCKRVGEeOCVep04dGjVqRFBQEE2aNOGpp54iPT0dAEVRSEpKMmt7np6eXL58ucjjw4cPZ//+/WZt05wUJ/+MGTPIysoq87XvPr887j9ASEgIly5dMvt1qzo6nY4PP/yQxo0b06xZM4KCgnj55ZdJSkrio48+olmzZrRs2ZLGjRvz1ltvGc9TFIXmzZsTFBRkfMXHxxuPBQUFmbSzZMkSFEXhiy++qMDePUCIB4yAgABx7NgxIYQQer1e9OvXT3z99ddCCCEAkZiYaNb2PDw8xKVLlwo9dvDgQdG9e/dSX1Or1d6nVCWnOPmLu18lkfHu88vj/gshxJo1a8To0aPNft2qzjPPPCMGDBggEhIShBBCGAwGsXLlSvHf//5XdOzYUWRkZAghcscyPDzceF5x4wSINm3aiMOHDxvLOnXqJNq2bSvmzp1bbn15kKmQmbgQgowcXbm+RBkSFOXk5JCRkYGbm1uBY//88w/du3enbdu2tGrVilWrVhmPKYrCrFmzaN++PXXr1mXJkiXGY+vXr6dJkya0aNGCt99+u9j2Fy5cyFNPPWV8v27dOpo0aULLli155513TGbBiqIwffp02rVrx3/+8x8ANm/eTOvWrWnRogVdunTh9OnTxrr5Z7R3z6bNIf+rr74K5M5yg4KCuHXrVqEyFiVLYecDLFiwoFC57mb79u0EBwfTunVrGjZsyNKlS5kwYQIhISHMmjXLpG7//v3ZtGkTycnJRV6vIhFCkKHNKNfXvb4PkZGRrFq1iiVLlhg//4qiMHz4cKytrXF3d8fW1hYAjUZDy5YtS9y/MWPGsHjxYgDOnz+PVqvloYceKuPdktwLTUU0kqnV0/SDzeXaxumPemNvXbLujBw5Ejs7Oy5fvkybNm0YMWKEyfGkpCRefvllNm7cSK1atYiLi6N169YEBwdTu3ZtAGxsbDh06BBnz56lXbt2jB49moSEBMaMGUNYWBhNmzblu+++M/7MLIzQ0FAmTZoEwK1bt3j++efZu3cvjRs3ZsmSJQXOVavV/PPPP8b6Tz31FKGhoTRv3pxly5YxbNgwTp06VaJ7cL/yf/vttyxcuJCwsDBcXV0LlbE4ijq/MLk0GtNxFULw1FNPER4eTq1atTAYDBw9epR///2XsLAwhg0bxs2bN/H29gbAysqK5s2bExYWxoABA0p0f8qTTF0mHZZ3KNc2Dj51EHsr+yKPHz16lMDAQDw9PQsce+KJJ/juu++oV68eISEhdO3alSeffBI7OztjnZCQENRqNQBubm7s3LnTeGzIkCF89tlnZGVlsXjxYsaMGWPRJsOqzgNnEwdYsWIF4eHhxMXFUadOHd555x2T4/v27ePixYv07duXoKAgevbsCcC5c+eMdZ5++mkAGjdujEajISYmhgMHDtCiRQuaNm0KwAsvvIC1tXWRcly7ds2oaPLObdy4MQDPPvtsgXOff/554/8HDx6kefPmNG/e3ChPdHQ0169fL9E9MIf8hZFfxrJQmFyFUbNmTcaPH8/q1avJysri4MGDdO/eHYAuXbpw5MiRAvWvXbt2X7I9KNSsWZMTJ06wbNkymjdvzoIFCwgODiYnJ8dYJywsjPDwcMLDw00UOICdnR29e/dm1apVrFq1iieffLKiu/BAUSEzcTsrNac/6l3ubZQWjUbD0KFDeeutt/j888+N5UIIHnroIfbt21fkuXk/NSF39qnT6QrUURSl2Pbt7e1LtTDo6OhYonpqtRq9Xm98X1gbpZX/p59+Ys6cOQC8/vrrjBkzpkQylkSWssh1+PBhdu3axcqVK5kyZQrPP/88zZo1A8DJyanAAmlWVpbJTLIysdPYcfCpg+XeRnG0bt2aiIgI4uPj8fDwKHBcrVYTHBxMcHAwEydOxNvbm5MnT9K6desStT9mzBgGDBhAnz59cHZ2LlMfJCWjQmbiiqJgb60p19e9FGZR7Nixg0aNGpmUBQcHc+nSJbZt22YsCw8PN5mJFMbDDz/M8ePHOXv2LACLFy8u9pwWLVoYZ/cdO3bk+PHjxve//PJLsed27NiREydOcPLkSQB+++03ateuTe3atWnQoAEHD+YqiT/++MPofXMvipP/mWeeMc688hS4k5PTPe3MxclSkvML4+zZs6jVanr27Ml//vMfMjMz8fT0JCUlBYDU1FQTEw3AmTNnSmXXLU8URcHeyr5cX/f6PjRo0IChQ4fywgsvGB94Qgh+//13Dh06xIULF4x1z549i1arxc/Pr8R97NChA9OmTTOujUjKjwqZiVsaeTZxnU5HQEAA3377rclxNzc3/vrrL958802mTJmCVqvF39+ftWvXFntdLy8vFi9ezODBg7G2tqZPnz6FznLyGDZsGJs3b6Znz57UqFGD77//nkGDBmFjY0OvXr1wdHQsoIzyt7Vs2TKeeeYZdDodbm5urFq1CkVRmDt3LhMnTmTatGn079+/WBnuR/4pU6bQq1cv7O3t2bJlS6F1ipOlJOcXdc2dO3fi4OCAvb09K1euxMHBgV9//ZXHHnuM3bt3m6xzXL58Gb1ebzFK3FJYvHgxM2fOpEOHDmg0GgwGA507d6Z79+5MnDiRpKQk7OzsUKvVLF++HC8vL+O5+W3ikGuivHsy9Prrr1dYXx5kFFEWtw6JWUhLSyM4OJj9+/fj4OBAamoqTk5OAKxdu5b//Oc/nDlzppKlrDqMGzeO48eP07t3b6ZNm2Ysf/fdd2nQoAEvvvhiJUonkZQPD+RM3FJwdHRk7ty5XLp0iWbNmvHVV1+xYsUK9Ho9zs7OLFu2rLJFrFLMnz+/0HIfH5/7XnCVSCwVOROXSCSSKswD6WIokUgk1QWpxCUSiaQKI5W4RCKRVGGkEs+HEIKUlJQyxWGRSCSSykAq8Xykpqbi4uJCampqZYtiUQghSEpKkg+3SkSOgaQopBKX3BOpQCqf8hgDrVZrjCf+0EMP0apVKwYNGkR4eLjZ2igLxcWw/+KLL4qMp1MSli5datyRnPd+0KBBJTr3woULtG7dmlatWrFkyRK6du16zw2AFYFU4hLJA8qYMWM4duwY+/fv59SpUxw7dozx48ebBHrLT/4YOJVFcUrcYDBgMBiKPf9uJV4aVq9eTbt27Th27FiRsYPuh8LiBJUEqcTJ3STStGlT2rVrV9miSCQVQkREBGvWrGHx4sUm8fR79uzJyJEjgVyF161bN4YOHUrz5s05dOhQkTHsQ0NDTTL6nDx5kjp16gC5YQ9cXV2ZPn06bdq0oUGDBmzcuNFYt6Qx7D/66COio6MZOXIkQUFBhIeHM2PGDIYOHUrv3r1p1qwZN27coE6dOia/Jtq2bUtoaCjff/89hw8fZtKkSQQFBRllSEtL48knn6R58+a0bduWixcvFmj7p59+Yu7cufzxxx8EBQUZ+53HrVu3GDJkCM2bN6dZs2YsXLjQeOzw4cMEBwfTokUL2rdvz969e03uyzvvvEPr1q35+uuvixuyoqnQFBQWTnJysgBEcnJyZYtiURgMBpGRkSEMBkNli1Jmpk6dKpo1a1YumYPKStOmTQt9RURECCGEiIiIMClv0qSJ8f88/v777wLnl4QVK1aIFi1aFFtnyZIlws7OTpw9e1YIIcTNmzeFu7u7OH78uBBCiF9++UU0adJEGAwGsXPnTtGyZUvjuSdOnBABAQFCCCEuXbokALF69WohhBCbNm0SDRs2NLnmqVOnhBBCLFy4UABFZpPKn5lLCCGmT58uatWqJWJiYoqs06ZNG7Fz504hhBBdunQRa9asMemjs7OzuHjxohBCiHfeeUe8/PLLhbY9ffp08frrrxvf57/WiBEjxLvvvmvsk6+vr9i/f7/Izs4Wfn5+4u+//xZCCBEWFia8vb1Famqq8b78+OOPhbZXUuRM/D5I15YsOmBVR1EU7OzsyhwpsrI5cuQIcXFxLFmyhHfffbeyxSkz5Xn/L1y4QFBQEI0aNTIxFQQHBxsDW91PDHtbW1uGDBkC5EbLzIuSaI4Y9v369TPG5S8LDz/8MHXr1i0gW2nYtm0br7zyCgA1atRgyJAhbNu2jXPnzqFSqejdOzcUd6dOnfD29jb+UrCysmLUqFFllh0ewNgpderUwcbGBjs7O7Kzs2nVqhWLFi3CwcEBFxeXEl9nX/Q+3tr1FmODxjKi0QisVFaF1vP09OTw4cPGn5Z3M3z4cCZPnszDDz9c7LlBQUGEhYUZA2RVJAaDgaioKPz8/FCpTJ/7iqKQmJhYZLRFS6BNmza0adMGyP1pfS9CQkL46aefjF/s8uJeWZgaNGhgrFPUGPTu3bvE2Zzy06pVKyIjI0lMTMTNzY369esTHh7O0qVLTRbrShrDXqPRFBs33sbGxvgQujvGfH7yP6i2bdvGm2++CeR+T6ZOnVroOXfLeC9Z7qYkMexLS3EP3PzH7O3tC3ynSssDORPPy+xz6tQpkpOTWbp0aamvsSZiDSk5KXxy6BOGrx/OvuiiE0gUxaFDh0hISChUgd9NeHh4pSjwPEQ5eaaY4wtjbqZMmcL06dMrW4wCmHMMAgMDefzxx03iiQPFxp4vLoZ9vXr1uHLlCrGxsQD8/PPPJZKjuBj2PXv2NMawz1Pgzs7OpYphf+jQIZOF2pKcXxZ69uzJokWLAIiNjeWPP/6gV69eNGrUCIPBwNatW4HcrGExMTEm6wf3ywOpxPO4n0TJLntcyPw8k4i3Iji84TCvbH2FCdsn8P1v35c5UXJxCzz5Ew4//fTTtG3blhYtWtC/f3+T1friki0XtSiVd/2ikiePGjWKgQMHEhQUVKC9olAUhWnTptGqVSsaNmxoEpGxNEmfz507h6+vr3Gx6bPPPqNPnz7873//4+WXXzZeMykpCU9PTxISEgqVp7DEykVhaYmVy4ulS5fSvHlzOnTowEMPPUSnTp3Ytm1bgXSFeeSPYd+iRQu++eYbYwx7Hx8f3n77bdq3b0/Hjh1xd3cvkQz5Y9i3bNmSiIiIYmPYT5w4kZdeesm4sFkYM2fOZP78+bRs2ZLFixebJGl++eWXmTVrlsnCpjmYN28eZ86coXnz5nTr1o2pU6fSoUMHrK2t+eOPP5g+fTotWrTgjTfeYPXq1SX+hVMi7suiXkrSs7VFvjJzdGWuWxoCAgJEw4YNRcuWLYWLi4vo3r270Gq1QgghAAGIK1euiKCgIBEdHS2EECI2Nlb4+fmJa9euGet99tlnQgghDv17SFjbW4uWi1uKxvMaC7WDWry14i2Rmp16z0WaevXqiRMnTggh7r3AAxgX5W7dumW8xuzZs8Urr7xico0zZ84IIYRYvHix8RrFLUrd3aczZ84IR0dH432JiYkRly5dEnq93qS9u+XKDyCmTZsmhBDiwoULws3NzaQvH374obHuvWRbvny5cXGqTp064tatWyIxMVF4eXkZ254zZ454/vnnC73PBoNB1KhRwzieer1eJCYmivHjx4tOnTqJjz/+uMA53bp1E3/++Weh16sM9Hq9cQwkkvxUqE28uIz33Rp5sWRMe+P7Nv+3jUxt4XazDnXdWfHKvU0QRbFixQqCgoLQ6XS88sor9O7dmxs3bhiPHzp0yJgoOT/nzp0zZrvPS+jbrkU77K3tmd9hPv+3+v+I8otiU+YmDq45yPjO40udKDn/As+ECRMKPW/58uX8/PPPZGVlkZWVZcxYXliy5VdffRUofFFq3LhxXL9+HV9fX5M+5U9S7Ovry6+//srPP/9Mdna2SXv3Ii8JQ7169ejcuTO7d+82rg3cK+lzftmefPJJdu7cSe/evdm+fbsxw8ywYcNYvHgxkyZN4ptvvmHFihVFypKXWPnJJ5+kX79+REZGotFoCAsLY9iwYdy8edNkcczSEivnzXar6uKypPx4oM0peYmSY2JiTEwL4nai5Dx7XHh4OFevXjVmU4eCiyG17WszNmgsgW6B1HGuQ0JWAh/u/xCdQcfJ2JOFtl9couSivqx79uxh3rx5bNy4kZMnTzJnzpxSJVsujsIWePbs2cNXX33FX3/9dd/t5e9TaX5O6nQ6Tp48ibu7u4knxMSJE/n222/5+++/8fLyolWrVkW2e/jwYV577TW2bNlCkyZN2Lt3r3E8u3TpwpEjR0zOsaTEypDbB42m7LlkJdWYipz2W4o5Jb8P6ZQpU8TgwYOFEHfMKZcvXxY1a9YUW7duNdY7duyYyM7ONtbLb0Lw8PAQly5dErdu3co1C5w8LpaeXCrqvVhPAKLhpw3Fm6FviujUaBNZOnfubGwj79w8U8iiRYsKNaesX79etGzZUuh0OpGdnS369etn9M/NM0vk+fX++OOPxmvkXT/PfPPrr7+Kpk2bmphTCutTXnuRkZEiMzPTpL3CzstfPn36dCFErp+wu7t7oaah/H0vSrYpU6aI5557TkRERAhfX1+jH7UQQvTu3Vv4+vqK5cuXF5AhjzNnzhjNEBcvXhReXl5i5syZIjQ0VAiR6yu8bNkyk3MaN25s8jmpbKQ5RVIUFWpOsbcueXOlqVtayjtR8ohhI7C2tqZ/z/587/I9Cgp/X/6b0KhQxjQbw5hmY7DT2JkkSi5pkuI+ffrwyy+/0KhRIzw8POjZs6dxdlpcsmVXV9ciEysXR58+ffj555/p0aMH3t7eJu3dC71eT6tWrUhPT2fevHlFulkWl/R5w4YN/P333xw6dAh7e3vmzJnDiBEj2LdvH7a2trz00kuMHz+eYcOGFSlHYYmVT506RUpKCpAb+Kx+/frG+jKxsqRKUdlPEUuivHZsnok/I57d9KxotrSZaLa0meixsof468JfIiUlRTRv3lykpaWZra2UlBTj/2vWrBGNGze+72uWZRZIETN0czNu3Djx0Ucflfq8Q4cOiUmTJgkhhBg2bJjJjr933nlHLFq0yGwymgM5E5cUxQO32acyaOzemCW9l7D1ylY+P/w50enRvBP2Dq1qtOL1D183Jko2Bw9KsuXo6Gi6d++Ou7s7mzcXvWBeFO3atWPp0qWEhITQu3dvk0VNmVhZUpWolomS9+/fz+TJk7G2tsbR0ZFly5aVaEdhSkoKLi4uJCcn4+zsXC6yZemy+PHUj/xw8gcydZkoKDze4HFeb/06nnYl8/qoDAwGw33vLJPcH3IMJIVRLZV4dHQ0rq6u2Nvb8+233xIfH1/klt38VIQSzyMmPYYvj37JhosbAHCwcuDlFi8zqskorNWlix1R3ggh0Gq1WFlZSe+ISkKOgaQoquVj3cfHB3t7ewCsra3RaCzPalTToSazQ2bzc9+faebRjHRtOnOPzGXQukHsuLrDohIwCCGIjo62KJkeNOQYSIrCYpX4119/Tdu2bbGxsSmQeUOr1TJ+/Hjc3Nxwd3dnwoQJhcbgiI+PZ8GCBbzwwgsVJHXpCaoRxLL+y/i408d42nkSlRrF6ztf5+WtLxOZGFnZ4kkkEgvHYpW4j48P06ZN46WXXipwbObMmezZs4fTp09z6tQpwsLCmDVrlkmdjIwMhg8fzrx580q8w7CyUCkqBtYfyIbBG3ix+YtYqaw4cOMAw/4cxqyDs0jOrt4xPCQSSdmxWCU+ZMgQBg0aVKgCXrx4MdOmTaNWrVrUqlWLqVOn8sMPPxiP63Q6nnjiCSZMmEBwcHBFin1fOFg58Hrr11k3aB09/XuiF3p+Pfsr/f7ox/Izy9EZKi/in7TDVj5yDCSFYfELmzNmzCA8PNy40SYxMRF3d3ciIiJo0KABkJtqqmHDhiQlJeHi4sLPP//MhAkTjOEe+/fvz1tvvVXg2tnZ2WRnZxvfp6Sk4OfnR2JiosnCpkqlKpC7T1EUFEUpt3KVSsWB6AP875//EZEUAUB9l/q80/4dOtbqWMA2qlKpEEKUqrwy+lRaGWWfZJ/M1afqiuWt+N2DtLQ0ABOXwbz/U1NTcXFxYfTo0YwePfqe15o9ezYffvhhgfKoqChj7G5HR0djiNO8tvPadHV1JTY2lszMTGO5h4cHTk5O3LhxA61Wayz39vbGzs6OqKgokw+1j48PGo2Gq1evmsjg7+9Pa8/WfNriUzbf2Mwvl37hQvIFXt76MsPqD+OZ2s8YZ2ZWVlbUrl2btLQ04uPjjdews7PD29ub5ORkk5jRpe2Tu7s7VlZWJCQk3HefdDod0dHRxjJFUQgICCArK4ubN28ay8u7T+Ycp4roU2pqKjqdDo1Gg5ubW7XoU0WOU1G7hasDVXYmHhkZadwqHRkZSWBgoHEmXlIsfSaefxaTkp3Ct8e/5ddzv2IQBl5q/hLjg8YXWb8k5SWVRQhBVFQUvr6+JjMaOcOruD4ZDAauXr2Kv78/arW6WvSpPGSXM/EqgJubG76+voSHhxuVeHh4OH5+fsUq8DzTSk5ODufOnTOGPW3UqBGdO3dm/vz5xpROKpWqwKAX9SEoafnJkycZMGAAly9fNmbsDgsLK/Y6eR9IAFc7V97t8C4N3Brw4f4PWXRiETXsa/BE4ycKrV/Udcoie94XorD7UprrlEXG8uqTOcsrqk959z+vTnXoU0WXV0csVonrdDrjy2AwkJWVhUqlwtramjFjxvDxxx/zyCOPADBr1ixj7OqiyMsCcvny5UKzgowbN8642ae88fHxMVHgpWFYw2HEZsayIHwBsw7Owt3WnUfrPGpmCSUSSVWhQh5XQggMGRmles2eMQN3e3vmzJrFtg0bcLe3Z0DPXhgyMpg6ZQoh7dvTqklTWjVpSpeOHY1pvkrDe++9x6xZs5g/fz7+/v4mCvz555/np59+AopP1bZ582Y6depEmzZtaN++PTt37jQemzFjBoGBgbRp04bffvvNWH758mUTm35x1y+MV1u8yoiGIxAI3g17l39i/il130uLlVXhiaAlFYccA0lhVIhN3JCRwbnWbcq1jUZHj6C6vUuzOPJm4klJSezYsYOZM2eyY8cOJk2axL59+zh06BDJyck0a9aMAwcOYG9vT7du3di4cSO1atUiLi6O1q1bs3//frKzs3n66afZvHkzzs7OREZGEhISwuXLl9m2bRtvv/02+/fvx8nJidGjR7Nnzx4uX75sIkNSUlKR18/LIlQYeoOeN3e9ybar23C0cmRpn6U0cm9kzlsqkUiqABZrTqkIHnnkEcLDw5k7dy7ffPONMe1XREQEDg4O+Pj4sHHjxiJTtZ09e5bIyEg6d+5sLFepVFy9epXt27czYsQI4wLpK6+8wp49ewrIsG/fvnumgisMtUrNJ50/4ZWtr3Dk5hFe3fYqP/f9GV8n3zLfj6IQQpCWloajo6P0Va4k5BhIiqJClLhiZ0ejo0fuXfE+2ygtNjY2tG3bFg8PD9q0acNff/2Fm5sb27dvp0ePHsCdVG379u0rcP6ZM2fo1asXy5cvv7d8RXzxirv+PeVX2zCv+zye+/s5IhIjeHXbq/zU9yfcbUuWabykCCGIj4/HwcFBKpBKQo6BpChKZRM/d+4cTzzxBLVq1cLGxoZatWrx5JNPcu7cuWLPUxQFlb19ub7K+sHu2bMnH3zwAT169DCuaM+fP5+ePXsCEBwczKVLl9i2bZvxnPDwcHJycujduzfbtm3j+PHjxmOHDh0yXnfVqlWkpqYihOC7774rtP3irl8SnK2d+bbnt/g4+HAl5Qrjto0jQ5tRupsgkUiqLCW2iV++fJlWrVrRsWNHhg0bRu3atbl+/TqrV6/m4MGDHD16tEo41Oe3RwMcPXqUNm3aEBAQgI2NDefPn0elUhEfH29cfDx69Chvvvkm8fHxJqnabG1t2bZtG++//z4ZGRnk5OTQqlUr48x8xowZLFu2DGdnZ/r27csvv/xSwCZ+r+uXlEvJl3hm0zMkZSfxiM8jfNXjK6xU5lkIy++j/CC5blkScgwkRVFiJf7aa69hbW3Nl19+WeDYpEmTyMzMLJCrsqpRkfHEy4Pjscd5ccuLZOoyGVBvAB93+hiVcv9feIPBQGxsLF5eXlKBVBJyDCRFUWIl3rhxYzZv3kxAQECBY1evXqVnz56cP3/e7AJWJFVdiQOEXQtj4o6J6ISO5x56jiltp1S2SBKJpBwp8SP95s2bhSpwyI21cOvWLbMJVdHMnz+fpk2b0q5du8oW5b4J8Q3hw0dy48EsPbWUH0/9eN/XFEKQlJQkExJUInIMJEVRYiV+dyyCu6nKH65x48Zx+vRp/vmn/DfNVAQD6w9kUptJAHx2+DNjCriyIhVI5SPHQFIUJXYxzMrKYvLkyUUezx9ISlL5jHloDHGZcfx8+mfe3/M+bjZuPFL7kcoWSyKRmJkSK/GnnnqKxMTEIo8/+eSTZhFIYh4UReHNtm8SnxnPxksbmRQ6icW9F9PMs1mJzjcIA8djj7PtyjbCrodR17Yun/t9Xs5SSySS0mLxoWgrgvnz5xujGJ4/f75KL2zejVavZdz2cey/sR83Gzd+7vczAc6Fr21o9Vr+ifmH7Ve3syNqB3GZcSbHv+z6Jd0DuleE2JK7MBgMJCQk4O7uLr1TJCbclxI/d+4cp0+fpk2bNvj7+5tTrkqhOninFEa6Np3nNz/P6fjT1HaszS/9fsHTLjftXYY2g33R+9h+dTu7ru0iNSfVeJ6jlSOdfTujM+jYcmUL9V3qs3rgajSqBzpag0RiUZRYiU+ZMoWgoCBjxpxff/2VZ555BhcXFzIyMli/fr1xl2NVpboqcYD4zHie2fQMV1Ov0sitEaObjmZn1E72Xt9Llj7LWM/d1p3u/t3p6d+T9jXbY6W2Iikrif5/9CdFm8KMh2cwtOHQSuzJg4mciUuKosRKvEGDBmzdupW6desCULduXSZMmMDkyZP5/vvv+fHHH8scI9tSqM5KHCAqNYrRG0cTnxVvUl7bsTY9/HvQw78HLb1aolapTY4bDAbm7ZvHDxd+wMvOiw2DN2Bvde+IkRLzIXdsSoqixEo8T7lBbpS/Zs2akZiYiL29PVqt1hhGtSpT3ZU4wJn4M0zcORFHK0d6+PegZ0BPGrk1Kjb2jMFg4MLlC4w/Op7o9GgmtJrAyy1erkCpJVKJS4qixMZNe3t7UlNTcXJy4sCBAzRv3hz7fPG78ycxlVguTTyasHXY1lKfZ6WyYmKriby7510Wn1zM0MCheNh5lIOEEomkNJT4kd69e3cmTZrE3r17mTdvHgMHDjQeO336ND4+PuUiYEVQnXZslgeKouDq6kqfun1o6tGUdG06C48vrGyxHijyxkCGoZXcTYnNKTExMYwaNYqDBw/SqVMnVq1ahaOjIwBvv/022dnZhQbHqko8COaU++XgjYO8uOVFNIqGdYPW4e9c9b2SJJKqjNn8xG/dukWNGjXMcalKQyrxwrk7gt5r215jz/U9PBrwKJ93lRuAKgIZxVBSFCX+NBS35T42Npbu3eUmkOpMZmam8f9JbSahoLDlyhaOxx4v5iyJOck/BhJJHiVW4uvWrWPmzJkFyuPi4ujevTtNmjQxq2ASy6WhW0MG1s9dE5lzZI4MyiSRVCIlVuJbtmxhwYIFLFiwwFgWHx9P9+7dCQwM5LfffisXASWWyfhW47FR23Dk5hF2XdtV2eJIJA8sJVbi9evX5++//+aDDz5g+fLlJCYm0qNHD+rVq8fKlStRq9X3voikSqIoCh4eHiaeETUdajKqySgA5h6Zi86gqyzxHggKGwOJBEqZKLlFixasW7eOsWPH0rFjRwICAli9ejUaTdWOpSFdDItHURScnJwKKJDnmz+Pi40LF5Mvsi5yXSVJ92BQ1BhIJCX2Tpk3b57x/+3bt7Nr1y6mTp2KjY2NsXzixInml7ACkd4phWMwGLhx4wa1atUq4Bnx8+mf+d8//5Pb8cuZ4sZA8mBTYiXerVu34i+kKOzYscMsQlUWUokXTnFbvnP0OQxcO5DradcZHzSeV1q+UklSVm/ktntJUZTYDrJz587ylENSRbFWWzOx1UTeCXuHJaeWMKzhMLkdXyKpQOQjXXLfyO34EknlIZW45J4oioK3t3eRi2oqRcWUNlMAWHVuFVdSrlSkeA8E9xoDyYNLtVTiWq2WRx55BFdXV1avXl3Z4lR5FEXBzs6uWAXSvlZ7OtXuhE7omHd0XpH1JGWjJGMgeTCplkpco9GwevVq3njjjcoWpVpgMBi4cuUKBoOh2HpyO375UdIxkDx4lEmJ63Q69u7dy4oVKwBIT08nPT3drILdD4qiUKtWrcoWo1pREiemhm4NebzB4wB8fvjzMm/HF0IQGhXK35f+LtP51RUZ3kBSGKVW4mfPnqVJkyaMHDmSF154Acj1G3/xxRfNKtjXX39N27ZtsbGxYdCgQSbHtFot48ePx83NDXd3dyZMmIBOV7E7BnOuXefGjBnkXLteoe1aOuOCxmGjtuHoraNl2o6frk3nvT3vMWHHBN7a/RY7r0qvKImkOEqtxMeOHcvrr7/OtWvXsLKyAqBr167s2bPHrIL5+Pgwbdo0XnrppQLHZs6cyZ49ezh9+jSnTp0iLCyMWbNmmbX9exH/3Xck/baCC336ED1tGjlRURXavqVyP9vxT8efZsSfI9hwcYOx7JNDn5ChzTC7nBJJdaHUSjw8PJyxY8cCGBdZnJ2dSU1NNatgQ4YMYdCgQXh6ehY4tnjxYqZNm0atWrWoVasWU6dO5YcffjBr+/fC5fGBOAQ/DDodyat/50KfvkT/5z1yLl+uUDkqAkVR8PHxKfGi2gvNX8DVxpWLyRdZG7n2nvWFEPxy+hee3vg0V1OvUtOhJgt7LqSWQy2i06NZdGLRffag6lPaMZA8OJRaiXt7e3P5LkV1/vx5fH19zSVTsSQmJnLt2jWCgoKMZUFBQVy9etWYyBlgxIgR/PTTT3z00Ue8/fbbhV4rOzublJQUkxfkLiLlfxVWZte6Nf6LF+O/7BccOnUCvZ7kNWu40K8/0e+8Q9aFCyb18+yZd1+nqHLIVW7lWV4aWTQaTYHrFFXf0cqRV1rk7txcEL6AtOy0ImWJz4hnwo4J/Pef/6Iz6Ojm142V/VfSsVZH3m3/LgBLTy0lMjHSrH0yV3lFjZMQApVKhRCi2vSposepulLqyFWvvfYaQ4cO5aOPPkKv17Nlyxbef/99xo8fXx7yFSAtLQ0AV1dXY1ne/6mpqbi4uACwcuXKe15r9uzZfPjhhwXKo6KicHJyAsDR0RFPT08SEhKMbee16erqSpqvL4ZpU7E5dw7tsuUYDh0ied16ktf/ibpLFzRPPoEqIABvb2/s7OyIiooyWaDy8fFBo9Fw9epVExn8/f3R6XRER0cbyxRFISAggKysLG7evGkst7Kyonbt2qSlpREfH28st7Ozw9vbm+TkZJKSkozl9+pTbGysSQICNzc3EhMT0Wg0JmsPxfVpWOAwlp5Yys3Mm8w/MJ+RASML9OlE0gk+P/M58TnxWKuseb7e8/Tz6UfyzWQyrDLo7t+dTjU7sSdmDx/s+oCPW36Mvb29Wfrk4eGBk5MTN27cMEnybanjlJqaSkJCAu7u7ri5uVWLPlXkONWpU4fqSpnSs+XFFb98+TL+/v6MHTu23JT4jBkzCA8PZ+3atUDuTNzd3Z3IyEjq168PQGRkJIGBgSQlJRmVeEnIzs4mOzubRYsWsWjRIvR6PZGRkSQmJprETlGpVAWe5oqioChKgfLs06eJ++Zb0rZvz6uIU+/eeLz2KnaNGpX4OnfPusqjvKSyCCGIiorC19fXJG5HUfXzyv+68Bfv7nkXe409GwZtwMvBCyEEOr2O7058x3cnvsMgDNRxrsP/Ov+PRm6NCsgYlRLFkPVDyNJnMeuRWfSv198sfTJXeUWNk8FwJ3aKWq2uFn0qD9mL61N1xWw5NsuLu5U4gJ+fH1988QVDhw4FYPXq1UyePLnA07e0mDMAVtaZM8Qt+IbUrVuNZU69euE59jVsq1gWpPwKpDRfBoMw8NRfT3Eq/hRPNn6S9zq8R0x6DO+GvcuRm0cAeLz+47zX4b1iox8uOr6Iecfm4WHrwfrB63G2fvCCk5V1DCTVnxJ9Gu62Gxf1Mic6nY6srCx0Oh0Gg4GsrCxycnIAGDNmDB9//DExMTHExMQwa9Yss7s43i+2TZrg+9U86q5bh1PfPqAopG7dyqXBQ4gaN57Mk6cqW8RyR6WomNwmNzfrqnOr+PXsrwz/czhHbh7BXmPP7JDZzOw0857ha5996FnqONchPiuer499XRGiSyRVhhLNxFUqVbGr4kIIFEVBr9ebTbAZM2YUsFd36dKF0NBQtFotb7zxBsuXLwdg1KhRzJ07t8zJKebPn8/8+fPR6/WcP3++XELRZkdEEPftQlI2boTbt9yxSxc8x43FrkULs7ZVHhgMhjLPAMduG0vY9TDj+ybuTfi0y6cEOAeU+BoHbhzgpS0voVJULO+/nIc8HiqTLFWZ+xkDSfWlREr8ypWSBTQKCCj5l9ISqYh44tkXLxK/cCHJf26A23Y7h5AQPMe+hn2rVuXS5v0ihECr1WJlZVUmF7eIxAiG/zkcvdAzqskoJrWZhLXautTXeXv322y6tIlmHs34pd8vqFUPTkrA+x0DSfXF4m3iFUFFzMTvJufyZeIWfkfy+vVw+xeMQ/DDeI4bh32bNuXadmkxhz32ROwJFEWhmWezMssRmxHLwLUDSdOm8X7H9xnRaESZr1XVkDZxSVGUSYnv37+f0NBQ4uLiTFae58yZY1bhKprKyOyTExVF3MKFJK9dB7fd9+w7dMBz7FgcOrSvEBnuhSUpkGVnlvHJoU9wsnbiz0F/PjAJKCxpDCSWRak/DfPnz6dHjx4cOnSI+fPnc+nSJRYuXEhMTEx5yFftsfbzw2fmTOr//TeuI0eClRUZBw9y9dlnuTJqNOn798vAR/kY2WgkTdybkJqTypwjVXvSIJGYg1Ir8S+++IJNmzaxZs0a7OzsWLNmDatWrTJJmFzVsIRs99a+tan14QwabNmM21NPolhZkXH4MFfHPM+Vp54mLWxPpSpzS7HDalQapnWchoLC+gvrORxzuLJFqjAsZQwklkWpzSnOzs5Gd0IPDw/i4uIA8PT0NNmxVRWxpETJ2ps3if/+B5JWrkRkZwNg27IFXmPH4tC58wP/hf5w/4esPr+aBq4NWPnYSqxUVpUtkkRSKZR6Jl6zZk3jdtw6deoQGhrKqVOnpJ3OzFh5e1Nz6nvU37oF92efRbG1Jevf40S98ioX+w8gbtEitDdvVYgsQggyMzMtyqzzRus3cLNxIzIpkmWnl1W2OOWOJY6BxDIoteZ97bXXOHjwIACTJ0/m0UcfpVWrVowbN87swlUUlmBOKQqrGjXw/s+7NNi2FfcXnkextyfn4kViP59DZLduXH35ZVI2bcJwe7ZeHgghuHnzpkUpEBcbFya3zd1ItODfBcSkV+81GUscA4llcN8uhlFRUaSlpdGkim0lLwxLMqcUhT4tjdTNm0n6Yw2ZR44Yy1UuLrj074fL4MHYNmtmVnOLpXpGGISBMX+P4eito/QK6MWcrtV3odNSx0BS+ZRaiV+/fh07Ozvc3d2NZYmJiWRmZuLj42N2ASuSqqDE85Nz5QpJa9eSvHYduhs3jOU2gQ1wGTQYl4GPofHyuu92LFmBnE88z4g/R6AXehb0WECIb0hli1QuWPIYSCqXUn8ahgwZQtRdWWyuXLliDEYlqTisAwKo8frrNNi2Ff/FP+A8YACKjQ3ZEZHc+vRTIrp2I+rV10jZvAVxO+5MWcnL4mRpNHRraMwkNOvgLLJ0WZUsUflhqWMgqVxKPRN3dXU1iQ8MufY6Nze3AuVVhcrYsVle6FNTSdm4ieQ1a8gMDzeWq11dcR4wAJfBg7Bt2rRaebeka9MZuHYgtzJu8WrLVxkXVLb1metp14lOi6ZdTctbG5FIiqLUSrxOnTocPHgQb29vY1lMTAxt27bl2rVrZhewIqlq5pR7kX3xEslr1pC8bh26W3c8WWwaNcJ1xHBcBw9GZV98BEHIfUinpaXh6Ohoscp/y+UtTNk1BSuVFWseX1Oq4FrJ2cl8d/w7lp9djs6g49Mun9KnTp9ylLb0VIUxkFQOpTanDBgwgDFjxnD9em6W9+vXr/PSSy8xcOBAswsnuT9s6tWlxpTJNNi5A79F3+Hcry+KtTXZ585x8/9mEtmtO7e++AJdbGyx1xFCEB8fb9GeEb0CevFI7UfQGrTMOjirRLJqDVqWn1nOgDUD+On0T8akzouOL7K4vlaFMZBUDqVW4rNnz8bW1hY/Pz/s7e3x9/fHysqK//73v+Uhn8QMKGo1jiEh1J4zh8Cw3XhPnYqVnx/65GTiv11IZPceRE+dSnZERGWLWmYUReG99u9hrbJmX/Q+tlzZUmRdIQS7onYxZN0QZh+aTVJ2EvVd6vNZl8+w19hzPvG8SehcicSSKbOL4a1bt7hy5QoBAQHUqFHD3HJVCtXNnFIcQq8ndft2EhYvMbGdO3QOweP557Hv0MH4s70qeUZ8E/4NC/5dQA27GqwfvB4HKweT4+cSzvHZ4c84cOMAAO627owLGseQwCFoVBo+P/w5S08tpVWNVvzU96fK6EKhVKUxkFQspVbiWVlZaDQaYwKGNWvWYGVlxYABA8pFwIqgOi1sloWMo8dIWLKE1G3bjAkrbJo0weP5MTj36YNQq4mNjcXLy8viFUi2PpvB6wYTlRrF6Kajebvd2wDEZcbx9bGvWRO5BoMwYKWyYlTTUbzU/CWcrJ2M59/KuEWf3/ugNWhZ2mcpbbwtIyywwWCoMmMgqVhKrcRDQkL49NNP6dixIx9++CHffvstGo2GF198kenTp5eXnBXCgzQTL4ycK1dI+PEnktasQdzOOK6pWRP30aNxHTEctZPTPa5gGey9vpdXt72KWlHzU9+fOBRziEXHF5GhywCgd53evNH6DXydfAs9Py8uS0jtEBb0XFCRokskpabUStzDw4Nbt26hVqupV68ef/75J87OzjzyyCP3nai4snnQlXgeusREklasIOGXZehvBzhTbG1xHjAAtyeewK6Z5adGmxI6hS1XtqCgIMj9iDfzaMbb7d+mVY3iMyhFpUQxYO0ADMLA6sdW08i9UUWIXCxCCJKTk3FxcZHeKRITSv27TK/XoygKkZGRGAwGHnroIfz8/EhMTCwP+SSVgMbNDc9XX6XBju3U+ngm1g0aILKySF69msvDhnFp2HCSVq/GkJFR2aIWydvt3sZeY49AUNOhJp+EfMKy/svuqcAB/Jz96B3QG4AfTvxQ3qKWCCEESUlJ0jtFUoBSz8R79epFYGAgN27cwMfHh/nz5xMVFUVwcHCBnZxVDTkTLxy9Xs/lTZuw2RlK2pYtCK0WAJWjIy4DB+I6ciS2jRpWspQFOR57nIjECPrV64edxq5U555NOMvwP4ejUlT8OehP/J39y0nKe5OUlcRbu9/CT+PHtK7TpE1cYkKpPw2LFi0y/qzLy0Z/4MABnn76abMLJ7EMFEVB3awZPp/+jwa7d1HjrbewCvDHkJZG4vLlXHr8cS4/9TTJ69aVazTF0tLCqwVDGw4ttQIHaOzemE61O2EQBpacWlIO0pUMIQTT903nwI0DrIpaxdmEs5Umi8QykYmS81HamXhUQgZrjl1nbNf6aNTVd3ZkMBhISEjA3d3dOAsUBgMZBw6Q+NsKUnfsMOYHVbu44DJ4MK4jR2BTt25lin3fHLl5hOf+fg4rlRV/D/2bGvYV70q76vwqPtr/kfF9d7/ufNn9ywqXQ2K5lEiJb9y4kX79+gGwfv36IutV9V2bpVHiBoNg+ML9HLmSSJsAN74YGYSf+723sFdHtLdukfzHHySuXIku+k40RfuOHXEbOQKnHj1QrK0rUcKy88ymZzh26xhjHhpjjF9eUVxMvsjIP0eSpc9ieMPhrD6/GoFg1WOraOzeuEJlkVguJVLizZo14+TJkwDULWJ2pSgKFy9eNK90FURZ/cTX/xvN1D9OkJqtw8lWw6zBzXmsZdUOx1sYhc3EC0Po9aSFhZH02wrSdu8GgwEAtYcHrkOH4jpkMNZ16lSQ1OZh97XdjNs+DnuNPVuGbcHFxqVC2tXqtTy98WnOJJyhY62OfNPjG6bsmMKO6B308O/BF92+qBA5JJaPNKfkoywLm1EJGUz87RjHriYBMKKtLzMGPoS9taYcJa1YyrJbUBsdTdLq1SStWm0Sm8W6QX2cunXHsXs37Fq2RLHwRTohBMP+HMb5xPOMDxrPKy1fqZB25xyew5JTS3C1ceX3gb/jaevJnjN7GH94PAJhMa6Pksqn1N+g/v37F1pe1U0pZcXP3Z6VrzzM+G4NUBRYefgaA+btISrBct3vKgIrHx+8Jk6kwY7t1J73JQ6PPAJqNTmRF4hftIgrTz5FREhnoqdNI3XHDgy3NxdZGoqi8EKzFwBYdmYZmbryl/PAjQPGxdQPgz802uL9Hfx5NOBRAL7999tyl0NSNbivbPf5cXd3JyEhwWyCVQb362K4/0I8k1aE4+Zgzdpxwdho1OUgZcVjrrgd+uRk0sL2kLZjB2m7d2NISzMeU2xtcQgOxql7Nxy7dkXj6WkO0c2CzqDjsTWPcS3tGu+2f5enm5SfJ1ZSVhJD1w/lVuYthjcczgcPfwDcGYMc5xyG/TlMzsYlRkqsxOfNmwfAO++8UyBi4YULF9i6dSunT582v4QViDn8xBPTc0jN0uHvkbvIqdMbSM7U4uFoY05RK5Ty2C0ocnLIOHyY1B07SduxA2109J2DioJdixY4PfooLkMGo3FzM0ub98PKcyv5vwP/R02HmmwcshErlfmz7AgheGPnG+yI2kEd5zqsfGyl0T0y/xi8tfstNl/eXO3zikpKRomVeLdu3QAICwsjJOROHkOVSoW3tzdvvPEG7du3Lx8pK4jy2Owzd+t5lh28wucjgujS8P7zXVZHhBBknz9P6vbtpO3YSdbtRXQAxcYG5wH9cR81CttKTMadrc+mz+99iMuMY+YjM3m8weNmbyPPnVCj0rC833KaeBTe34jECIauH4pA8PvA32noZnkbrSQVR6nNKW+++SafffZZecljNhYtWsSSJUuwtrZm8eLF1KtX757nmFuJ5+gMPD5/L2du5JqfXuxUl7f6NKpyZpaKjqCnvXmTtB07SFq1mqx8v+7sWrfGfdTTOPXqhVIJ+SYXn1zM3CNzqedSjzWPr0GlmO9e5HcnnNJmCs81e87k+N1jkBcbRs7GJdXSOyUhIYG+ffuyd+9ejh07xqeffsrKlSvveV55zMSztHpmbTzDT/uvANCstjPznmhFPS9Hs1y/IqisWNZCCDKPhZP4yy+kbNli3FCk8fLC9YmRuI0Ygcar4n7dpOWk8ejvj5Kak8rnXT7n0TqPmuW6OfocRm0cZXQnXNhrYYEHxN1jEJEYwZD1QwDkbPwBp0TfSDc3N9zd3e/5shQOHTpE165d0Wg0tGvXjnPnzlWaLLZWaj56vBmLnmmLm70VJ6+nMOCrPaw8HCWDGd0DRVGwb92K2nM+p8GO7XiOG4fayxNdbCxxX31NRPceXH/rbTLDwyvkXjpaO/JEoycA+E/Yf/jx1I/oDfr7vu5Xx77iTMIZXG1c+bjTxyWa4Qe6BdIroBcAC/9deN8ySKouJZqJ79q1q0QX69Kly30LlMfXX3/N0qVLOXHiBH379mXt2rXGY1qtlkmTJrFs2TIUReHpp59m7ty5xkQVy5cv5+rVq7z77rsANG/enBMnTtyzzfIOgBWTnMWkFeHsvxiPrZWKHVO64uNa+rgeFY0lZZUROTmkbN5C4i+/kPnvv8Zy22bNcBv1NM59+6KyKb9F5AxtBm/uetOYvq11jdbM7DQTPye/Ul1Hb9BzMv4kO6/u5IeTuZESv+z2Jd39uxdav7AxOJ94nqHrhwLwx8A/CHQLLGu3JFUYizWn/PHHH6hUKrZt28a1a9dMlPj06dNZt24dmzZtAqBv374MGTKEDz7IdcfatGkTu3bt4pNPPgEgKCiI8HwpyIqiIqIY6g2Cb3ddwMPBmifaV15kvNJgqZnWM0+cJHHZMlI2bkTk5ACgdnPDdcQI3J4YiVWtWuXSrhCC3yN+59N/PiVDl4Gdxo632r3FsMBhxd6fpKwk9kXvY/f13ey9vpek7CTjsfzuhEW1WdgYTA6dzNYrW+ldpzefdbH8tSqJ+Sm1Ev/oo4+KPJanRM3JjBkzCA8PN1Hifn5+zJ07l2HDhgGwatUq3nzzTa5cybU7JyQk0L9/f/bs2cO///7L7NmzWbVqVYFrZ2dnk50v6l5KSooxNnp+Ja5SqTDc3kKeh6IoKIpilvKjVxPZGxHHa13ro1YpxjaFEAXMBOYsL88+FVZeXn3SJSSQvPp3kn77DV1MTG4ltRrHHt1xe/ppHNq3L3ANc/TpWuo1Ptj/AUduHgGgk08npj88nRr2NYz390zCGfZc38Oe63s4Hnccg7hzvpOVEw/7PEwX3y70r9cflaIq9TidTzzP8A3DUVD4feDvBLoFWuw4lbRP5VFe2b8gy5NS7w0/duyYyfsbN25w7Ngx+vTpYzahiiMxMZFr164RFBRkLAsKCuLq1atGP1p3d3eeffZZQkJCsLKy4ocfCg/sP3v2bGM43fxERUXhdDsVmaOjI56eniQkJJCWb3OKq6srrq6uxMbGkplvt6GHhwdOTk7cuHED7e242wDe3t7Y2dkRFWVqC3f19Gbir+FcT8pk+6nrTO3hSw1HK/z9/dHpdETn859WFIWAgACysrK4efOmsdzKyoratWuTlpZGfHy8sdzOzg5vb2+Sk5NJSkoylpe2T25ubqSlpSGEQHd7cbG4Pvn4+KDRaApkeirXPg0bSoMXXyB67VrSVqzEcOIEaVu2krZlK9Z16qDu0wfRtQuKq2uZxqmoPn3f63t+OvUTX4d/zZ7oPQxeN5gR/iNIUiexO2o3cVlxJvUD3QLp4NWBh+weorFzYzQqDXZ2dqhVapKSkoocp9TUVOPn283NzThOtpm2BHsGsy9uH/OPzueLHl/cd58s6bNnrnGqU8Vi9pQGs5hTfv31V/bv32/cEGRO7p6JR0VF4e/vT2xsLJ63d/XFxsZSo0YNoqKi8PUtPG9iYVjCTBxgzbFoPlh3kvQcPS52VnwypBl9m/tYzGxICGG8t/lnNJY8w8s+f57EZctJ+WsDIuO2UtBocOrRA5fhw3B4+GFUarXZZq2RiZFM3TuVMwlnTI7bqm3pWKsjIbVD6FS7Ez5OpR9Xg8FgYhNXq9UmsuSfjf8x8A/quZi601ryOJVVRjkTv4NZlLjBYDA+Xc3N3Uo8MTERd3d3IiMjqV+/PgCRkZEEBgaSlJSEi0vpo8xZQrb7S3HpTPz1GCeuJwPwdAd/pvVvip115fuUW9LCZmnRp6WTsvGvXJ/zfIvbVr6+uA4bisvgIVh5mydOuNagZfGJxeyL3kdTj6aE+IbQ1rst1ur7D8N7rzGYtHMS265uo0+dPnza5dP7bk9SdTDLN3LFihU4OlaM37Obmxu+vr4mC5Xh4eH4+fmVSYEDjBs3jtOnT/PPP/+YScrSU9fTgd9fC+aVzrmzqGUHrzLw6z0kpOdUmkzVAbWjA24jRlB31Urqrl2D21NPoXJyQnvtGrFffElk9+5EjRtPamgoQn9/7oJWKiteafkKP/b9kXfav0OwT7BZFHhJeLXlqwBsvryZC0kXKqRNiWVQaiV+t8+4nZ0dr7zyCp9+at6nv06nIysrC51Oh8FgICsri5zbHghjxozh448/JiYmhpiYGGbNmsWLL75o1vbvRbZOz/4L8feuWAqsNSr+068JP7/QHi8nG+p4OuBmX/E7E+9GURS8vb0tyjOlLNg2bkzND94ncPcuas2ejV3r1qDXk7Z9O9defY3IHj2JnfcVORaYK/ZeY9DIvRE9/HsgENJv/AGj1OaUu33GHR0dadiwoXEh0FzMmDGjwKJjly5dCA0NRavV8sYbb7B8+XIARo0aZeInXlrKYk5ZuvcSM/48TXB9D97s3YjW/uYN0hSXlo1aUXBzyJ3JpWZp0eoF7g5VM0OOpZIdGUnSqtUkr1uHPt8CnH3btrgMHoxT796oHR0qT8BSkJfcWUFh7eNrqed671ATkqqPxfqJVwal8RP/Ytt55u+MRKvPvX09m9Rgcq9GNPUxvy1dCMHrv4Vz8FI8c0cEEdygYsO0GgwGoqKi8PPzq3I28ZJiyMkhdetWkn//g/T9++H210Kxs8P50V64DB6Mffv2lZbEoqRj8PqO19kRtYO+dfvyv87/q0AJJZVFiZX45Mn3zi84Z07VDMRT1oXNa4kZzNseweoj1zDcvosDWtRicq+GZo2NkpyhZcg3e7kQm46iwCP1PWlSy4lGNZ1pXNOJBjUcsbUqvwXQqrywWRa0N26QvP5PktesIefyZWO5xqcWLo8/juugQVgHBFSoTCUdgzPxZxixYUTubHzQ2gKeKpLqR4mVuEqlokmTJnTo0KHIOBVLliwxq3AVTVl3bF6MTWPutgj+/DfXr1atUhjaujYTewTi62ae5MkZOTo++vM0v/1T0F77cud6vNcvN2xpWraOAxfiaVzLidqudmaxYz9oSjwPIQRZ//5L0pq1pGzciCE11XjMpkkTHENCcOwcgl1QEEoZTXklpTRjMHHHRHZG7aRf3X78t/N/i60rqfqUWIl/+umnLF26FEVRGDNmDKNHj6ZGDfO4ZlkK97vt/nR0CnO2nmPbmVsAWKtVPNnej3HdG1DDydYsMp65kcK/UUmcjUnlbEwKZ2NSmf5YUwa3yvWP33chjqcWHQTAyUZD41pONKrpROOazjSplfvXwaZ0CudBVeL5MWRl5YbHXbOW9L17jUmgAVROTjgEB+PYOQSHTiFmc1k0ab8UYyBn4w8WpbaJHzp0iMWLF/P7778THBzM888/z4ABA1CrK9+fuayY20/86NVEPtt8jn23vVdsrVQ8F1yXV7vUw9XevAuTQggMAuN2/dBzt/hk01kuxKYZ7fX5mTW4OU91yI3ZYjAIFIV7ztaFEGi1WqysrKq8h4o50CUkkL5nD2m7w0jfs8dkQRRyZ+nOj/bCqXcfbOrVNUubpR2DCTsmEBoVSv96/fkk5BOzyCCxTMq8sJmVlcX//d//8d///pctW7bQvXvh0deqEuYOgLUvMo5Pt5zj2NUkIHdm/GJIPV4IqYtjKWfDpSVHZ+BiXBpnb6TembXfSGX+061pE5DrSbMu/DrfhF5gZDs/BreqXewDxmAwPLCz8OIQej1ZJ06QtjuMtLCw3KxE+b5SNg0b4tSnN859+mBTgsQkxVGaMTgdf5qRG0aiUlSsfXwtdV3M8zCRWB6lVuKpqan8+uuvLFmyhOjoaJ599lkmT56M6+2YFFWZ8ohiKIRgx9lbfLr5HGdjcm2qbvZWjO3agNEPB5TrgmRR8uTN5J5bcojQc7FAro9674dq8kQ7Px6u54FKdWe2J80pJUeXkEDazp2k/L0518slX6yZ+1HoZRmDCdsnEHotlAH1BjA7ZHap2pNUHUqsxHfs2MGSJUvYtGkTffr04bnnnqNHjx7V4ud1RWy7NxgEf524wdyt57kYlw6At7MN47sHMrKtH9aaileOyZla1odf59dDUZy+nUIOwM/djpFt/RjbtQEqlSKVeBnRJyWRun0HKZv/Jn3fXQo9MPCOQr8dPqI4yjIGp+JP8cSGJ1ApKtY9vo46LnXK2hWJBVMq75TGjRvzxBNPFDnrnjhxojllq3AqIp64Tm/gj2PX+XJbBNeTcgMz+bnb8UaPhgxqVdto265oTl5P5rd/rrLuWDSp2Tra13Fn5asPA6DX64m8dIXAenWkEi8jJgp9/wHIF5HPJrABDsGPYN+hPfZt2qAuJHxEWR+kebPxx+o9xqyQWWbpi8SyKLES79q1a7GzbkVR2LFjh9kEqwwqQonnka3T89uhKL7aEUlcWm4kxQY1HJncqyF9HqppYs6oSDJz9Gw8cQNPJxu6NMzNXxmbksnDn+xErVJws7fG1d7K+NfV3ppHH/KmW6MaxvNP30jG1d4aN3trXOysKu3BZKnok5NNZ+j5FDqKgk2Txji0a2+i1MuqxOVsvPojd2zmoyKVeB4ZOTp+3HeFb3ddIDkz98vcrLYzUx5tRNeGXhZhrvpx32Wmrz9V5PF3+jTmta65JoGT15MZ8NUe4zFFAWdbK9zsrajlYsfIdn4MalW73GWuKuiTk0nbs4eMf/4h49A/5Fy8aFohn1K369gRx4c7orItnbvq+O3j2XVtl5yNV1OkEscyQtGmZGn5PuwSP4RdJD0nN5pe2wA33uzdiI71PCpUlrsxGAwkpWWRphUkZWpJytCSmJFj/BsS6GX0eDl2NZGJvx0jKV1LarauwLXe69eYlzvnKvyIm6k8s/gQgd5ONPJ2pKG3Ew29nQj0dsTeuny9dywV7a1bRoWecegQOZcumRxX7OxwePhhHLt2wbFL1xL5pJ+KO8UTf+XOxtcPWk+Ac8XuNpWUL1KJ56MyZuJ3k5Cew7e7LvDjvstk63I3lIQEevLmo41o6edaKTKV9ae8Vm8gKUNLUkYOiRlariZkEOTnQoMaucHSNhyPZvzyY4We6+duxyud6zOq44OtcPKUevqBA6TsDEXEmWYKsm3aFMeuXXHs1hXbhx4qMrbLuO3j2H1tN4/UfoRJrSfR0K2hRfzKk9w/UonnwxKUeB43U7L4akcEvx2KQnc7MMujTb2Z8mgjGtU0b8TIe1Fe3inp2TrOxqRy/mYq52JSibiVyrmYNOMaweLn2tK9sTcAF2LTOHY1iV5NvHGxgPC8FY3BYODKlSt4Z2aSsWsXqaGhZB0/YeKTrvbyxLFLF5y6ds3NXORwJ/riybiTPPnXk8b3vo6+9PDvQY+AHrT0aolKkQvWVRWpxPNhSUo8j6iEDL7YFsGaY7lBthQFOtR1JyTQi86BXjzk41zui6AV7WKYkJ7DuZhUWge4YqPJ9aP/799n+Sb0AhqVQnADT/o2q8mjTb3xcLQpd3ksgcLGQBcXl7vJKDSU9D17MGRkGOsrVlbYd+iQO0vv2hVr39qEXQtj5bmV7IveR47hTrIRD1sPuvl3o4d/DzrU7ICV+sF7SFZlpBLPhyUq8Twib6UyZ+t5Np6IMSl3s7fikQaedA70olOgJz6udmZv2xJC0f5y4Ao/77/CuZt3glCpFGhf151+zWsxoq1fhW+cqkjuNQaGnBwyDx8mNTSUtJ2haO9KbGETGIhj167Yd+yAoWEdDqSfYvvV7ey+tps07Z2ExY5WjoT4htDDvwchtUOwtzJPADdJ+SGVOJaxsFlSrsSns+t8LGERcey/EE/aXYuH9b0cCAn0IiTQk471PEod7MrSuRCbxt8nY9h08gYnr+duUPJwsObgez3QqHOVW1q2rtzDGlgyQghyLl4k7bZCzzh2DO5KPaepWRPbxo1RN23Iqboa9tpdY1fCIeIy79jcrVXWPFrnUf7T4T84W1vm90EilbgJljwTLwyt3kB4VBJhEXGERcTyb1SSMa45gJVaoZW/G50DPekU6EXz2i5l8tkWQpCVlYWtra1FLYZFJWSw6eQNhIBXuuR6vBgMgkf+uwMvJxv6NKtJ50Av3B1y/dXtrdUWJX9puJ8x0CclkbZnL2m7dpF5/F+0V64WXtHZiSsd/DjUWM0+55tcN+QmPg9wDuDLbl9S3/XeO0slFY9U4vmoakr8bpIztey/EHdbqcdxNSHD5LiLnRWPNPAgJNCLFr4uBHg4lGjGWpW23Z+/mUrvL3ZT2Kdao1J4IaQu/+mbG3s9OVPL+2tP4mJnZXzVdLGlc0MvXOwsyy5szjHQp6WTfe4sWafPkHX2DFlnzpAdEWmy6UgA52rDvEEa4pzBTlgxPeBV+j3yHIq1TBFoSUglno+qrsTv5kp8unGWvu9CPKlZBf22PR1tqONhj7+HPXU8HAi4/beOh4PRC6QqKXGA2NRstpyO4e+TMZy5kUJyptYYlnd8twa82bsRAJfi0un2WWiB863VKro08mJ0xwA63961WtmU9xiInByyL17MVexnzpB95gxZZ8+SpE9j7iAVp+rktjn4oMKYrDY4te+Afbt22DZvjqoYpa4z6NALPTbqB2MBujKQSjwf1U2J50enN/DvtWT2RMSx90IcF2PTiEvLKfYcV3srAjwcCHC3o7a9gUHtA2lY07nKmSSEEGRq9SRnarHRqI3JphPSc/jj6DVSMrUk336dik4h4lbuQt+7fRvz6m0zTbZOjxBU2uJpZTxIhcFAdkQkKYcOMv/Gr/zhnbtYGnTBwMT1BhyzQLG1xa5lS2wbN8amYSA2gYHY1K9PlrXCinMrWHpqKSk5KfSu05snGz9JC88WVe7zY+lIJZ6P6qzECyM1S8uV+AyuxGdwOT6dK/HpXI7P4Ep8OjdTsgs9p7arHV0bedGtUQ2CG3hUy52V52JS+fPfaJ5o72dMr7fm2DXeX3uKR5t6M6BlLep75e4qtbdWY2elrhA3zxs3blCrVq1K+zW04cIGZuybTrYhh1pZdry9XkXtC8kmdbKsYEtrhfUPa0ixK6hamno05cnGT9KnTh9sNebJdvWgI5V4Ph40JV4cGTk6riZkcDkug4txaRy4mMCBi/Hk6O6kJbNWq2hf152ujbzo2qgG9b0cqu0sa/LKcP44er3I4/9M7YmXU67J4JcDV9h9PhZ/d3v83O3xc7fDz80eXzd77Kyrthvk2YSzvLHzDa6nXcdOY8f0+uMIjrInKfIMazP3s9rnulF5eycKhu41UDtesKWVin1NFbSa3M+Hs8GGx+w6MKLRcOo0KX08GMkdpBLPh1TihSOEIC0tDbW1HQcuxRN6Lpad524RlZBpUs/P3Y6uDWvQrbEXD9fzrPIKKz8Gg+Do1UQ2HL/BtjM3SUzPMca4ATj1YW+jO+cbvx1jbXh0odfxcrJhw4ROeDvnKq2T15NJzMhBQcEgBHohMBgEekNu2r1eTb1RqxSEEJy8cgs0NtRwtsXDwdroUlnRJGUl8dbutzhw4wAAvev05p+Yf0jIyvVm8bOvzRjXPnS55Yk+8gI5EZFkR0aSmJ3EjpYKW1qriHPJVeaKQdAmEgZEedLOsSm2DQKxrlsXjacnGi9PNJ6eqN3cUKpw+sfyRipxqpafeGVQmD1WCMHFuHR2nr3FrvOxHLyYQI4+3yxdo6JjPQ+6NvSiW+Ma1PV0KOryVRaDQZCl05ORo8fDwdr4K+TIlQROXk8hKiGDqMQMriZkci0hg9RsHSoFzs3si9VtBTxpRThrjhU9wz/9UW/srTUYDAYm/LSfv84mAbkbndwdbKjhZEMN59y/7/VrYkyxdyM5E51e4OVkUy52fJ1Bx7yj81hyaomxzM/Jj1davEL/ev3RqEzNbEII9PHxZEdGkhERwe4bYazVnCDc485GI594Qe8jBkJOCeyzQZWnmVQq1O7uaDw8cpW7pwdqT0/smjXDuW9fs/etqiGVeD7kTLxwSrKolp6tY/+FeHaeu0XouVhjwos8AjzsaeRdupgvznZWNK/tQrPaLjSt5VylZ/ZCCJIztdxIzqJJrTufrZkbTrMnMneDjUpRUKlArSioVApqReHnFzpgZ63GYDDw1vKD7L6STnxatsl+gDzy/xp4Z/VxVhzOXYh0sbOihpMNXk421Ha1o66XA6M6BuBse/9ulJsvb2ZNxBr61u1bqPK+FxcSL/Drvz/yZ9QmMgxZJsdUBlAbBGo9qA2gMWDyf0ddAB+99dd996GqI5V4PqQSL5zSekYIIYi8lWY0u/xzOcHo4ldW1CqFBl6ONPd1qTaKvTTkHwOBQnx6NrdSsolNzeZWahbx6TmM7drAWP/NVf+yPjza5NdRfk5+2Nu4R2DOlnMcupxAXU9H6nrmupjW83LAz93eGLsmP0Lkmnr0t80+ALZWqvtaD0nXpvPnhT/59eyvXEy+eO8TgJ5WzZn71PIyt1ldkEo8H1KJF47BYCA2NhYvL68yeUak3Z6lx6YW7vFSGALBzZRsTl5P5sT15ELPVasUAms40qy2Cy187yj26hhDpSxjkDf7v5Waq/BvpWYRlZBJbFoWMwc1N9Yb9f1B46+B/KgUqOFky4H3ehjLxiw5xM7bybXzoyjgYK3hyPs9jYr/m9ALHL2aiJ2VGo1awUqlyv2rVqFRKbzVp5Gx7s6zt4i8lYZaBXqy0Bq0aPVatAYdOXotQ1rXQlHp0QkdYRE3MQgdvZvUl7tIgernHyYxOyqVCm9v7zKf72ijoVfTsp8vRK5CP3FboZ+4lsSJ6ynEpWVzNiaVszGprD5yDbij2JvXdqG5rwsNvBxxsMl1BbS30WBvpcbOWo2N5v5mjhVNWcZAURRc7a1xtbemYTGmrHf7Nub8zVQux6VzKT6DS3FpXI7LIC1bR0KG6V6Cou6ZEJCl1WOdb7H12NVEtp6+WWS7eZuuANb/G13s2sDYTh2M9v5fdhnQGwzUf1gqcJBKXFIChBAkJyfj4uJSKYpPURRquthS08XW+DDIU+zHryUZZ+snricTl5ZjVOyrbiv2wlCrFKNCt7dW3/H5tlajUSmoVSqs1AoatQorlYJadft/tYJGpcLeWo2DjQZHGzWOthocrDU42mhwtNUYY7SoFAWFXFu3onD7paBSMB5TlNxrO9ioCzVd5FGeY9Dstnnq7vbi0nJIzdKalM8dEYTWYLhjt7/tH5+RoyMzR28i23PBdejaqAaZWj06vQGdQaDVG9DpBVqDwbi4C9AmwA0hBFpDrneOWqWgUeW2obl97/PoHOiJtrBFgQeUamlO0Wq1dO3alVOnTvH9998zbNiwEp0nzSmFU1W23QshiEnJ4sS1O0o9KiGDzBw9GdpcL5L8fu6WhrVahaNt7sPAwUaD0+2HQu57NZnp6Tg6OiLIjW0ihMBgyDU9GUTubDjXXi0QcLtM5JaTW9dKo8Lh9kPLweauv7d/reQ/bq1R3XkIke8BlO+BJMhNkJ2erSM9W096jo6MnNv/Z+vI0RuMsuXKndvfvD5gUiZQKYrx15OjjcYoi4NN7sPS3kaNg7VGJuC+TbWciWs0GlavXs3ChQvLva2HHnqo0PJ169bRoEEDIiMjefzxxwutc+pUbvLhzZs3M3ny5ALH69evz/r164FcN8gFCxYUqPPoo48yd+5cACZNmsSWLVsK1Bk7dizjxo0DYODAgVy4cKFAnTlz5tC7d+8i+6TVatmwYQMNGzasWn0ac6dPVoAVYBDCqOC++H45Xr4BnI+I4L3XnjFRNpCrXKb/uAmdQXDiwG7WfDMbQa5izLuOvacPbV/6hPRsHRdCfyf6wDpTQQTY12tNjUdfxiDg1paFZFw6ZnIcwKl1P3JaDyAhPYdbv3+ELtE0djyAW/cXsKvXBoDo78cWOA7gNXQaVm4+aBOjif19ZqF1fF7Mve+ZF4+QuOOHAsc1bjWpMfQDAFKPbiD16MYCdWzrtsK9x0sAJGxfRFb+PnGnT06tBwCYvU8vzVvD/KdaF1r/QaJaKnFFUahVq1ZliyGxUFS3Z5Cg0LCmEw3quOOm8ygyomNeYufN2RfY7Vgw2FN9fzfWjnsEgPk2J1lwwbFAnUeD6zD3/3J9mielb2FLeoTJcYMQjO7VkGHPdCY1S8fEQ25cMyTd3gCUe1yv1/N4kA9N2weioPDJb6ay5Fkyxnapj7dfHW5dt2f+ltuBpxRQwPjwea1rfTKydZxSX2XHPk3u7P32Q8kghHHGnqM3oFLyTi76niqASnVnlq66bU5q6O1Eq+Y1UVDYuNWW5EwNd8+fg+t7UCeoNgrwo50GhOmD1iCghpMNelsNSfnMNTaVtNnJ0qhUc8rXX3/N0qVLOXHiBH379mXt2rXGY1qtlkmTJrFs2TIUReHpp59m7ty5aDQlf+7MmDGDZs2aSXPKfWIwGEhISMDd3d2izSnVGUsZgzzzjCGfwgcqbKFYCEG2zkBGjh4FcHOQYXErdSbu4+PDtGnT2LZtG9eumS5CzZw5kz179nD69GkA+vbty6xZs/jgg9yfeAkJCfTr16/ANd9//3369+9f/sI/QKhUKjw9PStbjAcaSxkDJW+mXWA+XXHt21qpq6UbaVmpVCU+ZMgQAMLDwwso8cWLFzN37lyjWWTq1Km8+eabRiXu7u7OgQMH7qv97OxssrPv+B+npOSm+zIYDBgMdxbAVCqVyXvI+zAr5VauUqluz3pEuZWXVBYhBImJibi6uprMAqtyn6raOOWfiavV6mrRp/KQvbg+VVcs0iaemJjItWvXCAoKMpYFBQVx9epVo5vVvRgxYgSHDx/G0dGRQ4cO8b///a9AndmzZ/Phhx8WKI+KisLJKdev1tHREU9PTxISEkhLuxPnwdXVFVdXV2JjY8nMvLPF3MPDAycnJ27cuIE2X6YUb29v7OzsiIqKMvlQ+/j4oNFouHrVNGWWv78/Op2O6Og7gZQURSEgIICsrCxu3rzjf2tlZUXt2rVJS0sjPj7eWG5nZ4e3tzfJyckkJSUZy0vbJzc3N9LS0sjKykKnu5NYoir3qaqNU2pqKgkJCaSnp+Pm5lYt+lSR41SnTh2qKxbhYjhjxgzCw8ONNvGoqCj8/f2JjY01/oSMjY2lRo0aREVF4evra5Z2C5uJ+/n5kZiYaGITr84zvJLIIoQw3nc5E6+8mXiem6ecicuZeH4scibu6Ji7up+cnGxU4snJucHn82bI5sDGxgYbGxtjFMO8WWZaWlq1HvTSYjAYSE1NlfelEjEYDKSnp8sxuA+cnJyq1C7dkmKRStzNzQ1fX1/Cw8OpXz/XvSs8PBw/P78SmVJKy7hx4xg3bhzXrl3Dz88PPz8/s7chkUgql+rqdVapSlyn0xlfBoOBrKwsVCoV1tbWjBkzho8//phHHsn1v501axYvvvhiucrj4+NjtIe3b9+ef/75576u165du1Jdo6T1i6tXlmN3l9/9Ps/MFBUVVaFfgtLeP3NcozLGoCRlcgxKV6+wY+b8FW9JVKoSnzlzpsnCop2dHV26dCE0NJT333+f+Ph4mjRpAsCoUaN47733ylUelUpltLer1er7/rKU9holrV9cvbIcu7u8qHrOzs4VqkAelDEoaRnIMShpPXPIXVWoVOPajBkzjAsfea/Q0FAgd9V7/vz5JCYmkpiYyFdffVWqjT73S96W7oq8RknrF1evLMfuLjdH383BgzIGJS2rDKrjGFQ3LMI7RWLZyJ2slY8cA0lRyGVuyT2xsbFh+vTp2NjYVLYoDyxyDCRFIWfiEolEUoWRM3GJRCKpwkglLpFIJFUYqcQlEomkCiOVuOS+2L9/Pw8//DBdunShf//+JsGOJOXPzZs3CQ4OpkuXLnTq1ImTJ09WtkiSCkYubErui+joaFxdXbG3t+fbb78lPj6eqVOnVrZYDwx6fW5yYpVKRWhoKN9//z2//PJLZYslqUAsMnaKpOrg4+Nj/N/a2rpCN2RJcncm5pGUlETLli0rURpJZSDNKRK+/vpr2rZti42NDYMGDTI5ptVqGT9+PG5ubri7uzNhwgSTmOJ5xMfHs2DBAl544YUKkrr6cL/3//Tp0wQHBzNhwgS6dOlSgZJLLAGpxCXGNHkvvfRSgWP50+SdOnWKsLAwZs2aZVInIyOD4cOHM2/ePItIIVbVuN/737RpU/bt28eGDRuYMGFCRYktsRSERHKb6dOni8cff9ykzNfXV6xatcr4fuXKlcLf39/4XqvViscee0z88ccfFSVmtaUs9z8rK8v4/+XLl0XXrl3LXU6JZSENmJIiKUmavF9//ZXdu3eTkpLCl19+Sf/+/XnrrbcqT+hqREnu/9GjR3n33XeNWXTmzJlTeQJLKgWpxCVFkpcD0dXV1ViW939qaiouLi6MHj2a0aNHV4J01Z+S3P+HH36YXbt2VYJ0EktB2sQlRZI/TV4e5ZEmT1I48v5LSoJU4pIiyZ8mL4/yTJMnMUXef0lJkEpcgk6nIysryyRNXk5ODoAxTV5MTAwxMTEVkibvQUPef8l9Udkrq5LKZ/r06QIweXXp0kUIIUROTo4YO3ascHV1Fa6urmL8+PFCq9VWrsDVDHn/JfeD3HYvkUgkVRhpTpFIJJIqjFTiEolEUoWRSlwikUiqMFKJSyQSSRVGKnGJRCKpwkglLpFIJFUYqcQlEomkCiOVuEQikVRhpBKXSCSSKoxU4hKJRFKFkUpcUu3QarXY2tpSo0YN9Hq9sXzTpk3UqFGjEiWTSMyPVOKSasepU6cwGAw4OjoSFhZmLD9y5Aht2rSpRMkkEvMjlbik2nH06FGaNm3KyJEjWb16tbH8yJEjtG7duhIlk0jMj1TikmrH0aNHad26NUOHDmXNmjXkBeqUM3FJdUQqcUm1I0+Jt23bFisrK/bt20dsbCxRUVGlUuKvvfYaXbp0kTksJRaNjCcuqVbo9XqcnZ3ZunUrwcHBTJ48GYDevXvz9NNPExcXV6C+Wq0u9FotWrTg+PHj5S6zRHI/yGz3kmrF2bNnycrKIigoCIChQ4fy1FNP4eHhYbSHh4aG8umnn2Jra0ubNm1ITEzkn3/+QafTMWfOHNq3b88bb7zBhQsX6Nq1K7/99hs1a9asxF5JJEUjzSmSasXRo0dp1KgR9vb2AAQHB6PValm8eLGJKSUmJoaVK1fSqlUr9Ho9oaGhrFmzhnfeeQeAL774grp16xIaGioVuMSikTNxSbUizx6eh6IoDB48mAULFpiUt2/fHrVazcmTJ9m8eTNdu3YFIDU1taJFlkjuC6nEJdWKuXPnFiibP38+8+fPNynLs4M3bdqUgQMHMnv2bABjlnmJpKogzSmSB5r+/fsjhKBr165069aN999/v7JFkkhKhfROkUgkkiqMnIlLJBJJFUYqcYlEIqnCSCUukUgkVRipxCUSiaQKI5W4RCKRVGGkEpdIJJIqjFTiEolEUoWRSlwikUiqMFKJSyQSSRVGKnGJRCKpwkglLpFIJFUYqcQlEomkCvP/vufydL7liYYAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UyDLczUtEDJ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lot38_oMXTjJ"
      },
      "outputs": [],
      "source": [
        "!rm *.png"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}